#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\begin_preamble
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% ICML 2014 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Use the following line _only_ if you're still using LaTeX 2.09.
%\documentstyle[icml2014,epsf,natbib]{article}
% If you rely on Latex2e packages, like most moden people use this:

%comment this out in lyx, as it defines document class internally
%\documentclass{article}

% use Times
\usepackage{times}
% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
\usepackage{subfigure} 

% For citations
\usepackage{natbib}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2014} with
% \usepackage[nohyperref]{icml2014} above.
\usepackage{hyperref}
\usepackage{breakurl}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
%\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Employ the following version of the ``usepackage'' statement for
% submitting the draft version of the paper for review.  This will set
% the note in the first column to ``Under review.  Do not distribute.''
%\usepackage{icml2014} 
% Employ this version of the ``usepackage'' statement after the paper has
% been accepted, when creating the final version.  This will set the
% note in the first column to ``Proceedings of the...''
\usepackage{icml2015}
%\usepackage[accepted]{icml2014stylefiles/icml2014}

\icmltitlerunning{Kamiltonian Monte Carlo}
\end_preamble
\use_default_options false
\begin_modules
theorems-std
\end_modules
\maintain_unincluded_children false
\language british
\language_package none
\inputencoding auto
\fontencoding default
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize letterpaper
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 0
\use_package mathtools 1
\use_package mhchem 0
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine natbib
\cite_engine_type authoryear
\biblio_style plainnat
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 2
\tocdepth 2
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
twocolumn[
\end_layout

\begin_layout Plain Layout


\backslash
icmltitle{Kamiltonian Monte Carlo}
\end_layout

\begin_layout Plain Layout


\backslash
icmlauthor{Heiko Strathmann}{heiko.strathmann@gmail.com}
\end_layout

\begin_layout Plain Layout


\backslash
icmladdress{Gatsby Unit, CSML, University College London, UK}
\end_layout

\begin_layout Plain Layout


\backslash
icmlauthor{Dino Sejdinovic}{dino.sejdinovic}
\end_layout

\begin_layout Plain Layout


\backslash
icmladdress{Department of Statistics, University of Oxford, UK}
\end_layout

\begin_layout Plain Layout


\backslash
icmlauthor{Arthur Gretton}{arthur.gretton@gmail.com}
\end_layout

\begin_layout Plain Layout


\backslash
icmladdress{Gatsby Unit, CSML, University College London, UK}
\end_layout

\begin_layout Plain Layout


\backslash
icmlkeywords{kernel methods, hamiltonian monte carlo, adaptive mcmc, pseudo-marg
inal}
\end_layout

\begin_layout Plain Layout

]
\end_layout

\end_inset


\begin_inset FormulaMacro
\newcommand{\diag}{\text{diag}}
{\text{diag}}
\end_inset


\end_layout

\begin_layout Abstract
We propose an adaptive Kernel Hamiltonian Monte Carlo (K-HMC) algorithm
 to simulate from an arbritary target probability density.
 Our sampler fills a gap: when the density is intractable, classic HMC methodolo
gy 
\emph on
cannot
\emph default
 be applied -- one is left with random walk methods which suffer from bad
 mixing behaviour.
 We extend recent ideas of adaptively learning target covariance structure
 in a Reproducing Kernel Hilbert Space (RKHS), based on the history of the
 Markov chain.
 But rather than 
\emph on
locally 
\emph default
smoothing the chain history, we directly model its 
\emph on
global
\emph default
 log-density as an RKHS function via score matching.
 This is not only a more interpretable and well-posed problem, but it also
 provides a differentiable log-density approximation, which can directly
 be used to simulate trajectories from a Hamiltonian dynamical system.
 We construct an 
\emph on
exact 
\emph default
MCMC algorithm that (i) behaves similar to HMC in terms of autocorrelation
 and acceptance rates, but that (ii) does not require target gradients,
 and that in particular (iii) works well in high dimensions.
 We support our claims with experimental studies.
\end_layout

\begin_layout Paragraph
Next steps (Heiko)
\end_layout

\begin_layout Itemize
Proof read Gaussian kernel score matching
\end_layout

\begin_layout Itemize
Implement carefully
\end_layout

\begin_layout Itemize
Implement trajectories, possibly within a given framework
\end_layout

\begin_layout Itemize
Create plots for illustration
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Itemize
Why MCMC
\end_layout

\begin_layout Itemize
Kernel Metropolis Hastings 
\begin_inset CommandInset citation
LatexCommand citep
key "sejdinovic_kernel_2014"

\end_inset

 and pros/cons
\end_layout

\begin_layout Itemize
Want: HMC like sampler
\end_layout

\begin_layout Itemize
Idea: Hamiltonian dynamics on kernel energy as proposal
\end_layout

\begin_layout Itemize
Give paper outline
\end_layout

\begin_layout Itemize
Here are some plots that might be useful
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
target "http://nbviewer.ipython.org/gist/anonymous/329d11916eb29743a7de"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Paragraph
Paper outline
\end_layout

\begin_layout Standard
Section
\end_layout

\begin_layout Section
Background & Previous work
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "sec:previous_work"

\end_inset


\end_layout

\begin_layout Subsection
Notation
\end_layout

\begin_layout Itemize
Unnormalised target density 
\begin_inset Formula $\pi:\mathbb{R^{d}}\to\mathbb{R}$
\end_inset

 , unbiased estimator 
\begin_inset Formula $\mathbb{E}[\hat{\pi}(\theta)]=\pi(\theta)$
\end_inset


\end_layout

\begin_layout Itemize
Markov chain history at iteration 
\begin_inset Formula $t$
\end_inset

 with current position 
\begin_inset Formula $x_{t}$
\end_inset

: 
\begin_inset Formula $\left\{ x_{i}\right\} _{i=0}^{t-1}$
\end_inset


\end_layout

\begin_layout Itemize
History subsample 
\begin_inset Formula $\mathbf{z}=\left\{ z_{i}\right\} _{i=1}^{n}\subseteq\left\{ x_{i}\right\} _{i=0}^{t-1}$
\end_inset

 
\end_layout

\begin_layout Subsection
Problem setting
\end_layout

\begin_layout Standard
Write what we want to do and what are important parts of it.
\end_layout

\begin_layout Subsection
Previous work
\end_layout

\begin_layout Standard
Write about Adaptive MCMC, 
\begin_inset CommandInset citation
LatexCommand citet
key "Andrieu2008,Haario1999"

\end_inset

, Kameleon MCMC, 
\begin_inset CommandInset citation
LatexCommand citet
key "sejdinovic_kernel_2014"

\end_inset

, and how it improves over adaptive MH.
 Mention downsides and what we would like to have.
 In particular the pseudo-marginal case, 
\begin_inset CommandInset citation
LatexCommand citet
key "beaumont2003estimation,Andrieu2009a"

\end_inset

.
\end_layout

\begin_layout Subsection
Hamiltonian Monte Carlo
\begin_inset Note Note
status open

\begin_layout Plain Layout
AG: we don't really need a separate section on mean embeddings
\end_layout

\end_inset


\end_layout

\begin_layout Standard
We really would like to do HMC.
 Here is what it is.
\end_layout

\begin_layout Standard
Following 
\begin_inset CommandInset citation
LatexCommand citet
key "Neal2010"

\end_inset

, Hamiltonian Monte Carlo is based around a fictious dynamical system of
 particles 
\begin_inset Formula $(p,q)\in{\cal X}\times{\cal X}$
\end_inset

 whose evolution in time is described by the Hamiltonian,
\begin_inset Formula 
\begin{align*}
H(p,q) & =K(p)+U(q),
\end{align*}

\end_inset

which consists of potential energy 
\begin_inset Formula $U(q)$
\end_inset

, the log-pdf of the target 
\begin_inset Formula $\pi$
\end_inset

, and a kinetic energy function 
\begin_inset Formula $K(p)$
\end_inset

, usually a Gaussian.
 Evolution of the system,
\begin_inset Formula 
\begin{align*}
\frac{dq_{i}}{dt} & =\frac{\partial H}{\partial p_{i}}\\
\frac{dp_{i}}{dt} & =-\frac{\partial H}{\partial q_{i}},
\end{align*}

\end_inset

leaves the value of 
\begin_inset Formula $H$
\end_inset

 constant.
 Refering to 
\begin_inset CommandInset citation
LatexCommand citep
key "Neal2010"

\end_inset

 for details, such trajectories of 
\begin_inset Formula $(p,q)$
\end_inset

 along the contour lines of 
\begin_inset Formula $H$
\end_inset

 can be used to construct proposals for a Metropolis-Hastings type algorithm:
\end_layout

\begin_layout Enumerate
Re-sample momentum 
\begin_inset Formula $p'\sim K(p)$
\end_inset

.
 This produces a new point 
\begin_inset Formula $(p',q)$
\end_inset

 which has the same distribution as 
\begin_inset Formula $(p,q)$
\end_inset

 since 
\begin_inset Formula $p'$
\end_inset

 comes from the true marginal distribution of the chain.
 Note that the value of 
\begin_inset Formula $H$
\end_inset

 is changed in this step, i.e., we jump to another contour of 
\begin_inset Formula $H$
\end_inset

.
\end_layout

\begin_layout Enumerate
Starting from 
\begin_inset Formula $(p',q)$
\end_inset

, simulate the system for a number of discrete time steps to obtain 
\begin_inset Formula $(p^{*},q^{*})$
\end_inset

.
 This step requires access to gradients of 
\begin_inset Formula $H$
\end_inset

, and therefore of the log-pdf of the target 
\begin_inset Formula $\log\pi(\cdot)=U(\cdot)$
\end_inset

.
\end_layout

\begin_layout Enumerate
In order to correct for discretisaKamiltonian Monte Carlotion errors arising
 from numerical simulation, accept this proposal according to its new value
 of 
\begin_inset Formula $H(p^{*},q^{*})$
\end_inset

 using a Metorpolis-Hastings step with an acceptance probability 
\begin_inset Formula 
\begin{align*}
 & \min\left[1,\exp\left(H(p^{*},q^{*})-H(p',q)\right)\right]
\end{align*}

\end_inset

Note that this step does not require gradients but only evaluation of the
 target.
\end_layout

\begin_layout Standard
When iterating, the resulting Markov chain in 
\begin_inset Formula $(p,q)$
\end_inset

-space can be marginalised by simply dropping all 
\begin_inset Formula $p$
\end_inset

 components.
 One can show that 
\begin_inset Formula $\exp(-U(\cdot))\propto\pi(\cdot)$
\end_inset

 is the invariant distribution.
 Furthermore, acceptance rate of the third step will be usually large, while
 autocorrelation is small.
\end_layout

\begin_layout Paragraph
HMC and intractable densities
\end_layout

\begin_layout Standard
Write about how HMC cannot be applied if gradients is not known.
\end_layout

\begin_layout Standard
Kamiltonian Monte Carlo is based on the idea of replacing potential energy
 
\begin_inset Formula $U$
\end_inset

 in the second step by a kernel induced surrogate, that does not require
 gradients of the log-pdf of the target.
 Invariant distribution will remain unchanged as we 
\emph on
do not
\emph default
 change anything in the third step.
 We now give a brief review of score matching that we use to estimate potential
 energy.
\end_layout

\begin_layout Subsection
Score Matching
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout

\series bold
AG:
\series default
 Score matching references, 
\begin_inset CommandInset citation
LatexCommand citet
key "Hyvarinen-05,Hyvarinen-07"

\end_inset

.
 The latter gave solutions in terms of a finite mixture at fixed centres.
 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Following 
\begin_inset CommandInset citation
LatexCommand citet
key "Hyvarinen-05"

\end_inset

, we assume that a variable 
\begin_inset Formula $\xi\sim p_{x}$
\end_inset

 has some unknown probability density function 
\begin_inset Formula $p_{x}(\cdot)$
\end_inset

 defined on 
\begin_inset Formula $\mathbb{R}^{d}$
\end_inset

, and that the log probability takes the form 
\begin_inset Formula 
\[
\log p(\xi;\alpha)=\log q(\xi;\alpha)-\log Z(\alpha),
\]

\end_inset

 where 
\begin_inset Formula $\alpha$
\end_inset

 is a vector of parameters of yet unspecified dimension.
 We aim to approximate 
\begin_inset Formula $p_{x}(\cdot)$
\end_inset

 by 
\begin_inset Formula $p(\cdot;\hat{\theta})$
\end_inset

, i.e., to find 
\begin_inset Formula $\hat{\theta}$
\end_inset

 from a set of fixed i.i.d.
 samples 
\begin_inset Formula $\{x_{i}\sim p_{x}\}_{i=1}^{n}$
\end_inset

.
 From 
\begin_inset CommandInset citation
LatexCommand citet
after "equation. 2"
key "Hyvarinen-05"

\end_inset

, the criterion being optimized is the expected squared distance between
 score functions,
\emph on
 
\emph default

\begin_inset Formula 
\[
J(\alpha)=\frac{1}{2}\int_{\xi}p_{x}(\xi)\left\Vert \psi(\xi;\alpha)-\psi_{x}(\xi)\right\Vert ^{2}d\xi,
\]

\end_inset

where 
\begin_inset Formula 
\[
\psi(\xi;\alpha)=\nabla_{\xi}\log p(\xi;\alpha),
\]

\end_inset

and 
\begin_inset Formula $\psi_{x}(\xi)$
\end_inset

 is the derivative wrt 
\begin_inset Formula $\xi$
\end_inset

 of the unknown true density 
\begin_inset Formula $p_{x}(\xi)$
\end_inset

.
 As proved in 
\begin_inset CommandInset citation
LatexCommand citet
after "Theorem 1"
key "Hyvarinen-05"

\end_inset

, it is possible to express 
\begin_inset Formula $J(\alpha)$
\end_inset

 without access to the unknown 
\begin_inset Formula $\psi_{x}(\xi)$
\end_inset

 as
\begin_inset Formula 
\[
J(\alpha)=\int_{\xi}p_{x}(\xi)\sum_{\ell=1}^{d}\left[\partial_{\ell}\psi_{\ell}(\xi;\alpha)+\frac{1}{2}\psi_{\ell}(\xi;\alpha)^{2}\right]d\xi,
\]

\end_inset

where
\begin_inset Formula 
\[
\psi_{\ell}(\xi;\alpha)=\frac{\partial\log q(\xi;\alpha)}{\partial\xi_{\ell}}
\]

\end_inset

and
\begin_inset Formula 
\[
\partial_{\ell}\psi_{\ell}(\xi;\alpha)=\frac{\partial^{2}\log q(\xi;\alpha)}{\partial\xi_{\ell}^{2}}.
\]

\end_inset

Replacing the integral 
\begin_inset Formula $\int_{\xi}p_{x}(\xi)$
\end_inset

 with an average over the samples 
\begin_inset Formula $x_{i}$
\end_inset

 gives us a sample version of 
\begin_inset Formula $J(\alpha)$
\end_inset

, minimising of which is consistent,
\begin_inset Formula 
\begin{equation}
\hat{J}(\alpha)=\frac{1}{m}\sum_{\ell=1}^{d}\sum_{i=1}^{n}\left[\partial_{\ell}\psi_{\ell}(x_{i};\alpha)+\frac{1}{2}\psi_{\ell}^{2}(x_{i};\alpha)\right].\label{eq:score_matching_objective_sample_version}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "sec:kernel_hmc"

\end_inset


\end_layout

\begin_layout Standard
We now combine score matching with HMC.
\end_layout

\begin_layout Section
Kernel Hamiltonian Dynamics for MCMC
\end_layout

\begin_layout Subsection
Kernel Score Matching
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
AG: I think 
\begin_inset Quotes eld
\end_inset

regularization and smoothing
\begin_inset Quotes erd
\end_inset

 should go here: any empirical solution will already be regularized, so
 it doesn't make sense to have a separate regularization section.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Recall our aim is to replace the potential energy function in HMC, which
 is the log-pdf of the target 
\begin_inset Formula $\pi$
\end_inset

, by a surrogate whose gradients can be computed easily.
 More specifically, we will use kernelised score matching to fit the target
 log-pdf with a RKHS function, which corresponds to fitting an exponential
 family model in the RKHS.
 Elements 
\begin_inset Formula $f:{\cal X}\to\mathbb{R}$
\end_inset

 of an RKHS 
\begin_inset Formula ${\cal H}$
\end_inset

 uniquely induced by a reproducing kernel 
\begin_inset Formula $k:{\cal X}\times{\cal X}\to\mathbb{R}$
\end_inset

 can be written as linear combinations of feature maps, 
\begin_inset Formula $f(\cdot)=\sum_{i=1}^{m}\alpha_{i}k(x_{i},\cdot)$
\end_inset

.
 That is, we simply assume the parametric form for the unnormalised log-pdf
\begin_inset Formula 
\[
\log q(\xi;\alpha)=\log\Pr(\xi;\alpha)+\log Z(\alpha)=\sum_{i=1}^{n}\alpha_{i}k(x_{i},\xi),
\]

\end_inset

where 
\begin_inset Formula $\alpha\in\mathbb{R}^{n}$
\end_inset

 is an 
\begin_inset Formula $n$
\end_inset

-dimensional parameter vector.
 Note that where we did not specify its dimension previously, its dimensionality
 now equals the number of training points 
\begin_inset Formula $\{x_{i}\sim p_{x}\}_{i=1}^{n}$
\end_inset

.
\end_layout

\begin_layout Paragraph
Gaussian Kernel
\end_layout

\begin_layout Standard
We now implement score matching for the well known Gaussian kernel 
\begin_inset Formula $k(x,y)=\exp\left(-\frac{\|x-y\|^{2}}{\sigma}\right)$
\end_inset

.
 Note that its corresponding RKHS is infinite dimensional, we are fitting
 an infinite exponential family model in the RKHS, see also 
\begin_inset CommandInset citation
LatexCommand citet
key "SriFukKumGreHyv14"

\end_inset

.
 The score function is then given by
\begin_inset Formula 
\[
\psi_{\ell}(\xi;\alpha)=\frac{2}{\sigma}\sum_{i=1}^{m}\alpha_{i}(x_{i\ell}-\xi_{\ell})\exp\left(-\frac{\|x_{i}-\xi\|^{2}}{\sigma}\right).
\]

\end_inset

Omitting intermediate details (see appendix 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:appendix_kernel_score_matching"

\end_inset

), the resulting sample objective function 
\begin_inset Formula $J(\alpha)$
\end_inset

 in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:score_matching_objective_sample_version"

\end_inset

 is globally minimised for
\begin_inset Formula 
\begin{equation}
\hat{\alpha}=\frac{-\sigma}{2}(C+\lambda I)^{-1}b,\label{eq:kernel_score_matching_linear_system}
\end{equation}

\end_inset

where we added a regulariser using the norm of 
\begin_inset Formula $\alpha$
\end_inset

, and defined
\begin_inset Formula 
\begin{align*}
b & =\sum_{\ell=1}^{D}\left(\frac{2}{\sigma}(Ks_{\ell}+D_{s_{\ell}}K\mathbf{1}-2D_{x_{\ell}}Kx_{\ell})-K\mathbf{1}\right)\in\mathbb{R}^{n}\\
C & =\sum_{\ell=1}^{D}\left[D_{x_{\ell}}K-KD_{x_{\ell}}\right]\left[KD_{x_{\ell}}-D_{x_{\ell}}K\right]\in\mathbb{R}^{n\times n},
\end{align*}

\end_inset

and
\begin_inset Formula 
\begin{align*}
K_{ij}: & =\exp\left(-\frac{\|x_{i}-x_{j}\|^{2}}{\sigma}\right)\\
s_{i\ell} & :=x_{i\ell}^{2}\\
x_{\ell} & :=\left[\begin{array}{ccc}
x_{1\ell} & \dots & x_{m\ell}\end{array}\right]^{\top}\\
D_{x_{\ell}} & :=\diag(x_{\ell}).
\end{align*}

\end_inset


\end_layout

\begin_layout Subsection
Examples
\end_layout

\begin_layout Standard
Put some nice plots to illustrate ideas (no experiments yet)
\end_layout

\begin_layout Itemize
Low dimensional Banana
\end_layout

\begin_layout Itemize
Smoothing property of regularisation
\end_layout

\begin_layout Itemize
High dimensional banana, projected to 2D
\end_layout

\begin_layout Itemize
Comparison of HMC and KHMC trajectories
\end_layout

\begin_layout Subsection
Geometry extension (TODO)
\end_layout

\begin_layout Itemize
Can we use second order information of our density estimate? Then run RM-HMC
 or M-MALA
\end_layout

\begin_layout Itemize
What about the Kameleon proposal as a Hessian estimate? Formal relationship?
 In practice
\end_layout

\begin_layout Section
Kamiltonian Monte Carlo
\end_layout

\begin_layout Subsection
Proposal construction
\end_layout

\begin_layout Standard
Describe in words and then give algorithm
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Plain Layout

\series bold
Kamiltonian Monte Carlo (TODO)
\end_layout

\begin_layout Plain Layout

\emph on
Input
\emph default
: unnormalized target 
\begin_inset Formula $\pi$
\end_inset

, subsample size 
\begin_inset Formula $n$
\end_inset

, scaling parameters 
\begin_inset Formula $\nu,\gamma,$
\end_inset

 kernel 
\begin_inset Formula $k$
\end_inset

, 
\end_layout

\begin_layout Itemize
At iteration 
\begin_inset Formula $t+1$
\end_inset

,
\end_layout

\begin_deeper
\begin_layout Enumerate
Obtain a random subsample 
\begin_inset Formula $\mathbf{z}=\left\{ z_{i}\right\} _{i=1}^{n}$
\end_inset

 of the chain history 
\begin_inset Formula $\left\{ x_{i}\right\} _{i=0}^{t-1}$
\end_inset

,
\end_layout

\begin_layout Enumerate
Sample proposed point 
\begin_inset Formula $x^{*}$
\end_inset

 from 
\begin_inset Formula $q_{\mathbf{z}}(\cdot|x_{t})=\mathcal{N}(x_{t},\gamma^{2}I+\nu^{2}M_{\mathbf{z},x_{t}}HM_{\mathbf{z},x_{t}}^{\top})$
\end_inset

, 
\end_layout

\begin_layout Enumerate
Accept/Reject with the Metropolis-Hastings acceptance probability 
\begin_inset Formula $A(x_{t},x^{*})$
\end_inset

 in eq.
 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:MetAcceptProb"

\end_inset

,
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vspace{-0.5cm}
\end_layout

\end_inset

 
\end_layout

\begin_deeper
\begin_layout Plain Layout
\begin_inset Formula 
\begin{eqnarray*}
x_{t+1} & = & \begin{cases}
x^{*}, & \textrm{w.p.}\; A(x_{t},x^{*}),\\
x_{t}, & \textrm{w.p.}\;1-A(x_{t},x^{*}).
\end{cases}
\end{eqnarray*}

\end_inset


\end_layout

\end_deeper
\end_deeper
\end_inset


\end_layout

\begin_layout Subsection
Properties
\end_layout

\begin_layout Itemize
Properties of kernel energy function, consequences for algorithm
\end_layout

\begin_layout Itemize
Kernel choice via cross-validation (plot of objective function)
\end_layout

\begin_layout Itemize
Extensions:
\end_layout

\begin_deeper
\begin_layout Itemize
Second order information? 
\begin_inset CommandInset citation
LatexCommand citet
key "Girolami2011"

\end_inset


\end_layout

\begin_layout Itemize
Relation to geometry once again
\end_layout

\end_deeper
\begin_layout Paragraph
Linear costs via Incomplete Cholesky (TODO write properly)
\end_layout

\begin_layout Standard
Solving the linear system in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:kernel_score_matching_linear_system"

\end_inset

 for 
\begin_inset Formula $\alpha$
\end_inset

 requires 
\begin_inset Formula ${\cal O}(n^{3})$
\end_inset

 computation and 
\begin_inset Formula ${\cal O}(n^{2})$
\end_inset

 storage.
 This can be limiting, in particular if training data comes from an increasingly
 long Markov chain.
 
\begin_inset CommandInset citation
LatexCommand citet
key "sejdinovic_kernel_2014"

\end_inset

 sidestepped a similar problem via fixing the number of training data when
 computing MCMC proposals.
 However, we would ideally like to use 
\emph on
all
\emph default
 available data.
 We therefore compute an incomplete Cholesky factorisation 
\begin_inset Formula $K\approx LL^{T},$
\end_inset

 where 
\begin_inset Formula $L$
\end_inset

 is 
\begin_inset Formula $n\times t$
\end_inset

 and 
\begin_inset Formula $t\ll n$
\end_inset

, which costs 
\begin_inset Formula ${\cal O}(nt)$
\end_inset

 storage.
 Using the Woodbury identity, we can then rewrite the linear system into
 a form similar to 
\begin_inset Formula $L(L^{T}L+\lambda I)^{-1}L^{T}$
\end_inset

 which costs 
\begin_inset Formula ${\cal O}(t^{3}+t^{2}n)$
\end_inset

 computation.
 Treating 
\begin_inset Formula $t$
\end_inset

 as fixed, this is linear in both storage and computation, allowing to use
 all Markov chain history.
 See appendix 
\begin_inset CommandInset ref
LatexCommand eqref
reference "sec:appendix_kernel_score_matching"

\end_inset

 for details.
\end_layout

\begin_layout Section
Experiments
\end_layout

\begin_layout Subsection
Comparison of trajectories
\end_layout

\begin_layout Standard
Empirically compare on how well we can approximate the true HMC trajectories
\end_layout

\begin_layout Itemize
How to put in numbers? Maybe run leapfrog with same momentum and step size
 for the same number of steps.
 Then somehow compute how far this travelled, and the average acceptance
 probability.
 Latter is in particular interesting.
\end_layout

\begin_layout Itemize
2D Banana & other synthetic
\end_layout

\begin_layout Itemize
Higher dimensions, projections
\end_layout

\begin_layout Itemize
Kernel choice via cross-validation and how we get most similar trajectories
\end_layout

\begin_layout Itemize
Optimise acceptance rate? Is this even well conditioned? Plot 
\begin_inset Formula $J(\alpha)$
\end_inset

 and ESS, and see whether cross-validation kernel is good for mixing
\end_layout

\begin_layout Subsection
Pseudo-Marginal MCMC
\end_layout

\begin_layout Itemize
Reproduce Kameleon experiments from Kameleon paper with KHMC
\end_layout

\begin_layout Itemize
Higher dimensional, also compare to plain HMC, (and random walk)
\end_layout

\begin_layout Itemize
Mushroom as a cool dataset?
\end_layout

\begin_layout Subsection
Comparisons to geometry HMC?
\end_layout

\begin_layout Standard
For that, first need to work out how we would even do that.
\end_layout

\begin_layout Section
Conclusions
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "local"
options "icml2015"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
normalsize
\end_layout

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
onecolumn
\end_layout

\end_inset


\end_layout

\begin_layout Section
\start_of_appendix
Proofs
\end_layout

\begin_layout Subsection
\begin_inset CommandInset label
LatexCommand label
name "sec:appendix_kernel_score_matching"

\end_inset

Kernel Score Matching Details
\end_layout

\begin_layout Paragraph
Objective function
\end_layout

\begin_layout Standard
Assume
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\log q(\xi;\alpha):=\log\Pr(\xi;\alpha)+\log Z(\alpha)=\sum_{i=1}^{m}\alpha_{i}k(x_{i},\xi)
\]

\end_inset

where
\begin_inset Formula 
\[
k(x_{i},\xi)=\exp\left(-\frac{\|x_{i}-\xi\|^{2}}{\sigma}\right)=\exp\left(-\frac{1}{\sigma}\sum_{\ell=1}^{d}(x_{i\ell}-\xi_{\ell})^{2}\right)
\]

\end_inset

Thus
\begin_inset Formula 
\[
\psi_{\ell}(\xi;\alpha)=\frac{2}{\sigma}\sum_{i=1}^{m}\alpha_{i}(x_{i\ell}-\xi_{\ell})\exp\left(-\frac{\|x_{i}-\xi\|^{2}}{\sigma}\right)
\]

\end_inset

and
\begin_inset Formula 
\begin{align*}
\partial_{\ell}\psi_{\ell}(\xi;\alpha) & =\frac{-2}{\sigma}\sum_{i=1}^{m}\alpha_{i}\exp\left(-\frac{\|x_{i}-\xi\|^{2}}{\sigma}\right)+\left(\frac{2}{\sigma}\right)^{2}\sum_{i=1}^{m}\alpha_{i}(x_{i\ell}-\xi_{\ell})^{2}\exp\left(-\frac{\|x_{i}-\xi\|^{2}}{\sigma}\right)\\
 & =\frac{2}{\sigma}\sum_{i=1}^{m}\alpha_{i}\exp\left(-\frac{\|x_{i}-\xi\|^{2}}{\sigma}\right)\left[-1+\frac{2}{\sigma}(x_{i\ell}-\xi_{\ell})^{2}\right].
\end{align*}

\end_inset

Substituting this into 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:score_matching_objective_sample_version"

\end_inset

 yields
\begin_inset Formula 
\begin{align*}
J(\alpha) & =\frac{1}{m}\sum_{i=1}^{m}\sum_{\ell=1}^{d}\left[\partial_{\ell}\psi_{\ell}(x_{i};\alpha)+\frac{1}{2}\psi_{\ell}(x_{i};\alpha)^{2}\right]\\
 & =\frac{2}{m\sigma}\sum_{\ell=1}^{d}\sum_{i=1}^{m}\sum_{j=1}^{m}\alpha_{i}\exp\left(-\frac{\|x_{i}-x_{j}\|^{2}}{\sigma}\right)\left[-1+\frac{2}{\sigma}(x_{i\ell}-x_{j\ell})^{2}\right]\\
 & \qquad+\frac{2}{m\sigma^{2}}\sum_{\ell=1}^{d}\sum_{i=1}^{m}\left[\sum_{j=1}^{m}\alpha_{j}(x_{j\ell}-x_{i\ell})\exp\left(-\frac{\|x_{i}-x_{j}\|^{2}}{\sigma}\right)\right]^{2}
\end{align*}

\end_inset


\end_layout

\begin_layout Paragraph
Matrix form for the cost function
\end_layout

\begin_layout Standard
The expression for the term 
\begin_inset Formula $J(\alpha)$
\end_inset

 being optimized is the sum of two terms.
 
\end_layout

\begin_layout Standard
Consider the 
\series bold
first term
\series default
:
\begin_inset Formula 
\[
\sum_{\ell=1}^{d}\sum_{i=1}^{m}\sum_{j=1}^{m}\alpha_{i}\exp\left(-\frac{\|x_{i}-x_{j}\|^{2}}{\sigma}\right)\left[-1+\frac{2}{\sigma}(x_{i\ell}-x_{j\ell})^{2}\right]
\]

\end_inset

 The term we need to compute is
\begin_inset Formula 
\begin{align*}
 & \sum_{i=1}^{m}\sum_{j=1}^{m}\alpha_{i}\exp\left(-\frac{\|x_{i}-x_{j}\|^{2}}{\sigma}\right)(x_{i\ell}-x_{j\ell})^{2},\\
= & \sum_{i=1}^{m}\sum_{j=1}^{m}\alpha_{i}\exp\left(-\frac{\|x_{i}-x_{j}\|^{2}}{\sigma}\right)\left(x_{i\ell}^{2}+x_{j\ell}^{2}-2x_{i\ell}x_{j\ell}\right).
\end{align*}

\end_inset

Define 
\begin_inset Formula 
\[
x_{\ell}:=\left[\begin{array}{ccc}
x_{1\ell} & \hdots & x_{m\ell}\end{array}\right]^{\top}.
\]

\end_inset

The final term may be computed cheaply with the right ordering of operations,
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
We assume we have an incomplete Cholesy representation of 
\begin_inset Formula $K_{ij}:=\exp\left(-\frac{\|x_{i}-x_{j}\|^{2}}{\sigma}\right)$
\end_inset

, 
\begin_inset Formula 
\[
K\approx LL^{\top},
\]

\end_inset

where 
\begin_inset Formula $L$
\end_inset

 is 
\begin_inset Formula $m\times t$
\end_inset

 and 
\begin_inset Formula $t\ll m$
\end_inset

.
\end_layout

\end_inset


\begin_inset Formula 
\[
-2(\alpha\odot x_{\ell})^{\top}Kx_{\ell}\approx-2(\alpha\odot x_{\ell})^{\top}LL^{\top}x_{\ell},
\]

\end_inset

where 
\begin_inset Formula $\alpha\odot x_{\ell}$
\end_inset

 is the entrywise product.
 The remaining terms are sums with constant row or column terms, and can
 likewise be computed cheaply: define 
\begin_inset Formula $s_{\ell}:=x_{\ell}\odot x_{\ell}$
\end_inset

 with components 
\begin_inset Formula $s_{i\ell}=x_{i\ell}^{2}$
\end_inset

.
 Then
\begin_inset Formula 
\begin{align*}
\sum_{i=1}^{m}\sum_{j=1}^{m}\alpha_{i}k_{ij}s_{j\ell} & =\alpha^{\top}Ks_{\ell}\\
 & \approx\alpha^{\top}LL^{\top}s_{\ell},
\end{align*}

\end_inset

which is cheap to compute.
 Likewise
\begin_inset Formula 
\[
\sum_{i=1}^{m}\sum_{j=1}^{m}\alpha_{i}x_{i\ell}^{2}k_{ij}\approx(\alpha\odot s_{\ell})^{\top}LL^{\top}1.
\]

\end_inset


\end_layout

\begin_layout Standard
We now write out the 
\series bold
second term
\series default
.
 Considering only the 
\begin_inset Formula $\ell$
\end_inset

th dimension, this is
\begin_inset Formula 
\begin{align*}
 & \sum_{i=1}^{m}\left[\sum_{j=1}^{m}\alpha_{j}(x_{j\ell}-x_{i\ell})\exp\left(-\frac{\|x_{i}-x_{j}\|^{2}}{\sigma}\right)\right]^{2}
\end{align*}

\end_inset

In matrix notation, the inner sum is a column vector,
\begin_inset Formula 
\[
K(\alpha\odot x_{\ell})-\left(K\alpha\right)\odot x_{\ell}\approx LL^{\top}(\alpha\odot x_{\ell})-\left(LL^{\top}\alpha\right)\odot x_{\ell}.
\]

\end_inset

We then take the entrywise square and sum the resulting vector, where both
 operations cost 
\begin_inset Formula $O(m)$
\end_inset

.
\end_layout

\begin_layout Paragraph
Solving 
\begin_inset Formula $J(\alpha)$
\end_inset

 for 
\begin_inset Formula $\alpha$
\end_inset


\end_layout

\begin_layout Standard
If we denote by 
\begin_inset Formula $D_{x}$
\end_inset

 the matrix with the vector 
\begin_inset Formula $x$
\end_inset

 on its diagonal, then the following two relations hold
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
K(\alpha\odot x) & =KD_{x}\alpha\\
(K\alpha)\odot x & =D_{x}K\alpha
\end{align}

\end_inset

This means that 
\begin_inset Formula $J(\alpha)$
\end_inset

 as defined previously,
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
J(\alpha) & =\frac{2}{m\sigma}\sum_{\ell=1}^{D}\left[\frac{2}{\sigma}\left[\alpha^{T}Ks_{\ell}+(\alpha\odot s_{\ell})^{T}K\mathbf{1}-2(\alpha\odot x_{\ell})^{T}Kx_{\ell}\right]-\alpha^{T}K\mathbf{1}\right]\\
 & +\frac{2}{m\sigma^{2}}\sum_{\ell=1}^{D}\left[(\alpha\odot x_{\ell})^{T}K-x_{\ell}^{T}\odot(\alpha^{T}K)\right]\left[K(\alpha\odot x_{\ell})-(K\alpha)\odot x_{\ell}\right],
\end{align*}

\end_inset

can be rewritten
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
J(\alpha) & =\frac{2}{m\sigma}\alpha^{T}\sum_{\ell=1}^{D}\left[\frac{2}{\sigma}(Ks_{\ell}+D_{s_{\ell}}K\mathbf{1}-2D_{x_{\ell}}Kx_{\ell})-K\mathbf{1}\right]\\
 & +\frac{2}{m\sigma^{2}}\alpha^{T}\left(\sum_{\ell=1}^{D}\left[D_{x_{\ell}}K-KD_{x_{\ell}}\right]\left[KD_{x_{\ell}}-D_{x_{\ell}}K\right]\right)\alpha\\
 & =\frac{2}{m\sigma}\alpha^{T}b+\frac{2}{m\sigma^{2}}\alpha^{T}C\alpha
\end{align*}

\end_inset

where
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
b & =\sum_{\ell=1}^{D}\left(\frac{2}{\sigma}(Ks_{\ell}+D_{s_{\ell}}K\mathbf{1}-2D_{x_{\ell}}Kx_{\ell})-K\mathbf{1}\right)\in\mathbb{R}^{m}\\
C & =\sum_{\ell=1}^{D}\left[D_{x_{\ell}}K-KD_{x_{\ell}}\right]\left[KD_{x_{\ell}}-D_{x_{\ell}}K\right]\in\mathbb{R}^{m\times m}.
\end{align*}

\end_inset

Assuming 
\begin_inset Formula $C$
\end_inset

 is invertible, this is minimised by 
\emph on

\begin_inset Formula 
\[
\hat{\alpha}=\frac{-\sigma}{2}C^{-1}b.
\]

\end_inset


\end_layout

\begin_layout Paragraph
Incomplete Cholesky
\end_layout

\begin_layout Standard
TODO
\end_layout

\begin_layout Subsection
Consistency?
\end_layout

\end_body
\end_document
