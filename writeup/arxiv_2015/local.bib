@article{Betancourt2015,
arxivId = {arXiv:1502.0151},
author = {Betancourt, Michael},
journal = {arXiv preprint},
title = {{The Fundamental Incompatibility of Hamiltonian Monte Carlo and Data Subsampling}},
url = {http://arxiv.org/abs/1502.01510},
year = {2015}
}

@INPROCEEDINGS{ChenICML2014,
				author={Chen, T. and Fox, E.B. and Guestrin, C.},
				title={Stochastic Gradient {H}amiltonian {M}onte {C}arlo},
				booktitle = "ICML",
				year = {2014},
				month = {June},
				}


@article{RobertsRosenthal2007,
author = "Roberts, G.O. and Rosenthal, J.S.",
journal = "Journal of  Applied Probability",
month = "03",
number = "2",
pages = "458--475",
title = {{Coupling and ergodicity of adaptive Markov chain Monte Carlo algorithms}},
volume = "44",
year = "2007"
}

@Book{BerTho04,
  author =   {A. Berlinet and C. {Thomas-Agnan}},
  title =    {Reproducing Kernel Hilbert Spaces in Probability and Statistics},
  publisher =    {Kluwer},
  year =     {2004},
}

@book{hairer2006geometric,
  title={Geometric numerical integration: structure-preserving algorithms for ordinary differential equations},
  author={Hairer, Ernst and Lubich, Christian and Wanner, Gerhard},
  year={2006},
  publisher={Springer}
}

@article{FilipponeIEEETPAMI13,
author = {Filippone, M. and Girolami, M.},
title = {{Pseudo-marginal Bayesian inference for Gaussian Processes}},
journal ={IEEE Transactions on Pattern Analysis and Machine Intelligence},
year = {2014},
}

@book{leimkuhler2004,
  title={Simulating hamiltonian dynamics},
  author={Leimkuhler, Benedict and Reich, Sebastian},
  year={2004},
  publisher={Cambridge University Press}
}

@article{Andrieu2009a,
author = {Andrieu, Christophe and Roberts, Gareth O.},
doi = {10.1214/07-AOS574},
issn = {0090-5364},
journal = {The Annals of Statistics},
month = apr,
number = {2},
pages = {697--725},
title = {{The pseudo-marginal approach for efficient Monte Carlo computations}},
volume = {37},
year = {2009}
}
@article{Andrieu2008,
author = {Andrieu, Christophe and Thoms, Johannes},
doi = {10.1007/s11222-008-9110-y},
issn = {0960-3174},
journal = {Statistics and Computing},
keywords = {adaptive mcmc,also denoted $\pi$,assumed for simplicity to,controlled,from which,have a density with,markov chain,mcmc,measure,r n x,re-,spect to the lebesgue,stochastic approximation,x},
month = dec,
number = {4},
pages = {343--373},
title = {{A tutorial on adaptive MCMC}},
volume = {18},
year = {2008}
}
@article{beaumont2003estimation,
author = {Beaumont, M A},
journal = {Genetics},
number = {3},
pages = {1139--1160},
title = {{Estimation of population growth or decline in genetically monitored populations}},
volume = {164},
year = {2003}
}
@article{Girolami2002,
abstract = {Kernel principal component analysis has been introduced as a method of extracting a set of orthonormal nonlinear features from multivariate data, and many impressive applications are being reported within the literature. This article presents the view that the eigenvalue decomposition of a kernel matrix can also provide the discrete expansion coefficients required for a nonparametric orthogonal series density estimator. In addition to providing novel insights into nonparametric density estimation, this article provides an intuitively appealing interpretation for the nonlinear features extracted from data using kernel principal component analysis.},
author = {Girolami, Mark},
doi = {10.1162/089976602317250942},
issn = {0899-7667},
journal = {Neural computation},
pages = {669--688},
pmid = {11860687},
title = {{Orthogonal series density estimation and the kernel eigenvalue problem.}},
volume = {14},
year = {2002}
}
@article{Girolami2011,
author = {Girolami, Mark and Calderhead, Ben},
doi = {10.1111/j.1467-9868.2010.00765.x},
issn = {13697412},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
keywords = {bayesian inference,geometry in statistics,hamiltonian monte carlo methods,langevin diffusion,markov chain monte carlo,methods,riemann manifolds},
month = mar,
number = {2},
pages = {123--214},
title = {{Riemann manifold Langevin and Hamiltonian Monte Carlo methods}},
volume = {73},
year = {2011}
}
@article{Haario1999,
author = {Haario, Heikki and Saksman, Eero and Tamminen, Johanna},
doi = {10.1007/s001800050022},
journal = {Computational Statistics},
keywords = {adaptive mcmc,convergence,experimental design,hastings algorithm,mcmc,metropolis},
number = {3},
pages = {375--395},
title = {{Adaptive proposal distribution for random walk Metropolis algorithm}},
volume = {14},
year = {1999}
}
@article{Hyvarinen-05,
author = {Hyv\"{a}rinen, A},
journal = {Journal of Machine Learning Research},
pages = {695--709},
title = {{Estimation of non-normalized statistical models by score matching}},
volume = {6},
year = {2005}
}
@article{Hyvarinen-07,
author = {Hyv\"{a}rinen, A},
journal = {Computational Statistics \& Data Analysis},
pages = {2499--2512},
title = {{Some extensions of score matching}},
volume = {51},
year = {2007}
}
@article{Hyvarinen2006,
abstract = {One often wants to estimate statisticalmodels where the probability density function is known only up to a multiplicative normalization constant. Typically, one then has to resort to Markov Chain Monte Carlomethods, or approximations of the normalization constant. Here,we propose that such models can be estimated by minimizing the expected squared distance between the gradient of the log-density given by the model and the gradient of the log-density of the observed data. While the estimation of the gradient of log-density function is, in principle, a very difficult non-parametric problem, we prove a surprising result that gives a simple formula for this objective function. The density function of the observed data does not appear in this formula, which simplifies to a sample average of a sum of some derivatives of the log-density given by the model. The validity of the method is demonstrated on multivariate Gaussian and independent component analysis models, and by estimating an overcomplete filter set for natural image data.},
author = {Hyv\"{a}rinen, Aapo},
doi = {10.1.1.109.4126},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {contrastive divergence,markov chain,monte carlo,non-normalized densities,pseudo-likelihood,statistical estimation},
pages = {695--708},
title = {{Estimation of non-normalized statistical models by score matching}},
url = {http://jmlr.csail.mit.edu/papers/volume6/hyvarinen05a/old.pdf},
volume = {6},
year = {2006}
}
@inproceedings{Muandet2014,
author = {Muandet, Krikamol},
booktitle = {Advances in Neural Information Processing Systems},
file = {:home/heiko/Downloads/spectral-kmse.pdf:pdf},
title = {{Kernel Mean Estimation via Spectral Filtering}},
url = {http://papers.nips.cc/paper/5319-kernel-mean-estimation-via-spectral-filtering},
year = {2014}
}
@article{Neal2010,
archivePrefix = {arXiv},
arxivId = {arXiv:1206.1901v1},
author = {Neal, RM},
eprint = {arXiv:1206.1901v1},
file = {:home/heiko/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Neal - 2010 - MCMC using Hamiltonian dynamics.pdf:pdf},
journal = {Handbook of Markov Chain Monte Carlo},
title = {{MCMC using Hamiltonian dynamics}},
year = {2010}
}
@inproceedings{Rahimi2007,
abstract = {To accelerate the training of kernel machines, we propose to map the input data to a randomized low-dimensional feature space and then apply existing fast linear methods. Our randomized features are designed so that the inner products of the transformed data are approximately equal to those in the feature space of a user specified shift-invariant kernel. We explore two sets of random features, provide convergence bounds on their ability to approximate various radial basis kernels, and show that in large-scale classification and regression tasks linear machine learning algorithms that use these features outperform state-of-the-art large-scale kernel machines.},
author = {Rahimi, Ali and Recht, Ben},
booktitle = {Advances in Neural Information Processing Systems},
doi = {10.1.1.145.8736},
file = {:home/heiko/Downloads/07.rah.rec.nips.pdf:pdf},
isbn = {160560352X},
number = {1},
title = {{Random features for large-scale kernel machines}},
year = {2007}
}
@inproceedings{sejdinovic_kernel_2014,
abstract = {A Kernel Adaptive Metropolis-Hastings algorithm is introduced, for the purpose of sampling from a target distribution with strongly nonlinear support. The algorithm embeds the trajectory of the Markov chain into a reproducing kernel Hilbert space (\{RKHS)\}, such that the feature space covariance of the samples informs the choice of proposal. The procedure is computationally efficient and straightforward to implement, since the \{RKHS\} moves can be integrated out analytically: our proposal distribution in the original space is a normal distribution whose mean and covariance depend on where the current sample lies in the support of the target distribution, and adapts to its local covariance structure. Furthermore, the procedure requires neither gradients nor any other higher order information about the target, making it particularly attractive for contexts such as Pseudo-Marginal \{MCMC.\} Kernel Adaptive Metropolis-Hastings outperforms competing fixed and adaptive samplers on multivariate, highly nonlinear target distributions, arising in both real-world and synthetic examples. Code may be downloaded at https://github.com/karlnapf/kameleon-mcmc},
author = {Sejdinovic, Dino and Strathmann, Heiko and Garcia, Maria Lomeli and Andrieu, Christophe and Gretton, Arthur},
booktitle = {International Conference of Machine Learning},
keywords = {Computer Science - Learning,Statistics - Machine Learning},
title = {{Kernel Adaptive Metropolis-Hastings}},
year = {2014}
}
@techreport{SriFukKumGreHyv14,
author = {Sriperumbudur, B and Fukumizu, K and Kumar, R and Gretton, A and Hyv\"{a}rinen, A},
institution = {ArXiv e-prints},
number = {1312.3516},
title = {{Density Estimation in Infinite Dimensional Exponential Families}},
year = {2014}
}
