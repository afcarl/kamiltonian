#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\begin_preamble
\usepackage{nips14submit_e,times}
\usepackage{hyperref}
\usepackage{url}
%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09


\title{Kamiltonian Monte Carlo}


\author{
Heiko Strathmann%\thanks{ Use footnote for providing further information about author (webpage, alternative address)---\emph{not} for acknowledging funding agencies.}
\\
Gatsby Unit\\
University College London \\
\texttt{heiko.strathmann@gmail.com} \\
\And
Dino Sejdinovoc \\
Department of Statistics \\
University of Oxford \\
\texttt{dino.sejdinovic@gmail.com} \\
\AND
Arthur Gretton \\
Gatsby Unit\\
University College London \\
\texttt{athur.gretton@gmail.com} 
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\nipsfinalcopy % Uncomment for camera-ready version
\end_preamble
\use_default_options false
\begin_modules
theorems-std
\end_modules
\maintain_unincluded_children false
\language british
\language_package none
\inputencoding auto
\fontencoding default
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize letterpaper
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 0
\use_package mathtools 1
\use_package mhchem 0
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plainnat
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 2
\tocdepth 2
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\diag}{\text{diag}}
{\text{diag}}
\end_inset


\end_layout

\begin_layout Title
Kamiltonian Monte Carlo
\end_layout

\begin_layout Abstract
We propose an adaptive Kernel Hamiltonian Monte Carlo (KMC) algorithm to
 simulate from an arbritary target probability density.
 Our sampler fills a gap: when the density is intractable, classic HMC methodolo
gy 
\emph on
cannot
\emph default
 be applied -- one is left with random walk methods which suffer from bad
 mixing behaviour.
 We extend recent ideas of adaptively learning target covariance structure
 in a Reproducing Kernel Hilbert Space (RKHS), based on the history of the
 Markov chain.
 But rather than 
\emph on
locally 
\emph default
smoothing the chain history, we directly model its 
\emph on
global
\emph default
 log-density as an RKHS function via fitting a infinite exponential family
 model.
 This is not only a more interpretable and well-posed problem, but it also
 provides a differentiable log-density approximation, which can directly
 be used to simulate trajectories from a Hamiltonian dynamical system.
 We construct an 
\emph on
exact 
\emph default
MCMC algorithm that (i) behaves similar to HMC in terms of autocorrelation
 and acceptance rates, but that (ii) does not require target gradients,
 and that in particular (iii) works well in high dimensions.
 We support our claims with experimental studies.
\end_layout

\begin_layout Paragraph
Next steps (Heiko)
\end_layout

\begin_layout Itemize

\strikeout on
Proof read Gaussian kernel score matching
\end_layout

\begin_layout Itemize

\strikeout on
Implement carefully
\end_layout

\begin_layout Itemize

\strikeout on
Implement trajectories, possibly within a given framework
\end_layout

\begin_layout Itemize

\strikeout on
Create plots for illustration
\end_layout

\begin_layout Itemize

\strikeout on
Write framework to compute average acceptance probability of a trajectory
\end_layout

\begin_layout Itemize

\strikeout on
Write section that empirically analyses approximate trajectories on Gaussians
 and Banana in terms of dimension
\end_layout

\begin_layout Itemize
Random features
\end_layout

\begin_layout Itemize
Write section on optimising 
\begin_inset Formula $\lambda,\sigma$
\end_inset

 via x-validation or stochastic optimisation
\end_layout

\begin_layout Itemize
Translate finite sample convergence rates into all HMC quantities
\end_layout

\begin_layout Itemize

\strikeout on
Adapt notation and keywords from 
\begin_inset CommandInset citation
LatexCommand cite
key "Betancourt2015"

\end_inset

: Hamiltonian flow, level set
\end_layout

\begin_layout Itemize
Work over notation and unify the score matching with the HMC parts
\end_layout

\begin_layout Itemize
find out whats next
\end_layout

\begin_layout Paragraph*
Experiment & co
\end_layout

\begin_layout Itemize

\strikeout on
Write framework for doing trajectory comparisons on cluster
\end_layout

\begin_layout Itemize
Set up NUTS sampler for using pymc for kamiltonian and hamiltonian
\end_layout

\begin_layout Itemize
Set up sampling on cluster
\end_layout

\begin_layout Itemize
Banana quantile experiment, comparing against HMC and Kameleon
\end_layout

\begin_layout Itemize
GP experiment, comparing against Kameleon
\end_layout

\begin_layout Itemize
some new experiment?
\end_layout

\begin_layout Itemize

\strikeout on
Create surface plots for the x-validation of 
\begin_inset Formula $\lambda,\sigma$
\end_inset


\end_layout

\begin_layout Standard
Trajectory experiment on Gaussian.
 To illustrate that it maintains acceptance prob in high dimensions
\end_layout

\begin_layout Enumerate

\strikeout on
Increase 
\begin_inset Formula $d$
\end_inset

 for a fixed number of data.
 Should at some point fall apart
\end_layout

\begin_layout Enumerate
Increase number of data to show that 
\begin_inset Quotes eld
\end_inset

falling apart
\begin_inset Quotes erd
\end_inset

 happens later.
\end_layout

\begin_layout Enumerate
Translate to a guideline how many samples one needs to achieve similar performan
ce in high dimensions (exponential?)
\end_layout

\begin_layout Enumerate
Use low-rank Cholesky to exlore limitations of scaling (appendix: compare
 low-rank with exact)
\end_layout

\begin_layout Enumerate

\strikeout on
Sanity check: is covered volume roughly equal?
\end_layout

\begin_layout Standard
Quantiles on Gaussian/Banana.
 To illustrate that it continues to explore tails in high dimensions
\end_layout

\begin_layout Enumerate
Same as above, but with MCMC and NUTS
\end_layout

\begin_layout Standard
Need something on burnin time.
\end_layout

\begin_layout Standard

\series bold
Open questions:
\end_layout

\begin_layout Itemize
Is the value of the approximate hamiltonian bounded?
\end_layout

\begin_layout Itemize
Is the distance of the approximate hamiltonian and the true hamiltonian
 bounded? Sam mentioned that for leapfrog, the distance between true level
 set and discretised version is bounded.
 See Geometric Numerical Integration by Hairer and Lubich and reference
 is Neal's chapter 5
\end_layout

\begin_layout Itemize
Maximum distance of trajectory to Hamiltonian contour (2d plot), how does
 it behave?
\end_layout

\begin_layout Itemize
What do the tails of the desnity estimate do?
\end_layout

\begin_layout Itemize
Convergence rates of estimator translated into guarantees of the markov
 chain?
\end_layout

\begin_layout Itemize
Multimodality smoothing might be nice as an extension
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Itemize
Why MCMC
\end_layout

\begin_layout Itemize
Intractable densities
\end_layout

\begin_layout Itemize
Kernel Metropolis Hastings 
\begin_inset CommandInset citation
LatexCommand citep
key "sejdinovic_kernel_2014"

\end_inset

 and pros/cons
\end_layout

\begin_layout Itemize
Want: HMC like sampler
\end_layout

\begin_layout Itemize
Idea: Hamiltonian dynamics on kernel energy as proposal
\end_layout

\begin_layout Itemize
Give paper outline
\end_layout

\begin_layout Itemize
Here are some plots that might be useful
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
target "http://nbviewer.ipython.org/gist/anonymous/329d11916eb29743a7de"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Paragraph
Paper outline
\end_layout

\begin_layout Standard
Section
\end_layout

\begin_layout Section
Background & previous work
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "sec:previous_work"

\end_inset


\end_layout

\begin_layout Subsection
Notation
\end_layout

\begin_layout Itemize
Unnormalised target density 
\begin_inset Formula $\pi:\mathbb{R^{d}}\to\mathbb{R}$
\end_inset

 , unbiased estimator 
\begin_inset Formula $\mathbb{E}[\hat{\pi}(\theta)]=\pi(\theta)$
\end_inset


\end_layout

\begin_layout Itemize
Markov chain history at iteration 
\begin_inset Formula $t$
\end_inset

 with current position 
\begin_inset Formula $x_{t}$
\end_inset

: 
\begin_inset Formula $\left\{ x_{i}\right\} _{i=0}^{t-1}$
\end_inset


\end_layout

\begin_layout Itemize
History subsample 
\begin_inset Formula $\mathbf{z}=\left\{ z_{i}\right\} _{i=1}^{n}\subseteq\left\{ x_{i}\right\} _{i=0}^{t-1}$
\end_inset

 
\end_layout

\begin_layout Standard
HMC:
\end_layout

\begin_layout Itemize
Number of leapfrog steps, 
\begin_inset Formula $L$
\end_inset


\end_layout

\begin_layout Itemize
Step size 
\begin_inset Formula $\epsilon$
\end_inset


\end_layout

\begin_layout Subsection
Problem setting
\end_layout

\begin_layout Standard
Write what we want to do and what are important parts of it.
\end_layout

\begin_layout Subsection
Previous work
\end_layout

\begin_layout Standard
Write about Adaptive MCMC, 
\begin_inset CommandInset citation
LatexCommand cite
key "Haario1999"

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "Andrieu2008"

\end_inset

, Kameleon MCMC, 
\begin_inset CommandInset citation
LatexCommand citet
key "sejdinovic_kernel_2014"

\end_inset

, and how it improves over adaptive MH.
 Mention downsides and what we would like to have.
 In particular the pseudo-marginal case, 
\begin_inset CommandInset citation
LatexCommand cite
key "beaumont2003estimation"

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "Andrieu2009a"

\end_inset

.
\end_layout

\begin_layout Subsection
Hamiltonian Monte Carlo
\begin_inset Note Note
status open

\begin_layout Plain Layout
AG: we don't really need a separate section on mean embeddings
\end_layout

\end_inset


\end_layout

\begin_layout Standard
We follow the presentation in 
\begin_inset CommandInset citation
LatexCommand cite
key "neal2011mcmc"

\end_inset

, 
\begin_inset CommandInset citation
LatexCommand cite
key "Betancourt2015"

\end_inset

.
 Hamiltonian Monte Carlo (HMC) is based on a fictuous dynamical system and
 utilises deterministic, measure-preserving maps to generate efficient Markov
 transitions.
 Starting from the unnormalised target density of the form 
\begin_inset Formula $\pi(q)\propto\exp(-U(q))$
\end_inset

, refered to as 
\emph on
potential energy, 
\emph default
we define a distribution of auxiliary variables, refered to as 
\emph on
momentum
\emph default
 or 
\emph on
kinetic energy
\emph default
, distributed proportional to 
\begin_inset Formula $\exp(-K(p))$
\end_inset

.
 Note that both potential and momentum are real-valued.
 The joint distribution is given by
\begin_inset Formula 
\begin{align*}
\exp\left(-K(p)-U(q)\right) & \propto\exp\left(-H(p,q)\right)
\end{align*}

\end_inset

where we defined the (seperable)
\emph on
 Hamiltonian
\emph default
 
\begin_inset Formula $H(p,q):=K(p,q)+U(q)$
\end_inset

.
 The Hamiltonian defines the 
\emph on
Hamiltonian flow
\emph default
 on the joint 
\begin_inset Formula $(p,q)$
\end_inset

 space, parametrised in 
\begin_inset Formula $t\in\mathbb{R}$
\end_inset

, and is a map
\emph on

\begin_inset Formula 
\begin{align*}
\phi_{t}^{H}:(p,q) & \mapsto(p^{*},q^{*}),\forall t\in\mathbb{R}\\
\phi_{t}^{H}\circ\phi_{s}^{H} & =\phi_{s+t}^{H}
\end{align*}

\end_inset


\emph default
The flow preserves the joint distribution, i.e.
 for some 
\begin_inset Formula $(p,q)$
\end_inset

 and some 
\begin_inset Formula $t\in\mathbb{R}$
\end_inset

 
\begin_inset Formula 
\[
\exp\left(-H(p,q)\right)=\exp\left(-H(\phi_{t}^{H}(p,q)\right)
\]

\end_inset

which allows to construct of Markov chains.
 For a chain at state 
\begin_inset Formula $(p,q)$
\end_inset

, we repeatedly
\end_layout

\begin_layout Enumerate
re-sample the axuiliary momentum 
\begin_inset Formula $p'\sim\exp(-K(p))$
\end_inset


\end_layout

\begin_layout Enumerate
apply the Hamiltonian flow for time 
\begin_inset Formula $t$
\end_inset

, which gives 
\begin_inset Formula $(p^{*},q^{*})=\phi_{t}^{H}(p',q$
\end_inset

)
\end_layout

\begin_layout Standard
Note that both steps leave the joint distribution unchanged.
 As re-sampling the momentum is independent of 
\begin_inset Formula $q$
\end_inset

, the 
\begin_inset Formula $q$
\end_inset

-marginal of the above procedure forms a Markov chain with 
\begin_inset Formula $\pi$
\end_inset

 as its stationaly distribution.
 Trajectories of the Hamiltonian flow can be thought of as walking along
 the contour lines of the Hamiltonian 
\begin_inset Formula $\{\phi_{t}^{H}(p,q)\mid t\in\mathbb{R}\}$
\end_inset

, referred to as 
\emph on
level set
\emph default
.
 Consequently, re-sampling 
\begin_inset Formula $p$
\end_inset

 corresponds to 
\begin_inset Quotes eld
\end_inset

jumping
\begin_inset Quotes erd
\end_inset

 to another level set.
\end_layout

\begin_layout Standard
For seperable Hamiltonians, the flow over an interval 
\begin_inset Formula $\tau$
\end_inset

 can be generated by the 
\emph on
Hamiltonian operator 
\begin_inset Formula $\hat{H},$
\end_inset

 
\emph default

\begin_inset Formula 
\[
\phi_{\tau}^{H}=e^{\tau\hat{H}}
\]

\end_inset

where
\begin_inset Formula 
\begin{align}
\hat{H} & =\frac{\partial H}{\partial p}\frac{\partial}{\partial q}-\frac{\partial H}{\partial q}\frac{\partial}{\partial p}\nonumber \\
 & =\frac{\partial K}{\partial p}\frac{\partial}{\partial q}-\frac{\partial U}{\partial q}\frac{\partial}{\partial p}\nonumber \\
 & =:\hat{K}+\hat{U}\label{eq:potential_energy_operator}
\end{align}

\end_inset


\end_layout

\begin_layout Paragraph
Approximate integration of Hamiltonian flow
\end_layout

\begin_layout Standard
In practice, 
\begin_inset Formula $\hat{H}$
\end_inset

 is usually unavailable.
 Therefore, we need to resort to approximate integration schemes to apply
 the Hamiltonian flow.
 A typical choice are 
\emph on
symplectic integrators,
\emph default
 which produce discrete approximations that accurately track the original
 trajectories while preserving crucial properties such as reversability
 and volume preservation.
 In this work, we limit ourselves to the standard leap-frog integrator,
 see 
\begin_inset CommandInset citation
LatexCommand cite
key "neal2011mcmc"

\end_inset

 for details.
 For a desired approximation length 
\begin_inset Formula $\tau$
\end_inset

 and given a discretisation step size 
\begin_inset Formula $\epsilon$
\end_inset

, it is given by the 
\begin_inset Formula $L=\tau/\epsilon$
\end_inset

 times successive composition
\begin_inset Formula 
\begin{align}
\phi_{\epsilon,\tau}^{H}: & =\left(\phi_{\frac{\epsilon}{2}}^{U}\circ\phi_{\epsilon}^{K}\circ\phi_{\frac{\epsilon}{2}}^{U}\right)^{L}\label{eq:leap_frog_flow}\\
 & =e^{\tau\hat{H}}+{\cal O}(\epsilon^{2})\nonumber 
\end{align}

\end_inset

The second equality implies that such descrete trajectories correspond to
 a level set of a modified Hamiltonian within a 
\begin_inset Formula ${\cal O}(\epsilon^{2})$
\end_inset

 pertubation of the original one, see 
\begin_inset CommandInset citation
LatexCommand cite
key "leimkuhler2004"

\end_inset

, 
\begin_inset CommandInset citation
LatexCommand cite
key "hairer2006geometric"

\end_inset

.
 While this discretisation error is small in practice, it still induces
 a bias on the resulting Markov chain.
 In order to maintain asymptotic correctess, a Metropolis acceptance procedure
 can be applied.
 Starting from a 
\begin_inset Formula $(p',q)$
\end_inset

, this is achieved by accepting an end-point of the approximate trajectory
 with probability
\begin_inset Formula 
\begin{align}
\alpha(\phi_{\epsilon,\tau}^{H}(p',q)) & =\min\left[1,\exp\left(H\circ\phi_{\epsilon,\tau}^{H}(p',q)-H(p',q)\right)\right].\label{eq:hmc_accept_prob}
\end{align}

\end_inset

It is clear that 
\begin_inset Formula $\lim_{\epsilon\to0}H\circ\phi_{\epsilon,\tau}^{H}(p',q)-H(p',q)=0$
\end_inset

 and the above acceptance probability tends to 
\begin_inset Formula $1$
\end_inset

 as the stepsize tends to 
\begin_inset Formula $0$
\end_inset

.
 This makes HMC a powerful sampling algorithm, which is able to propose
 distant, uncorrelated moves with a high acceptance probability.
\end_layout

\begin_layout Paragraph
HMC and intractable densities
\end_layout

\begin_layout Standard
In our case, the the gradient of 
\begin_inset Formula $\log\pi(q)=\text{const}-U(q)$
\end_inset

 and therefore the potential energy operator 
\begin_inset Formula 
\[
\hat{U}=-\frac{\partial U}{\partial q}\frac{\partial}{\partial p}
\]

\end_inset

is assumed to be unavailable
\begin_inset Foot
status open

\begin_layout Plain Layout
Unavailable due to analytic intractability, as opposed to computationally
 expensive in large 
\begin_inset Formula $n$
\end_inset

 cases.
 
\begin_inset Formula $H(p,q)$
\end_inset

 itself is available.
\end_layout

\end_inset

.
 A typical example of this scenario is the pseudo-marignal case, 
\begin_inset CommandInset citation
LatexCommand cite
key "beaumont2003estimation"

\end_inset

,
\begin_inset CommandInset citation
LatexCommand cite
key "Andrieu2009a"

\end_inset

, for which usually only adaptive random walk MCMC algorithms are available,
 see for example 
\begin_inset CommandInset citation
LatexCommand cite
key "FilipponeIEEETPAMI13"

\end_inset

.
 While (kernel) adaptive Metropolis-Hastings can be used to generate more
 efficient proposals, 
\begin_inset CommandInset citation
LatexCommand cite
key "sejdinovic_kernel_2014"

\end_inset

,
\begin_inset CommandInset citation
LatexCommand cite
key "Andrieu2008"

\end_inset

, these still suffer from random walk behaviour.
 It is well known that overcoming the latter results in significantly more
 efficeint sampling, 
\begin_inset CommandInset citation
LatexCommand cite
key "neal2011mcmc"

\end_inset

.
\end_layout

\begin_layout Standard
Kamiltonian Monte Carlo is based on the idea of replacing the potential
 energy operator 
\begin_inset Formula $\hat{U}$
\end_inset

 in  
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:potential_energy_operator"

\end_inset

 by a kernel induced surrogate computed from the history of the Markov chain.
 As we will see, the surrogate will be tractable and while not requiring
 gradients of the log-target density.
 It induces a kernel Hamiltonian flow, which can be numerically integrated
 using the integrator in  
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:leap_frog_flow"

\end_inset

.
 As the Hamiltonian 
\begin_inset Formula $H(p,q)$
\end_inset

 itself is 
\emph on
not
\emph default
 changed, any deviation of the alternative kernel level set from the true
 level set is corrected for via the Metropolis acceptance procedure in 
 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:hmc_accept_prob"

\end_inset

.
 Consequently, the stationary distribution of the Markov chain will 
\emph on
remain correct
\emph default

\begin_inset Foot
status open

\begin_layout Plain Layout
As usually when constructing adaptive MCMC algorithms, we will need to take
 care generating proposals based on the history of the Markov chain.
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
We now describe a technique to obtain a kernel induced potential energy
 surrogate, via fitting an infinite dimensional exponential family model
 using score matching.
\end_layout

\begin_layout Section
Kernel Hamiltonian dynamics
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "sec:kernel_hmc"

\end_inset


\end_layout

\begin_layout Standard
Recall our aim is to replace the potential energy operator 
\begin_inset Formula $\hat{U}$
\end_inset

 in  
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:potential_energy_operator"

\end_inset

 by a kernel induced surraget that is tractable and does not require gradients
 of the log-target.
 To that end, we fit an infinite exponential family model,
\begin_inset CommandInset citation
LatexCommand cite
key "SriFukKumGreHyv14"

\end_inset

, of the form
\begin_inset Formula 
\begin{equation}
\exp\left(\langle f,k(x,\cdot)\rangle_{{\cal H}}-A(f)\right).\label{eq:infinite_exp_family}
\end{equation}

\end_inset

Here, 
\begin_inset Formula ${\cal H}$
\end_inset

 is a reproducing kernel Hilbert space (RKHS) of real valued functions on
 
\begin_inset Formula ${\cal X}$
\end_inset

.
 It has a uniquely associated symmetric, positive definite function (
\emph on
kernel
\emph default
) 
\begin_inset Formula $k:{\cal X}\times{\cal X}\rightarrow\mathbb{R}$
\end_inset

, which satisfies 
\begin_inset Formula $f(x)=\langle f,k(x,\cdot)\rangle$
\end_inset

 for any 
\begin_inset Formula $f\in{\cal H}$
\end_inset

, 
\begin_inset CommandInset citation
LatexCommand cite
key "BerTho04"

\end_inset

.
 The cannonical feature map 
\begin_inset Formula $\varphi\in{\cal H}$
\end_inset

 defined as 
\begin_inset Formula $\varphi:\mathcal{X}\to\mathcal{H},\varphi:x\mapsto k(\cdot,x)$
\end_inset

 here takes the role of the 
\emph on
sufficient statistics
\emph default
 and 
\begin_inset Formula $f\in{\cal H}$
\end_inset

 are the 
\emph on
natural parameters
\emph default
.
 
\begin_inset Formula $A(f):=\log\int_{{\cal X}}\exp(\langle f,k(x,\cdot)\rangle_{{\cal H}})dx$
\end_inset

 is the cimulant generating function.
  
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:infinite_exp_family"

\end_inset

 defines a very rich class of probability distributions and a broad class
 of denisties (for example continuous densities defined on compact domains)
 can be approximated arbritarily well.
 It was shown in 
\begin_inset CommandInset citation
LatexCommand cite
key "SriFukKumGreHyv14"

\end_inset

 that, under mild conditions, it is possible to consistently fit an 
\emph on
unnormalised
\emph default
 version of  
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:infinite_exp_family"

\end_inset

 using score matching, and convergences rates in Kullback-Leibler divergence,
 Hellinger and total-variation distances were established.
 Furthermore, experimental results suggest good performance in high dimensions,
 as for example opposed to kernel density estimation.
\end_layout

\begin_layout Standard
The proposed empirical estimator to 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:infinite_exp_family"

\end_inset

 in 
\begin_inset CommandInset citation
LatexCommand cite
key "SriFukKumGreHyv14"

\end_inset

 requires to invert an 
\begin_inset Formula $(n\times d)^{2}$
\end_inset

 matrix, where 
\begin_inset Formula $n$
\end_inset

 is the number of 
\begin_inset Formula $d$
\end_inset

-dimensional data, resulting in computational costs of 
\begin_inset Formula ${\cal O}(n^{6}d^{6})$
\end_inset

.
 As we are eventually interested in constructing an adaptive MCMC sampler
 that uses 
\emph on
all
\emph default
 of the history of the Markov chain to guide proposals, a cost scaling as
 a sixth order polynomial is infeasible.
 We therefore will develop an approximate estimator that, after a 
\emph on
linear 
\emph default
one-off cost, uses rank-one updates whose costs are 
\emph on
constant 
\emph default
in 
\begin_inset Formula $n$
\end_inset

.
\end_layout

\begin_layout Standard
We begin by giving a brief review of score matching.
\end_layout

\begin_layout Subsection
Score Matching
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout

\series bold
AG:
\series default
 Score matching references, 
\begin_inset CommandInset citation
LatexCommand citet
key "Hyvarinen-05,Hyvarinen-07"

\end_inset

.
 The latter gave solutions in terms of a finite mixture at fixed centres.
 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Following 
\begin_inset CommandInset citation
LatexCommand citet
key "Hyvarinen-05"

\end_inset

 (using a slightly different notation), we assume that a variable 
\begin_inset Formula $\xi\sim\pi$
\end_inset

 has some unknown probability density function 
\begin_inset Formula $\pi(\cdot)$
\end_inset

 defined on 
\begin_inset Formula $\mathbb{R}^{d}$
\end_inset

.
 We model the log probability with a parametric model of the form 
\begin_inset Formula 
\begin{equation}
\log\tilde{\pi}(\xi;\theta)=\log q(\xi;\theta)-\log Z(f),\label{eq:score_matching_parametric_model}
\end{equation}

\end_inset

 where 
\begin_inset Formula $f$
\end_inset

 is a vector of parameters of yet unspecified dimension (c.f.
 natural parameters 
\begin_inset Formula $f$
\end_inset

 of 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:infinite_exp_family"

\end_inset

), and 
\begin_inset Formula $Z(\theta)$
\end_inset

 an unknown normalising constant.
 We aim to approximate 
\begin_inset Formula $\pi(\cdot)$
\end_inset

 by 
\begin_inset Formula $\tilde{\pi}(\cdot;\hat{\theta})$
\end_inset

, i.e., to find 
\begin_inset Formula $\hat{\theta}$
\end_inset

 from a set of fixed i.i.d.
 samples 
\begin_inset Formula $\{x_{i}\sim\pi\}_{i=1}^{n}$
\end_inset

 -- 
\emph on
without 
\emph default
estimating 
\begin_inset Formula $Z(\theta)$
\end_inset

.
 From 
\begin_inset CommandInset citation
LatexCommand citet
after "equation. 2"
key "Hyvarinen-05"

\end_inset

, the criterion being optimized is the expected squared distance between
 score functions,
\emph on
 
\emph default

\begin_inset Formula 
\[
J(\theta)=\frac{1}{2}\int_{\xi}\pi(\xi)\left\Vert \psi(\xi;\theta)-\psi_{\pi}(\xi)\right\Vert ^{2}d\xi,
\]

\end_inset

where 
\begin_inset Formula 
\[
\psi(\xi;\theta)=\nabla_{\xi}\log\tilde{\pi}(\xi;\theta),
\]

\end_inset

and 
\begin_inset Formula $\psi_{\pi}(\xi)$
\end_inset

 is the derivative wrt 
\begin_inset Formula $\xi$
\end_inset

 of the unknown true density 
\begin_inset Formula $\pi(\xi)$
\end_inset

.
 As proved in 
\begin_inset CommandInset citation
LatexCommand citet
after "Theorem 1"
key "Hyvarinen-05"

\end_inset

 using partial integration, it is possible to express 
\begin_inset Formula $J(f)$
\end_inset

 without access to the unknown 
\begin_inset Formula $\psi_{\pi}(\xi)$
\end_inset

 as
\begin_inset Formula 
\[
J(\theta)=\int_{\xi}\pi(\xi)\sum_{\ell=1}^{d}\left[\partial_{\ell}\psi_{\ell}(\xi;\theta)+\frac{1}{2}\psi_{\ell}(\xi;\theta)^{2}\right]d\xi,
\]

\end_inset

where
\begin_inset Formula 
\begin{equation}
\psi_{\ell}(\xi;\theta)=\frac{\partial\log q(\xi;\theta)}{\partial\xi_{\ell}}\label{eq:score_match_first_score}
\end{equation}

\end_inset

and
\begin_inset Formula 
\begin{equation}
\partial_{\ell}\psi_{\ell}(\xi;\theta)=\frac{\partial^{2}\log q(\xi;\theta)}{\partial\xi_{\ell}^{2}}.\label{eq:score_match_second_score}
\end{equation}

\end_inset

Replacing the integral 
\begin_inset Formula $\int_{\xi}\pi(\xi)$
\end_inset

 with an average over the samples 
\begin_inset Formula $x_{i}$
\end_inset

 gives us a sample version of 
\begin_inset Formula $J(\theta)$
\end_inset

, minimising of which is consistent,
\begin_inset Formula 
\begin{equation}
\hat{J}(\theta)=\frac{1}{m}\sum_{i=1}^{n}\sum_{\ell=1}^{d}\left[\partial_{\ell}\psi_{\ell}(x_{i};\theta)+\frac{1}{2}\psi_{\ell}^{2}(x_{i};\theta)\right].\label{eq:score_matching_objective_sample_version}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
We next apply the score matching estimator in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:score_matching_objective_sample_version"

\end_inset

 to estimate the parameters of the infinite exponential family model in
 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:infinite_exp_family"

\end_inset

.
 But rather than following the 
\begin_inset Formula ${\cal O}(n^{6}d^{6})$
\end_inset

 approach from 
\begin_inset CommandInset citation
LatexCommand cite
key "SriFukKumGreHyv14"

\end_inset

, we use an approximate feature space version with a finite set of random
 set of basis functions.
\end_layout

\begin_layout Subsection
Random Fourier features for score matching
\end_layout

\begin_layout Standard
Estimating 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:infinite_exp_family"

\end_inset

 requires 
\begin_inset Formula ${\cal O}(n^{6}d^{6})$
\end_inset

 computation and 
\begin_inset Formula ${\cal O}(n^{2}d^{2})$
\end_inset

 storage.
 This is clearly limiting, in particular if training samples comes from
 an increasingly long Markov chain.
 
\begin_inset CommandInset citation
LatexCommand cite
key "sejdinovic_kernel_2014"

\end_inset

, whose computational costs are 
\begin_inset Formula ${\cal O}(nd^{2})$
\end_inset

 side stepped the problem of increasing computation via repeatedly updating
 a random subsample of fixed size from the Markov chain history, and recomputing
 their proposal mechanism.
 In this section, we take a different approach: indead of fitting an exact
 infinite dimensional model on a subset of the available data, we fit an
 approximation to the infinite model to 
\emph on
all
\emph default
 available data.
 To that end, we construct an approximate estimator of 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:infinite_exp_family"

\end_inset

, whose computational costs remain constant as 
\begin_inset Formula $n\to\infty$
\end_inset

.
\end_layout

\begin_layout Standard
We follow the framework developed by 
\begin_inset CommandInset citation
LatexCommand cite
key "Rahimi2007"

\end_inset

.
 Assume a translation invariant kernel 
\begin_inset Formula $k(x,y)=\tilde{k}(x-y)$
\end_inset

.
 Bochners theorem gives the representation
\begin_inset Formula 
\[
k(x,y)=\tilde{k}(x-y)=\int_{\mathbb{R}^{d}}\exp\left(i\omega^{T}(x-y)\right)d\Gamma(\omega),
\]

\end_inset

where 
\begin_inset Formula $\Gamma$
\end_inset

 is the Fourier transform of the kernel.
 One can define an 
\begin_inset Formula $m$
\end_inset

-dimensional approximate feature space 
\begin_inset Formula ${\cal H}_{m}=\mathbb{R}^{m}$
\end_inset

 with a (fixed) set of random basis vectors 
\begin_inset Formula $\omega_{i}\sim\Gamma$
\end_inset

.
 Denote by 
\begin_inset Formula $\phi_{x}\in\mathbb{R}^{m}$
\end_inset

 the embedding of a point 
\begin_inset Formula $x\in{\cal X}=\mathbb{R^{d}}$
\end_inset

 into 
\begin_inset Formula ${\cal H}_{m}=\mathbb{R}^{m}$
\end_inset

.
 We then can approximate the kernel function using the finite rank expansion
 
\begin_inset Formula $K_{ij}:=k(z_{i},z_{j})\approx\phi_{i}^{T}\phi_{j}$
\end_inset

.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Rahimi2007"

\end_inset

 established consistency, finite sample convergence rates, and good performance
 in practice.
 
\end_layout

\begin_layout Subsubsection
Finite dimensional approximate infinite exponential family estimators
\end_layout

\begin_layout Standard
The infinite dimensional exponential family form in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:infinite_exp_family"

\end_inset

 can be expressed in this approximate feature space via replacing natural
 parameters 
\begin_inset Formula $f\in{\cal H}$
\end_inset

 by 
\begin_inset Formula $\theta\in{\cal H}_{m}$
\end_inset

 and sufficient statistics 
\begin_inset Formula $k(x,\cdot)\in{\cal H}$
\end_inset

 by 
\begin_inset Formula $\phi_{x}\in{\cal H}_{m}$
\end_inset

 as
\begin_inset Formula 
\begin{align*}
\exp\left(\langle f,k(x,\cdot)\rangle_{{\cal H}}-A(f)\right) & \approx\exp\left(\langle\theta,k(x,\cdot)\rangle_{{\cal H}_{m}}-A(f)\right)\\
 & =\exp\left(\theta^{T}\phi_{x}-A(f)\right)
\end{align*}

\end_inset

Via setting the unnormalised log-density of the parametric form in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:score_matching_parametric_model"

\end_inset

 to 
\begin_inset Formula $\log q(\xi;\theta)=\theta^{T}\phi_{x}$
\end_inset

, we obtain an approximate infinite exponential family model of the target
 density 
\begin_inset Formula $\pi$
\end_inset

.
 In order to fit, we need to compute the score matching functions in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:score_match_first_score"

\end_inset

,
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:score_match_second_score"

\end_inset

, which have the simple form
\begin_inset Formula 
\begin{align*}
\psi_{\ell}(\xi;\theta) & =\theta^{T}\dot{\phi}_{x}^{\ell}\quad\text{and}\quad\partial_{l}\psi_{\ell}(\xi;\theta)=\theta^{T}\ddot{\phi}_{x}^{\ell},
\end{align*}

\end_inset

where we defined the 
\begin_inset Formula $m$
\end_inset

-dimensional feature vector derivatives 
\begin_inset Formula $\dot{\phi}_{\xi}^{\ell}:=\frac{\partial}{\partial\xi_{\ell}}\phi_{\xi}$
\end_inset

 and 
\begin_inset Formula $\ddot{\phi}_{\xi}^{\ell}:=\frac{\partial^{2}}{\partial\xi_{\ell}^{2}}\phi_{\xi}$
\end_inset

.
 Plugging those into the empirical score matching objective in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:score_matching_objective_sample_version"

\end_inset

, we arrive at
\begin_inset Formula 
\begin{align*}
J(\theta) & =\frac{1}{n}\sum_{i=1}^{n}\sum_{\ell=1}^{d}\left[\partial_{\ell}\psi_{\ell}(x_{i};\theta)+\frac{1}{2}\psi_{\ell}^{2}(x_{i};\theta)\right]\\
 & =\frac{1}{n}\sum_{i=1}^{n}\sum_{\ell=1}^{d}\left[\theta^{T}\ddot{\phi}_{x_{i}}^{\ell}+\frac{1}{2}\theta^{T}\left(\ddot{\phi}_{\xi}^{\ell}\ddot{\phi}_{\xi}^{\ell T}\right)\theta\right]\\
 & =\theta^{T}C\theta-\theta^{T}b
\end{align*}

\end_inset

 where
\begin_inset Formula 
\[
b:=-\frac{1}{n}\sum_{i=1}^{n}\sum_{\ell=1}^{d}\ddot{\phi}_{x_{i}}^{\ell}\in\mathbb{R}^{m}\quad\text{and}\quad C:=\frac{1}{2n}\sum_{i=1}^{n}\sum_{\ell=1}^{d}\left(\dot{\phi}_{x_{i}}^{\ell}\dot{\phi}_{x_{i}}^{\ell}{}^{T}\right)\in\mathbb{R}^{m\times m}.
\]

\end_inset

Assuming 
\begin_inset Formula $C$
\end_inset

 is invertible (trivial for 
\begin_inset Formula $n\geq m$
\end_inset

), the objective is uniquely minimised by
\begin_inset Formula 
\[
\hat{\theta}=C^{-1}b.
\]

\end_inset

 
\end_layout

\begin_layout Subsubsection
Example: Gaussian kernel
\end_layout

\begin_layout Standard
The approximate feature mapping for the well known Gaussian kernel
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
k(x,y)={\cal \exp}\left(\frac{\Vert x-y\Vert_{2}^{2}}{2\sigma^{2}}\right)
\]

\end_inset

is given by
\begin_inset Formula 
\[
\phi_{x}=\left[\cos(\omega_{1}^{T}x+u_{1}),\dots,\cos(\omega_{m}^{T}x+u_{m})\right],
\]

\end_inset

with random basis vectors
\begin_inset Formula 
\begin{align*}
w_{i}\sim{\cal N}\left(w_{i}\Big\vert\mathbf{0},\frac{1}{2}\sigma^{-2}I_{m}\right),
\end{align*}

\end_inset

and random offsets 
\begin_inset Formula 
\[
u_{i}\sim\texttt{Uniform}[0,2\pi]
\]

\end_inset

for 
\begin_inset Formula $i=1\dots m$
\end_inset

.
\end_layout

\begin_layout Subsubsection
Rank-one updates
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "seeger2004low"

\end_inset

 and 
\begin_inset Flex URL
status open

\begin_layout Plain Layout

https://github.com/jcrudy/choldate
\end_layout

\end_inset

 write how 
\begin_inset Formula $C^{-1}$
\end_inset

 can be updated if a new point 
\begin_inset Formula $x_{i}$
\end_inset

 arrives.
\end_layout

\begin_layout Subsection
Kernel induced Hamiltonian flow
\end_layout

\begin_layout Standard
We now combine the (approximate) infinite exponential family estimator with
 Hamiltonian dynamics to define a 
\emph on
kernel induced Hamiltonian flow
\emph default
.
 More precisely, we define a kernel induced Hamiltonian operator 
\begin_inset Formula $\hat{H}_{k}=\hat{K}+\hat{U}_{k}$
\end_inset

, where 
\begin_inset Formula $\hat{K}$
\end_inset

 is defined as in  
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:potential_energy_operator"

\end_inset

, and 
\begin_inset Formula 
\begin{equation}
\hat{U}_{k}=\frac{\partial U_{k}}{\partial p}\frac{\partial}{\partial q}\label{eq:kernel_potential_energy_operator}
\end{equation}

\end_inset

with 
\begin_inset Formula $\nabla U_{k}=\nabla\log q(\xi;\alpha)$
\end_inset

 being the gradient of the fitted infinite exponential family model from
 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:density_estimate_gradient"

\end_inset

.
 Using the described numerical leap-frog integration, the approximate kernel
 induced Hamiltonian flow then follows similar to  
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:leap_frog_flow"

\end_inset

, 
\begin_inset Formula 
\begin{equation}
\phi_{\epsilon,\tau}^{\tilde{H}_{k}}:=\left(\phi_{\frac{\epsilon}{2}}^{U_{k}}\circ\phi_{\epsilon}^{K}\circ\phi_{\frac{\epsilon}{2}}^{U_{k}}\right)^{L}\label{eq:kernel_leap_frog_flow}
\end{equation}

\end_inset

It is clear that the kernel induced potential energy operator 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:kernel_potential_energy_operator"

\end_inset

 results different level sets than those induced by the discretised true
 Hamiltonian flow 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:potential_energy_operator"

\end_inset

.
 In order to correct for any induced bias on the resulting Markov chain,
 we perform the Metropolis acceptance step from  
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:hmc_accept_prob"

\end_inset

.
 We propose the end-point of an approximate integration of the kernel induced
 Hamiltonian flow, and accept according to the value of the 
\emph on
true
\emph default
 Hamiltonian,
\begin_inset Formula 
\begin{equation}
\alpha(\phi_{\epsilon,\tau}^{\tilde{H}_{k}}(p',q))=\min\left[1,\exp\left(H\circ\phi_{\epsilon,\tau}^{\tilde{H}_{k}}(p',q)-H(p',q)\right)\right].\label{eq:kmc_accept_prob}
\end{equation}

\end_inset

Consequently, any deviations of the level sets induced by 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:kernel_leap_frog_flow"

\end_inset

 from those induced by 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:leap_frog_flow"

\end_inset

 will result in a decreased acceptance probability of approximate integrations
 
\begin_inset Formula $\alpha(\phi_{\epsilon,\tau}^{\tilde{H_{k}}}(p',q))$
\end_inset

.
 We therefore need to controll the approximation quality of the kernel induced
 potential energy to maintain good performance in practice.
 At this point, it is unclear, whether and how finite sample rates for estimatin
g infinite exponential family models, 
\begin_inset CommandInset citation
LatexCommand cite
key "SriFukKumGreHyv14"

\end_inset

, can be translated to (bound?) the difference in acceptance probabilities
 
\begin_inset Formula 
\[
\vert\alpha(\phi_{\epsilon,\tau}^{\tilde{H}_{k}}(p',q))-\alpha(\phi_{\epsilon,\tau}^{\tilde{H}}(p',q))\vert
\]

\end_inset

Further complication comes from the approximation to the solution of the
 density estimator described in section 
\begin_inset CommandInset ref
LatexCommand eqref
reference "sub:exp_family_lite"

\end_inset

.
 Consistency here is does not really help as gradient subsamples in the
 sense of 
\begin_inset CommandInset citation
LatexCommand cite
key "ChenICML2014"

\end_inset

 are consistent but are shown to exhibit catastrophic performance as demonstrate
d in 
\begin_inset CommandInset citation
LatexCommand cite
key "Betancourt2015"

\end_inset

.
 However, we provide a thorough numerical study in the experiments.
\end_layout

\begin_layout Paragraph
An illustrative example
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "par:illustrative_example"

\end_inset


\end_layout

\begin_layout Standard
We now illustrate how well trajectories along the level set of the kernel
 induced Hamiltonian flow are aligned with their true counterparts.
 We begin with a simple toy example of a Hamiltonian 
\begin_inset Formula $H(p,q)=K(p)+U(q)$
\end_inset

 defined by standard Gaussian both momentum 
\begin_inset Formula $K(p)=-\frac{1}{2}p^{T}p$
\end_inset

 target density, i.e.
 
\begin_inset Formula $U(q)=\frac{1}{2}q^{T}q$
\end_inset

.
 We fit the approximate infinite exponential family model using 
\begin_inset Formula $500$
\end_inset

 points with 
\begin_inset Formula $\lambda=1$
\end_inset

 and a Gaussian kernel with 
\begin_inset Formula $\sigma=500$
\end_inset

.
 Starting from a point in 
\begin_inset Formula $(p,q)$
\end_inset

-space, we simulate approximate trajectories on both true and kernel induced
 Hamiltonian level sets, using 
\begin_inset Formula $L=300$
\end_inset

 leapfrog steps of size 
\begin_inset Formula $\epsilon=0.1$
\end_inset

.
 For each point on the discrete trajectory, we compute the Metropolis acceptance
 probability in  
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:hmc_accept_prob"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:kmc_accept_prob"

\end_inset

.
 See Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:kmc_trajectories"

\end_inset

 for an example trajectory.
 For plain HMC, the average acceptance probability quantifies how well the
 leap-frog integration of the level sets align with the true level set --
 and is usually large.
 For kernel induced Hamiltonian dynamics, the average acceptance probability
 quantifies both the integration error 
\emph on
and
\emph default
 how well the kernel induced Hamiltonian flow matches the true Hamiltonian
 flow -- where we are interested in the latter and will investigate it in
 the experiments.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/gaussian_trajectories_hmc.eps

\end_inset


\begin_inset Graphics
	filename figures/gaussian_trajectories_kmc.eps

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/gaussian_trajectories_momentum_hmc.eps

\end_inset


\begin_inset Graphics
	filename figures/gaussian_trajectories_momentum_kmc.eps

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/gaussian_trajectories_acceptance_hmc.eps

\end_inset


\begin_inset Graphics
	filename figures/gaussian_trajectories_acceptance_kmc.eps

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:kmc_trajectories"

\end_inset

Hamiltonian trajectories on 2-dimensional standard Gaussian both target
 and momentum.
 Starting (red stars) at 
\begin_inset Formula $q_{0}=(0,-3)^{T}$
\end_inset

, we randomly sample momentum and simulate for 
\begin_inset Formula $L=300$
\end_inset

 leap-frog steps of size 
\begin_inset Formula $\epsilon=0.1$
\end_inset

.
 End points of such trajectories (blue stars) form the proposal of HMC-like
 algorithms.
 The acceptance probability is reported on the bottom.
 
\series bold
Left:
\series default
 Plain Hamiltonian trajectories oscilate on a stable orbit.
 Acceptance prbability according to 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:kmc_accept_prob"

\end_inset

 
\series bold
Right:
\series default
 Kernel induced trajectories on an empirical energy function estimated from
 
\begin_inset Formula $N=200$
\end_inset

 samples (blue points) with 
\begin_inset Formula $\lambda=1$
\end_inset

 and a Gaussian kernel with 
\begin_inset Formula $\sigma=500$
\end_inset

.
 Acceptance prbability according to 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:kmc_accept_prob"

\end_inset

.
 The trajectories slightly drift compared to the true Hamiltonian flow.
 The acceptance probability oscilates in a larger interval and its average
 is slightly lower.
 Quantification of this difference is crucial for kernel induced Hamiltonian
 dynamics being useful in practice.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Geometry extension (TODO)
\end_layout

\begin_layout Itemize
Can we use second order information of our density estimate? Then run RM-HMC
 or M-MALA
\end_layout

\begin_layout Itemize
What about the Kameleon proposal as a Hessian estimate? Formal relationship?
 In practice
\end_layout

\begin_layout Section
Kamiltonian Monte Carlo
\end_layout

\begin_layout Standard
We now describe how to utilise the described kernel induced Hamiltonian
 flow in order to construct a gradient free, adaptive kernel Hamiltonian
 Monte Carlo algorithm: 
\emph on
Kamiltonian Monte Carlo, 
\emph default
see Algorithm 
\begin_inset CommandInset ref
LatexCommand eqref
reference "alg:Kamiltonian-Monte-Carlo"

\end_inset

.
\end_layout

\begin_layout Standard
The remainder of this section covers details on adaptive proposal distributions
 needed to ensure both anyptotic correctness and computational feasability,
 and describe various tuning mechanisms for applying KMC in practice.
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
placement H
wide false
sideways false
status open

\begin_layout Plain Layout

\series bold
\begin_inset Caption Standard

\begin_layout Plain Layout

\series bold
Kamiltonian Monte Carlo
\begin_inset CommandInset label
LatexCommand label
name "alg:Kamiltonian-Monte-Carlo"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\emph on
Input
\emph default
: unnormalized target 
\begin_inset Formula $\pi$
\end_inset

, momentum 
\begin_inset Formula $\exp(-K(p))$
\end_inset

, adaptation schedule 
\begin_inset Formula $p_{t}$
\end_inset

, kernel width 
\begin_inset Formula $\sigma$
\end_inset

 
\end_layout

\begin_layout Itemize
At iteration 
\begin_inset Formula $t+1$
\end_inset

, previous state 
\begin_inset Formula $x_{t}$
\end_inset


\end_layout

\begin_layout Itemize

\shape italic
Update density estimate.
 
\shape default
With probability 
\begin_inset Formula $p_{t}$
\end_inset

,
\end_layout

\begin_deeper
\begin_layout Enumerate
Draw a random subsample 
\begin_inset Formula $\mathbf{z}=\left\{ z_{i}\right\} _{i=1}^{n}$
\end_inset

 of the chain history 
\begin_inset Formula $\left\{ x_{i}\right\} _{i=0}^{t-1}$
\end_inset


\end_layout

\begin_layout Enumerate
Update density estimator, i.e.
 solve
\begin_inset Formula 
\[
\hat{\alpha}=\frac{-\sigma}{2}(C+\lambda(K+I))^{-1}b
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize

\shape italic
Kernel induced Hamiltonian proposal.
\end_layout

\begin_deeper
\begin_layout Enumerate
\begin_inset ERT
status open

\begin_layout Plain Layout

[5.]
\backslash
setcounter{enumi}{3}
\end_layout

\end_inset

Sample momentum 
\begin_inset Formula $p'\sim\exp(-K(p))$
\end_inset


\end_layout

\begin_layout Enumerate
Using 
\begin_inset Formula $\hat{\alpha}$
\end_inset

, simulate kernel Hamiltonian flow 
\begin_inset Formula $(p^{*},x^{*})=\phi_{\epsilon,\tau}^{\tilde{H}_{k}}(p',x_{t})$
\end_inset


\end_layout

\begin_layout Enumerate
Perform Metropolis step 
\begin_inset Formula 
\begin{align*}
x_{t+1} & =\begin{cases}
x^{*} & \text{with probability }\alpha(\phi_{\epsilon,\tau}^{\tilde{H}_{k}}(p',x_{t}))\\
x_{t} & \text{otherwise}
\end{cases}
\end{align*}

\end_inset

 
\end_layout

\end_deeper
\end_inset


\end_layout

\begin_layout Subsection
Constructing a feasible adaptive MCMC proposal
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "sub:adaptive_subsampling"

\end_inset


\end_layout

\begin_layout Standard
It is well known that construction of MCMC algorithms that utilise the history
 of the Markov chain for constructing proposals yields asymptotic bias.
 We follow 
\begin_inset CommandInset citation
LatexCommand cite
key "sejdinovic_kernel_2014"

\end_inset

, who adapted the idea of 
\begin_inset Quotes eld
\end_inset

vanishing adaptation
\begin_inset Quotes erd
\end_inset

, 
\begin_inset CommandInset citation
LatexCommand cite
key "Andrieu2008"

\end_inset

, to avoid such biases.
 They update random subsamples of the Markov chain history with a decaying
 probability.
 The approach also has the advantage that it limits 
\begin_inset Formula $n$
\end_inset

 -- computational costs will remain constant as the length of the Markov
 chain approaches infinity.
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $\left\{ x_{i}\right\} _{i=0}^{t-1}$
\end_inset

 be the history of the Markov chain at iteration 
\begin_inset Formula $t$
\end_inset

.
 To guarantee convergence to the correct stationary distirbution, we introduce
 adaptation probabilities 
\begin_inset Formula $\left\{ p_{t}\right\} _{i=0}^{\infty}$
\end_inset

 such that 
\begin_inset Formula 
\[
\lim_{t\to\infty}p_{t}=0\quad\text{and}\quad\sum_{t=0}^{\infty}=\infty.
\]

\end_inset

At iteration 
\begin_inset Formula $t>n$
\end_inset

, we update the current subsample 
\begin_inset Formula $\mathbf{z}=\left\{ z_{i}\right\} _{i=1}^{n}$
\end_inset

 of the history, and construct our kernel induced Hamiltonian flow based
 proposal with 
\begin_inset Formula $\mathbf{z}$
\end_inset

.
 Intuitively, the probability for updating the chain history to the latest
 state gets arbritarily small, while the scheme never stops updating 
\begin_inset Formula $\mathbf{z}$
\end_inset

.
 
\begin_inset CommandInset citation
LatexCommand cite
after "Theorem 1"
key "RobertsRosenthal2007"

\end_inset

 guarantees that the resulting Markov kernel is ergodic and converges to
 the correct target.
\end_layout

\begin_layout Subsection
Learning free parameters
\end_layout

\begin_layout Standard
KMC has two free parameters, the Gaussian kernel bandwith 
\begin_inset Formula $\sigma$
\end_inset

, and the regularisation parameter 
\begin_inset Formula $\lambda$
\end_inset

.
 Earlier adaptive kernel-based MCMC methods, 
\begin_inset CommandInset citation
LatexCommand cite
key "sejdinovic_kernel_2014"

\end_inset

, did not cover chover choice of kernel parameters, i.e.
 the Gaussian bandwidth 
\begin_inset Formula $\sigma$
\end_inset

 in our case.
 As the choice of such parameters ius crucial to obtain an efficient sampler,
 this clearly limits its generality.
 While in practice, pilot runs can to be devised in order to tune the parameter,
 a general framework to cover their choice is desirable.
\end_layout

\begin_layout Standard
As KMC's performance is eventually tied with the accuracy of the infinite
 exponential family model in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "sub:exp_family_lite"

\end_inset

 and its approximate estimator in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:score_match_b"

\end_inset

, we can the score matching objective function in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:score_matching_objective_sample_version"

\end_inset

 to learn 
\begin_inset Formula $\sigma,\lambda$
\end_inset

.
 In order to avoid overfitting, we use a standard cross-validation procedure
 to evaluate the objective function on a test set that was held out during
 the fitting procedure.
 We can then use the cross-validated estimate of 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:score_matching_objective_sample_version"

\end_inset

 to compare different 
\begin_inset Formula $\sigma,\lambda$
\end_inset

 pairs with each other.
 As the 
\begin_inset Formula $\sigma,\lambda$
\end_inset

 surface is highly non-convex, we have to resort to global optimisation
 procedures, such as the computationally expensive grid-search 
\begin_inset CommandInset citation
LatexCommand cite
key "hsu2003practical"

\end_inset

, or faster alternatives from stochastic 
\begin_inset CommandInset citation
LatexCommand cite
key "hansen1996adapting"

\end_inset

 or Bayesian optimisation 
\begin_inset CommandInset citation
LatexCommand cite
key "hernandez2014predictive"

\end_inset

.
\end_layout

\begin_layout Standard
Figure shows the 
\begin_inset Formula $\sigma,\lambda$
\end_inset

 for the 
\begin_inset Formula $2$
\end_inset

-dimensional Gaussian and Banana examples from Section 
\begin_inset CommandInset ref
LatexCommand eqref
reference "par:illustrative_example"

\end_inset

.
\end_layout

\begin_layout Subsection
Smoothing and and annealing interpretation
\end_layout

\begin_layout Standard
Write about the fact that we have an 
\begin_inset Quotes eld
\end_inset

annealing
\begin_inset Quotes erd
\end_inset

 interpretation of the smoothing.
\end_layout

\begin_layout Subsection
Incorporating geometric approaches
\end_layout

\begin_layout Standard
Write about how this can be turned into a geometric version aka RM-HMC
\end_layout

\begin_layout Section
Experiments
\end_layout

\begin_layout Subsection
Trajectories in high dimensions
\end_layout

\begin_layout Standard
In order to quantify KMC's behaviour in high dimensions, we study average
 acceptance probabilities along trajectories as described in section 
\begin_inset CommandInset ref
LatexCommand eqref
reference "par:illustrative_example"

\end_inset

.
 We use the following two synthetic examples with increasing dimension:
 a standard Gaussian and the Banana described in 
\begin_inset CommandInset citation
LatexCommand citet
key "Haario1999"

\end_inset

, both with standard Gaussian momentum.
\end_layout

\begin_layout Standard
Figure 
\begin_inset CommandInset ref
LatexCommand eqref
reference "fig:kmc_trajectories_mean_acceptance"

\end_inset

 shows the average acceptance probability along a trajectory as a function
 of dimension of 
\begin_inset Formula $p$
\end_inset

 and 
\begin_inset Formula $q$
\end_inset

 in the same spirit as 
\begin_inset CommandInset citation
LatexCommand cite
after "Figure 3"
key "Betancourt2015"

\end_inset

.
 Error bars are obtained over 
\begin_inset Formula $100$
\end_inset

 trials in each of which we fit the infinite exponential family estimator
 on 
\begin_inset Formula $n=500$
\end_inset

 randomly sampled data and then run a random number of leapfrog steps in
 
\begin_inset Formula $[100,1000]$
\end_inset

 on both true and kernel induced flow.
 The plot shows that while the spread of the acceptance rate gets worse
 in high dimensions, most of the time, it is close to the true Hamiltonian
 ground truth.
 Note that in order to ensure that the kernel induced Hamiltonian flow cannot
 
\begin_inset Quotes eld
\end_inset

cheat
\begin_inset Quotes erd
\end_inset

 via oscilating in small areas of the space, we monitored the volume of
 the levels sets and ensured they are roughly equal.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/average_accept_gaussian_target.eps

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:kmc_trajectories_mean_acceptance"

\end_inset

Behaviour of kernel induced Hamiltonian flow in high dimensions.
 We show quantiles of the distribution of the average acceptance probability
 along a simulated trajectory (true Hamiltonian flow has neglectible variance).
 While worst-case acceptance probability of the kernel induced flow suffers
 in high dimensions, the median acceptance probability is remarkably high
 and close to the true Hamiltonian ground truth.
 
\series bold
Left:
\series default
 Gaussian.
 
\series bold
Right:
\series default
 Banana.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Comparison of trajectories
\end_layout

\begin_layout Standard
Empirically compare on how well we can approximate the true HMC trajectories
\end_layout

\begin_layout Itemize
Kernel choice via cross-validation and how we get most similar trajectories
\end_layout

\begin_layout Itemize
Optimise acceptance rate? Is this even well conditioned? Plot 
\begin_inset Formula $J(\alpha)$
\end_inset

 and ESS, and see whether cross-validation kernel is good for mixing
\end_layout

\begin_layout Subsection
Pseudo-Marginal MCMC
\end_layout

\begin_layout Itemize
Reproduce Kameleon experiments from Kameleon paper with KHMC
\end_layout

\begin_layout Itemize
Higher dimensional, also compare to plain HMC, (and random walk)
\end_layout

\begin_layout Itemize
Mushroom as a cool dataset?
\end_layout

\begin_layout Subsection
Comparisons to geometry HMC?
\end_layout

\begin_layout Standard
For that, first need to work out how we would even do that.
\end_layout

\begin_layout Section
Conclusions
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "local"
options "unsrt"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
normalsize
\end_layout

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
onecolumn
\end_layout

\end_inset


\end_layout

\begin_layout Section
\start_of_appendix
Proofs
\end_layout

\begin_layout Subsection
\begin_inset CommandInset label
LatexCommand label
name "sec:appendix_kernel_score_matching"

\end_inset

Kernel Score Matching Details
\end_layout

\begin_layout Paragraph
Objective function
\end_layout

\begin_layout Standard
Assume
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\log q(\xi;\alpha):=\log\Pr(\xi;\alpha)+\log Z(\alpha)=\sum_{i=1}^{m}\alpha_{i}k(x_{i},\xi)
\]

\end_inset

where
\begin_inset Formula 
\[
k(x_{i},\xi)=\exp\left(-\frac{\|x_{i}-\xi\|^{2}}{\sigma}\right)=\exp\left(-\frac{1}{\sigma}\sum_{\ell=1}^{d}(x_{i\ell}-\xi_{\ell})^{2}\right)
\]

\end_inset

Thus
\begin_inset Formula 
\[
\psi_{\ell}(\xi;\alpha)=\frac{2}{\sigma}\sum_{i=1}^{m}\alpha_{i}(x_{i\ell}-\xi_{\ell})\exp\left(-\frac{\|x_{i}-\xi\|^{2}}{\sigma}\right)
\]

\end_inset

and
\begin_inset Formula 
\begin{align*}
\partial_{\ell}\psi_{\ell}(\xi;\alpha) & =\frac{-2}{\sigma}\sum_{i=1}^{m}\alpha_{i}\exp\left(-\frac{\|x_{i}-\xi\|^{2}}{\sigma}\right)+\left(\frac{2}{\sigma}\right)^{2}\sum_{i=1}^{m}\alpha_{i}(x_{i\ell}-\xi_{\ell})^{2}\exp\left(-\frac{\|x_{i}-\xi\|^{2}}{\sigma}\right)\\
 & =\frac{2}{\sigma}\sum_{i=1}^{m}\alpha_{i}\exp\left(-\frac{\|x_{i}-\xi\|^{2}}{\sigma}\right)\left[-1+\frac{2}{\sigma}(x_{i\ell}-\xi_{\ell})^{2}\right].
\end{align*}

\end_inset

Substituting this into 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:score_matching_objective_sample_version"

\end_inset

 yields
\begin_inset Formula 
\begin{align*}
J(\alpha) & =\frac{1}{m}\sum_{i=1}^{m}\sum_{\ell=1}^{d}\left[\partial_{\ell}\psi_{\ell}(x_{i};\alpha)+\frac{1}{2}\psi_{\ell}(x_{i};\alpha)^{2}\right]\\
 & =\frac{2}{m\sigma}\sum_{\ell=1}^{d}\sum_{i=1}^{m}\sum_{j=1}^{m}\alpha_{i}\exp\left(-\frac{\|x_{i}-x_{j}\|^{2}}{\sigma}\right)\left[-1+\frac{2}{\sigma}(x_{i\ell}-x_{j\ell})^{2}\right]\\
 & \qquad+\frac{2}{m\sigma^{2}}\sum_{\ell=1}^{d}\sum_{i=1}^{m}\left[\sum_{j=1}^{m}\alpha_{j}(x_{j\ell}-x_{i\ell})\exp\left(-\frac{\|x_{i}-x_{j}\|^{2}}{\sigma}\right)\right]^{2}
\end{align*}

\end_inset


\end_layout

\begin_layout Paragraph
Matrix form for the cost function
\end_layout

\begin_layout Standard
The expression for the term 
\begin_inset Formula $J(\alpha)$
\end_inset

 being optimized is the sum of two terms.
 
\end_layout

\begin_layout Standard
Consider the 
\series bold
first term
\series default
:
\begin_inset Formula 
\[
\sum_{\ell=1}^{d}\sum_{i=1}^{m}\sum_{j=1}^{m}\alpha_{i}\exp\left(-\frac{\|x_{i}-x_{j}\|^{2}}{\sigma}\right)\left[-1+\frac{2}{\sigma}(x_{i\ell}-x_{j\ell})^{2}\right]
\]

\end_inset

 The term we need to compute is
\begin_inset Formula 
\begin{align*}
 & \sum_{i=1}^{m}\sum_{j=1}^{m}\alpha_{i}\exp\left(-\frac{\|x_{i}-x_{j}\|^{2}}{\sigma}\right)(x_{i\ell}-x_{j\ell})^{2},\\
= & \sum_{i=1}^{m}\sum_{j=1}^{m}\alpha_{i}\exp\left(-\frac{\|x_{i}-x_{j}\|^{2}}{\sigma}\right)\left(x_{i\ell}^{2}+x_{j\ell}^{2}-2x_{i\ell}x_{j\ell}\right).
\end{align*}

\end_inset

Define 
\begin_inset Formula 
\[
x_{\ell}:=\left[\begin{array}{ccc}
x_{1\ell} & \hdots & x_{m\ell}\end{array}\right]^{\top}.
\]

\end_inset

The final term may be computed cheaply with the right ordering of operations,
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
We assume we have an incomplete Cholesy representation of 
\begin_inset Formula $K_{ij}:=\exp\left(-\frac{\|x_{i}-x_{j}\|^{2}}{\sigma}\right)$
\end_inset

, 
\begin_inset Formula 
\[
K\approx LL^{\top},
\]

\end_inset

where 
\begin_inset Formula $L$
\end_inset

 is 
\begin_inset Formula $m\times t$
\end_inset

 and 
\begin_inset Formula $t\ll m$
\end_inset

.
\end_layout

\end_inset


\begin_inset Formula 
\[
-2(\alpha\odot x_{\ell})^{\top}Kx_{\ell}\approx-2(\alpha\odot x_{\ell})^{\top}LL^{\top}x_{\ell},
\]

\end_inset

where 
\begin_inset Formula $\alpha\odot x_{\ell}$
\end_inset

 is the entrywise product.
 The remaining terms are sums with constant row or column terms, and can
 likewise be computed cheaply: define 
\begin_inset Formula $s_{\ell}:=x_{\ell}\odot x_{\ell}$
\end_inset

 with components 
\begin_inset Formula $s_{i\ell}=x_{i\ell}^{2}$
\end_inset

.
 Then
\begin_inset Formula 
\begin{align*}
\sum_{i=1}^{m}\sum_{j=1}^{m}\alpha_{i}k_{ij}s_{j\ell} & =\alpha^{\top}Ks_{\ell}\\
 & \approx\alpha^{\top}LL^{\top}s_{\ell},
\end{align*}

\end_inset

which is cheap to compute.
 Likewise
\begin_inset Formula 
\[
\sum_{i=1}^{m}\sum_{j=1}^{m}\alpha_{i}x_{i\ell}^{2}k_{ij}\approx(\alpha\odot s_{\ell})^{\top}LL^{\top}1.
\]

\end_inset


\end_layout

\begin_layout Standard
We now write out the 
\series bold
second term
\series default
.
 Considering only the 
\begin_inset Formula $\ell$
\end_inset

th dimension, this is
\begin_inset Formula 
\begin{align*}
 & \sum_{i=1}^{m}\left[\sum_{j=1}^{m}\alpha_{j}(x_{j\ell}-x_{i\ell})\exp\left(-\frac{\|x_{i}-x_{j}\|^{2}}{\sigma}\right)\right]^{2}
\end{align*}

\end_inset

In matrix notation, the inner sum is a column vector,
\begin_inset Formula 
\[
K(\alpha\odot x_{\ell})-\left(K\alpha\right)\odot x_{\ell}\approx LL^{\top}(\alpha\odot x_{\ell})-\left(LL^{\top}\alpha\right)\odot x_{\ell}.
\]

\end_inset

We then take the entrywise square and sum the resulting vector, where both
 operations cost 
\begin_inset Formula $O(m)$
\end_inset

.
\end_layout

\begin_layout Paragraph
Solving 
\begin_inset Formula $J(\alpha)$
\end_inset

 for 
\begin_inset Formula $\alpha$
\end_inset


\end_layout

\begin_layout Standard
If we denote by 
\begin_inset Formula $D_{x}$
\end_inset

 the matrix with the vector 
\begin_inset Formula $x$
\end_inset

 on its diagonal, then the following two relations hold
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
K(\alpha\odot x) & =KD_{x}\alpha\\
(K\alpha)\odot x & =D_{x}K\alpha
\end{align}

\end_inset

This means that 
\begin_inset Formula $J(\alpha)$
\end_inset

 as defined previously,
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
J(\alpha) & =\frac{2}{m\sigma}\sum_{\ell=1}^{D}\left[\frac{2}{\sigma}\left[\alpha^{T}Ks_{\ell}+(\alpha\odot s_{\ell})^{T}K\mathbf{1}-2(\alpha\odot x_{\ell})^{T}Kx_{\ell}\right]-\alpha^{T}K\mathbf{1}\right]\\
 & +\frac{2}{m\sigma^{2}}\sum_{\ell=1}^{D}\left[(\alpha\odot x_{\ell})^{T}K-x_{\ell}^{T}\odot(\alpha^{T}K)\right]\left[K(\alpha\odot x_{\ell})-(K\alpha)\odot x_{\ell}\right],
\end{align*}

\end_inset

can be rewritten
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
J(\alpha) & =\frac{2}{m\sigma}\alpha^{T}\sum_{\ell=1}^{D}\left[\frac{2}{\sigma}(Ks_{\ell}+D_{s_{\ell}}K\mathbf{1}-2D_{x_{\ell}}Kx_{\ell})-K\mathbf{1}\right]\\
 & +\frac{2}{m\sigma^{2}}\alpha^{T}\left(\sum_{\ell=1}^{D}\left[D_{x_{\ell}}K-KD_{x_{\ell}}\right]\left[KD_{x_{\ell}}-D_{x_{\ell}}K\right]\right)\alpha\\
 & =\frac{2}{m\sigma}\alpha^{T}b+\frac{2}{m\sigma^{2}}\alpha^{T}C\alpha
\end{align*}

\end_inset

where
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
b & =\sum_{\ell=1}^{D}\left(\frac{2}{\sigma}(Ks_{\ell}+D_{s_{\ell}}K\mathbf{1}-2D_{x_{\ell}}Kx_{\ell})-K\mathbf{1}\right)\in\mathbb{R}^{m}\\
C & =\sum_{\ell=1}^{D}\left[D_{x_{\ell}}K-KD_{x_{\ell}}\right]\left[KD_{x_{\ell}}-D_{x_{\ell}}K\right]\in\mathbb{R}^{m\times m}.
\end{align*}

\end_inset

Assuming 
\begin_inset Formula $C$
\end_inset

 is invertible, this is minimised by 
\emph on

\begin_inset Formula 
\[
\hat{\alpha}=\frac{-\sigma}{2}C^{-1}b.
\]

\end_inset


\end_layout

\begin_layout Subsection
Infinite exponential family 
\begin_inset Quotes eld
\end_inset

lite
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "sub:exp_family_lite"

\end_inset


\end_layout

\begin_layout Standard
From the representer theorem, a consistent estimator to  
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:infinite_exp_family"

\end_inset

 is a linear combination of 
\begin_inset Formula $n\times d$
\end_inset

 terms, 
\begin_inset CommandInset citation
LatexCommand cite
key "SriFukKumGreHyv14"

\end_inset

.
 In order to save computation, we project the original solution onto the
 (growing) span of 
\begin_inset Formula $\left\{ k(z_{i},\cdot)\right\} _{i=1}^{n}$
\end_inset


\begin_inset Foot
status open

\begin_layout Plain Layout
Note that other (growing) basis sets might equally be used
\end_layout

\end_inset

, where 
\begin_inset Formula $\left\{ z_{i}\right\} _{i=1}^{n}$
\end_inset

 are points from the history of the Markov chain (details in the next section).
 Consequently, this approximate solution can be written as 
\begin_inset Formula $\sum_{i=1}^{m}\alpha_{i}k(x_{i},\cdot)$
\end_inset

.
 We then assume the following parametric form for the unnormalised log-target
\begin_inset Formula 
\[
\log q(\xi;\alpha)=\log\Pr(\xi;\alpha)+\log Z(\alpha)=\sum_{i=1}^{n}\alpha_{i}k(x_{i},\xi),
\]

\end_inset

where 
\begin_inset Formula $\alpha\in\mathbb{R}^{n}$
\end_inset

 is the 
\begin_inset Formula $n$
\end_inset

-dimensional parameter vector.
 Given an empirical fit of 
\begin_inset Formula $\hat{\alpha}$
\end_inset

, the resulting unnormalised log-density estimator has gradients
\begin_inset Formula 
\begin{equation}
\nabla_{\theta}\log q(\theta;\hat{\alpha})=\sum_{i=1}^{n}\hat{\alpha}_{i}\nabla_{\theta}k(x_{i},\theta),\label{eq:density_estimate_gradient}
\end{equation}

\end_inset

which are easily obtained for common kernels.
\end_layout

\begin_layout Paragraph
Gaussian Kernel
\end_layout

\begin_layout Standard
We now provide an example implementation of score matching for the well
 known Gaussian kernel 
\begin_inset Formula 
\[
k(x,y)=\exp\left(-\frac{\|x-y\|^{2}}{\sigma}\right),
\]

\end_inset

whose corresponding RKHS is infinite dimensional.
 The score function is then given by
\begin_inset Formula 
\[
\psi_{\ell}(\xi;\alpha)=\frac{2}{\sigma}\sum_{i=1}^{m}\alpha_{i}(x_{i\ell}-\xi_{\ell})\exp\left(-\frac{\|x_{i}-\xi\|^{2}}{\sigma}\right).
\]

\end_inset

Omitting intermediate details (see appendix 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:appendix_kernel_score_matching"

\end_inset

), the resulting sample objective function 
\begin_inset Formula $J(\alpha)$
\end_inset

 in  
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:score_matching_objective_sample_version"

\end_inset

 is globally minimised for
\begin_inset Formula 
\begin{equation}
\hat{\alpha}=\frac{-\sigma}{2}(C+\lambda(K+I))^{-1}b,\label{eq:kernel_score_matching_linear_system}
\end{equation}

\end_inset

which is a 
\begin_inset Formula $n$
\end_inset

-dimensional linear system where we added a regulariser using the norm of
 
\begin_inset Formula $\alpha$
\end_inset

 
\emph on
and
\emph default
 the norm of the solution 
\begin_inset Formula $\hat{f}$
\end_inset

, and defined
\begin_inset Formula 
\begin{align}
b & =\sum_{\ell=1}^{D}\left(\frac{2}{\sigma}(Ks_{\ell}+D_{s_{\ell}}K\mathbf{1}-2D_{x_{\ell}}Kx_{\ell})-K\mathbf{1}\right)\in\mathbb{R}^{n}\label{eq:score_match_b}\\
C & =\sum_{\ell=1}^{D}\left[D_{x_{\ell}}K-KD_{x_{\ell}}\right]\left[KD_{x_{\ell}}-D_{x_{\ell}}K\right]\in\mathbb{R}^{n\times n},\label{eq:score_match_C}
\end{align}

\end_inset

and
\begin_inset Formula 
\begin{align*}
K_{ij}: & =\exp\left(-\frac{\|x_{i}-x_{j}\|^{2}}{\sigma}\right)\\
s_{i\ell} & :=x_{i\ell}^{2}\\
x_{\ell} & :=\left[\begin{array}{ccc}
x_{1\ell} & \dots & x_{m\ell}\end{array}\right]^{\top}\\
D_{x_{\ell}} & :=\diag(x_{\ell}).
\end{align*}

\end_inset

See appendix 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:appendix_kernel_score_matching"

\end_inset

 for an expression in terms of 
\emph on
training
\emph default
 and 
\emph on
testing
\emph default
 data.
\end_layout

\begin_layout Subsection
Reducing computational costs via incomplete Cholesky
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "sub:linear_computational_costs"

\end_inset


\end_layout

\begin_layout Standard
Solving the linear system in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:kernel_score_matching_linear_system"

\end_inset

 requires 
\begin_inset Formula ${\cal O}(n^{3})$
\end_inset

 computation and 
\begin_inset Formula ${\cal O}(n^{2})$
\end_inset

 storage.
 This can be limiting, in particular if data comes from an increasingly
 long Markov chain.
 We therefore now apply a low-rank approximation to the kernel matrix via
 incomplete Cholesky 
\begin_inset CommandInset citation
LatexCommand cite
after "Alg. 5.12"
key "shawe2004kernel"

\end_inset

, that is a standard way to achieve linear computational costs for kernel
 methods.
 We rewrite any kernel matrix of the form 
\begin_inset Formula $K_{ij}=k(x_{i},x_{j})$
\end_inset

 as
\begin_inset Formula 
\[
K\approx LL^{T},
\]

\end_inset

where 
\begin_inset Formula $L\in\mathbb{R}^{n\times\ell}$
\end_inset

 is obtained via dual partial Gram–Schmidt orthonormalisation and costs
 both 
\begin_inset Formula ${\cal O}(n\ell)$
\end_inset

 computation and storage.
 Usually 
\begin_inset Formula $\ell\ll n$
\end_inset

, and 
\begin_inset Formula $\ell$
\end_inset

 can be chosen via an accuarcy cut-off parameter on the kernel spectrum
 in the same fashion as for other low-rank approximations, such as PCA
\begin_inset Foot
status open

\begin_layout Plain Layout
In this paper, we solely use the Gaussian kernel, whose spectrum decays
 exponentially fast.
\end_layout

\end_inset

.
 Given such representation of 
\begin_inset Formula $K$
\end_inset

, we can rewrite any dot product with a vector 
\begin_inset Formula $b\in\mathbb{R}^{n}$
\end_inset

 as 
\begin_inset Formula 
\[
Kb\approx(LL^{T})b=L(L^{T}b),
\]

\end_inset

where each left multiplication of 
\begin_inset Formula $L$
\end_inset

 costs 
\begin_inset Formula ${\cal O}(n\ell)$
\end_inset

 and we never need to store 
\begin_inset Formula $LL^{T}$
\end_inset

.
 This idea can be used to achieve costs of 
\begin_inset Formula ${\cal O}(n\ell)$
\end_inset

 when (approximately) computing 
\begin_inset Formula $b$
\end_inset

 in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:score_match_b"

\end_inset

, and left-multiplying 
\begin_inset Formula $C$
\end_inset

 and 
\begin_inset Formula $C+\lambda(K+I)$
\end_inset

 in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:score_match_C"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:kernel_score_matching_linear_system"

\end_inset

.
 Combining the technique with conjugate gradient (CG) 
\begin_inset CommandInset citation
LatexCommand cite
key "shewchuk1994introduction"

\end_inset

 allows to solve 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:kernel_score_matching_linear_system"

\end_inset

 with a maximum of 
\begin_inset Formula $n$
\end_inset

 such matrix-vector products, yielding a total computational cost of 
\begin_inset Formula ${\cal O}(n^{2}\ell)$
\end_inset

.
 In practice, we are not interested in an exact solution of 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:kernel_score_matching_linear_system"

\end_inset

 and we therefore can stop CG after a fixed number of iterations 
\begin_inset Formula $\tau\ll n$
\end_inset

.
 We arrive at a total cost of 
\begin_inset Formula ${\cal O}(n\ell\tau)$
\end_inset

 computation and 
\begin_inset Formula ${\cal O}(n\ell)$
\end_inset

 storage.
 We will publish the implementation in the near future for further reference.
\end_layout

\begin_layout Standard
Combined with a random subsample 
\begin_inset Formula $\mathbf{z}=\left\{ z_{i}\right\} _{i=1}^{n}$
\end_inset

 of the chain history 
\begin_inset Formula $\left\{ x_{i}\right\} _{i=0}^{t-1}$
\end_inset

 as described in Section 
\begin_inset CommandInset ref
LatexCommand eqref
reference "sub:adaptive_subsampling"

\end_inset

, we arrive at the same computational costs as previous kernel adaptive
 MCMC methods, 
\begin_inset CommandInset citation
LatexCommand cite
key "sejdinovic_kernel_2014"

\end_inset

.
\end_layout

\begin_layout Subsection
Consistency?
\end_layout

\end_body
\end_document
