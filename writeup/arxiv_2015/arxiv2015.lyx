#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\begin_preamble
\usepackage{nips14submit_e,times}
\usepackage{hyperref}
\usepackage{url}
%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09


\title{Kamiltonian Monte Carlo}


\author{
Heiko Strathmann%\thanks{ Use footnote for providing further information about author (webpage, alternative address)---\emph{not} for acknowledging funding agencies.}
\\
Gatsby Unit\\
University College London \\
\texttt{heiko.strathmann@gmail.com} \\
\And
Dino Sejdinovic \\
Department of Statistics \\
University of Oxford \\
\texttt{dino.sejdinovic@gmail.com} \\
\AND
Arthur Gretton \\
Gatsby Unit\\
University College London \\
\texttt{athur.gretton@gmail.com} 
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\nipsfinalcopy % Uncomment for camera-ready version

\DeclareMathOperator*{\argmin}{arg\,min}
\end_preamble
\use_default_options false
\begin_modules
theorems-std
\end_modules
\maintain_unincluded_children false
\language british
\language_package none
\inputencoding auto
\fontencoding default
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize letterpaper
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 0
\use_package mathtools 1
\use_package mhchem 0
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plainnat
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 2
\tocdepth 2
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\diag}{\text{diag}}
{\text{diag}}
\end_inset


\end_layout

\begin_layout Title
Gradient free Hamiltonian Monte Carlo via
\begin_inset Newline newline
\end_inset

doubly stochastic infinite exponential families
\end_layout

\begin_layout Abstract
We propose an adaptive, gradient free MCMC algorithm based on Hamiltonian
 Monte Carlo (HMC), which allows simulation from an intractable target probabili
ty density.
 Our sampler fills a gap: when the target density is intractable and its
 gradients are not available, while pseudo-marginal MCMC framework allows
 exact approximation, i.e., sampling from an asymptotically correct target,
 classic HMC methodology 
\emph on
cannot
\emph default
 be applied -- one is left with random walk methods, which tend to suffer
 from bad mixing behaviour.
 We extend the idea by 
\begin_inset CommandInset citation
LatexCommand cite
key "sejdinovic_kernel_2014"

\end_inset

 of adaptively learning target covariance structure in a Reproducing Kernel
 Hilbert Space (RKHS), based on the history of the Markov chain.
 But rather than 
\emph on
locally 
\emph default
smoothing the chain history, we directly model gradients of the 
\emph on
global
\emph default
 log-density.
 To that end, we employ two recently developed concepts from kernel methods:
 (i) fitting unnormalised infinite exponential family models via score matching,
 and (ii) reducing excessive computational costs via efficient doubly stochastic
 approximations using mini-batches and random basis functions.
 This novel combination provides a robust kernel-induced Hamiltonian flow
 which can be used to simulate trajectories on a level set that converges
 to the true level set in the Markov chain's asymptotic limit.
 We construct an 
\emph on
exact 
\emph default
MCMC algorithm, 
\emph on
Kamiltonian Monte Carlo (KMC)
\emph default
, that (i) behaves similarly to HMC in terms of autocorrelation and acceptance
 rates, but that (ii) does not require target gradients, and that in particular
 (iii) offers substantial mixing improvements in high dimensions.
 We support our claims with theoretical analysis in the form of finite sample
 convergence rates and experimental studies.
\end_layout

\begin_layout Paragraph
Next steps (Heiko)
\end_layout

\begin_layout Itemize

\strikeout on
Proof read Gaussian kernel score matching
\end_layout

\begin_layout Itemize

\strikeout on
Implement carefully
\end_layout

\begin_layout Itemize

\strikeout on
Implement trajectories, possibly within a given framework
\end_layout

\begin_layout Itemize

\strikeout on
Create plots for illustration
\end_layout

\begin_layout Itemize

\strikeout on
Write framework to compute average acceptance probability of a trajectory
\end_layout

\begin_layout Itemize

\strikeout on
Write section that empirically analyses approximate trajectories on Gaussians
 and Banana in terms of dimension
\end_layout

\begin_layout Itemize

\strikeout on
Random features
\end_layout

\begin_layout Itemize
Rewrite vanishing adaptation.
 Use decaying schedule rather than vanishing adaptation.
\end_layout

\begin_layout Itemize
Write section on optimising 
\begin_inset Formula $\lambda,\sigma$
\end_inset

 via x-validation or stochastic optimisation
\end_layout

\begin_layout Itemize
Proof convergence for random feature approximation
\end_layout

\begin_layout Itemize
Rates for the above?
\end_layout

\begin_layout Itemize
Translate finite sample convergence rates into all HMC quantities
\end_layout

\begin_layout Itemize

\strikeout on
Adapt notation and keywords from 
\begin_inset CommandInset citation
LatexCommand cite
key "Betancourt2015"

\end_inset

: Hamiltonian flow, level set
\end_layout

\begin_layout Itemize
Work over notation and unify the score matching with the HMC parts
\end_layout

\begin_layout Itemize
Do experiments (see below)
\end_layout

\begin_layout Paragraph*
Experiment & co
\end_layout

\begin_layout Itemize
Write framework for doing trajectory comparisons on cluster.
\end_layout

\begin_layout Itemize
Set up NUTS sampler for using pymc for kamiltonian and hamiltonian
\end_layout

\begin_layout Itemize
Set up sampling on cluster
\end_layout

\begin_layout Itemize
Banana quantile experiment, comparing against HMC and Kameleon
\end_layout

\begin_layout Itemize
GP experiment, comparing against Kameleon
\end_layout

\begin_layout Itemize
some new experiment?
\end_layout

\begin_layout Itemize
Create surface plots for the x-validation of 
\begin_inset Formula $\lambda,\sigma$
\end_inset


\end_layout

\begin_layout Standard
Trajectory experiment on Gaussian.
 To illustrate that it maintains acceptance prob in high dimensions
\end_layout

\begin_layout Enumerate
Increase 
\begin_inset Formula $d$
\end_inset

 for a fixed number of data.
 Should at some point fall apart
\end_layout

\begin_layout Enumerate
Increase number of data to show that 
\begin_inset Quotes eld
\end_inset

falling apart
\begin_inset Quotes erd
\end_inset

 happens later.
\end_layout

\begin_layout Enumerate
Translate to a guideline how many samples one needs to achieve similar performan
ce in high dimensions (exponential?)
\end_layout

\begin_layout Enumerate
Sanity check: is covered volume roughly equal?
\end_layout

\begin_layout Standard
Quantiles on Gaussian/Banana.
 To illustrate that it continues to explore tails in high dimensions
\end_layout

\begin_layout Enumerate
Same as above, but with MCMC and NUTS
\end_layout

\begin_layout Standard
Need something on burnin time.
\end_layout

\begin_layout Standard

\series bold
Open questions:
\end_layout

\begin_layout Itemize
Which strategy? Random feature approx to density and then score matching
 or doubly stochastic approx to the infinite exp.
 family objective?
\end_layout

\begin_layout Itemize
Is the value of the approximate hamiltonian bounded?
\end_layout

\begin_layout Itemize
Is the distance of the approximate hamiltonian and the true hamiltonian
 bounded? Sam mentioned that for leapfrog, the distance between true level
 set and discretised version is bounded.
 See Geometric Numerical Integration by Hairer and Lubich and reference
 is Neal's chapter 5
\end_layout

\begin_layout Itemize
Maximum distance of trajectory to Hamiltonian contour (2d plot), how does
 it behave?
\end_layout

\begin_layout Itemize
What do the tails of the density estimate do?
\end_layout

\begin_layout Itemize
Convergence rates of estimator translated into guarantees of the markov
 chain?
\end_layout

\begin_layout Itemize
Multimodality smoothing might be nice as an extension
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Itemize
Why MCMC
\end_layout

\begin_layout Itemize
Intractable densities
\end_layout

\begin_layout Itemize
Kernel Metropolis Hastings 
\begin_inset CommandInset citation
LatexCommand citep
key "sejdinovic_kernel_2014"

\end_inset

 and pros/cons
\end_layout

\begin_layout Itemize
Want: HMC like sampler
\end_layout

\begin_layout Itemize
Idea: Hamiltonian dynamics on kernel energy as proposal
\end_layout

\begin_layout Itemize
Give paper outline
\end_layout

\begin_layout Itemize
Here are some plots that might be useful
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
target "http://nbviewer.ipython.org/gist/anonymous/329d11916eb29743a7de"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Paragraph
Paper outline
\end_layout

\begin_layout Standard
Section
\end_layout

\begin_layout Section
Background & previous work
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "sec:previous_work"

\end_inset


\end_layout

\begin_layout Subsection
Notation
\end_layout

\begin_layout Itemize
Unnormalised target density 
\begin_inset Formula $\pi:\mathbb{R^{d}}\to\mathbb{R}$
\end_inset

 , unbiased estimator 
\begin_inset Formula $\mathbb{E}[\hat{\pi}(\theta)]=\pi(\theta)$
\end_inset


\end_layout

\begin_layout Itemize
Markov chain history at iteration 
\begin_inset Formula $t$
\end_inset

 with current position 
\begin_inset Formula $x_{t}$
\end_inset

: 
\begin_inset Formula $\left\{ x_{i}\right\} _{i=0}^{t-1}$
\end_inset


\end_layout

\begin_layout Itemize
History subsample 
\begin_inset Formula $\mathbf{z}=\left\{ z_{i}\right\} _{i=1}^{n}\subseteq\left\{ x_{i}\right\} _{i=0}^{t-1}$
\end_inset

 
\end_layout

\begin_layout Standard
HMC:
\end_layout

\begin_layout Itemize
Number of leapfrog steps, 
\begin_inset Formula $L$
\end_inset


\end_layout

\begin_layout Itemize
Step size 
\begin_inset Formula $\epsilon$
\end_inset


\end_layout

\begin_layout Subsection
Problem setting
\end_layout

\begin_layout Standard
Write what we want to do and what are important parts of it.
\end_layout

\begin_layout Subsection
Previous work
\end_layout

\begin_layout Standard
Write about Adaptive MCMC, 
\begin_inset CommandInset citation
LatexCommand cite
key "Haario1999"

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "Andrieu2008"

\end_inset

, Kameleon MCMC, 
\begin_inset CommandInset citation
LatexCommand citet
key "sejdinovic_kernel_2014"

\end_inset

, and how it improves over adaptive MH.
 Mention downsides and what we would like to have.
 In particular the pseudo-marginal case, 
\begin_inset CommandInset citation
LatexCommand cite
key "beaumont2003estimation"

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "Andrieu2009a"

\end_inset

.
\end_layout

\begin_layout Subsection
Hamiltonian Monte Carlo
\end_layout

\begin_layout Standard
We follow the presentation in 
\begin_inset CommandInset citation
LatexCommand cite
key "neal2011mcmc"

\end_inset

, 
\begin_inset CommandInset citation
LatexCommand cite
key "Betancourt2015"

\end_inset

.
 Hamiltonian Monte Carlo (HMC) is based on a fictitious dynamical system
 and utilises deterministic, measure-preserving maps to generate efficient
 Markov transitions.
 Starting from the unnormalised target density of the form 
\begin_inset Formula $\pi(q)\propto\exp(-U(q))$
\end_inset

, refered to as 
\emph on
potential energy, 
\emph default
we define a distribution of auxiliary variables, refered to as 
\emph on
momentum
\emph default
 or 
\emph on
kinetic energy
\emph default
, distributed proportional to 
\begin_inset Formula $\exp(-K(p))$
\end_inset

.
 Note that both potential and momentum are real-valued.
 The joint distribution is given by
\begin_inset Formula 
\begin{align*}
\exp\left(-K(p)-U(q)\right) & \propto\exp\left(-H(p,q)\right)
\end{align*}

\end_inset

where we defined the (seperable)
\emph on
 Hamiltonian
\emph default
 
\begin_inset Formula $H(p,q):=K(p,q)+U(q)$
\end_inset

.
 The Hamiltonian defines the 
\emph on
Hamiltonian flow
\emph default
 on the joint 
\begin_inset Formula $(p,q)$
\end_inset

 space, parametrised in 
\begin_inset Formula $t\in\mathbb{R}$
\end_inset

, and is a map
\emph on

\begin_inset Formula 
\begin{align*}
\phi_{t}^{H}:(p,q) & \mapsto(p^{*},q^{*}),\forall t\in\mathbb{R}\\
\phi_{t}^{H}\circ\phi_{s}^{H} & =\phi_{s+t}^{H}
\end{align*}

\end_inset


\emph default
The flow preserves the joint distribution, i.e.
 for some 
\begin_inset Formula $(p,q)$
\end_inset

 and some 
\begin_inset Formula $t\in\mathbb{R}$
\end_inset

 
\begin_inset Formula 
\[
\exp\left(-H(p,q)\right)=\exp\left(-H(\phi_{t}^{H}(p,q)\right)
\]

\end_inset

which allows to construct of Markov chains.
 For a chain at state 
\begin_inset Formula $(p,q)$
\end_inset

, we repeatedly
\end_layout

\begin_layout Enumerate
re-sample the axuiliary momentum 
\begin_inset Formula $p'\sim\exp(-K(p))$
\end_inset


\end_layout

\begin_layout Enumerate
apply the Hamiltonian flow for time 
\begin_inset Formula $t$
\end_inset

, which gives 
\begin_inset Formula $(p^{*},q^{*})=\phi_{t}^{H}(p',q$
\end_inset

)
\end_layout

\begin_layout Standard
Note that both steps leave the joint distribution unchanged.
 As re-sampling the momentum is independent of 
\begin_inset Formula $q$
\end_inset

, the 
\begin_inset Formula $q$
\end_inset

-marginal of the above procedure forms a Markov chain with 
\begin_inset Formula $\pi$
\end_inset

 as its stationaly distribution.
 Trajectories of the Hamiltonian flow can be thought of as walking along
 the contour lines of the Hamiltonian 
\begin_inset Formula $\{\phi_{t}^{H}(p,q)\mid t\in\mathbb{R}\}$
\end_inset

, referred to as 
\emph on
level set
\emph default
.
 Consequently, re-sampling 
\begin_inset Formula $p$
\end_inset

 corresponds to 
\begin_inset Quotes eld
\end_inset

jumping
\begin_inset Quotes erd
\end_inset

 to another level set.
\end_layout

\begin_layout Standard
For seperable Hamiltonians, the flow over an interval 
\begin_inset Formula $\tau$
\end_inset

 can be generated by the 
\emph on
Hamiltonian operator 
\begin_inset Formula $\hat{H},$
\end_inset

 
\emph default

\begin_inset Formula 
\[
\phi_{\tau}^{H}=e^{\tau\hat{H}}
\]

\end_inset

where
\begin_inset Formula 
\begin{align}
\hat{H} & =\frac{\partial H}{\partial p}\frac{\partial}{\partial q}-\frac{\partial H}{\partial q}\frac{\partial}{\partial p}\nonumber \\
 & =\frac{\partial K}{\partial p}\frac{\partial}{\partial q}-\frac{\partial U}{\partial q}\frac{\partial}{\partial p}\nonumber \\
 & =:\hat{K}+\hat{U}\label{eq:potential_energy_operator}
\end{align}

\end_inset


\end_layout

\begin_layout Paragraph
Approximate integration of Hamiltonian flow
\end_layout

\begin_layout Standard
In practice, 
\begin_inset Formula $\hat{H}$
\end_inset

 is usually unavailable.
 Therefore, we need to resort to approximate integration schemes to apply
 the Hamiltonian flow.
 A typical choice are 
\emph on
symplectic integrators,
\emph default
 which produce discrete approximations that accurately track the original
 trajectories while preserving crucial properties such as reversability
 and volume preservation.
 In this work, we limit ourselves to the standard leap-frog integrator,
 see 
\begin_inset CommandInset citation
LatexCommand cite
key "neal2011mcmc"

\end_inset

 for details.
 For a desired approximation length 
\begin_inset Formula $\tau$
\end_inset

 and given a discretisation step size 
\begin_inset Formula $\epsilon$
\end_inset

, it is given by the 
\begin_inset Formula $L=\tau/\epsilon$
\end_inset

 times successive composition
\begin_inset Formula 
\begin{align}
\phi_{\epsilon,\tau}^{H}: & =\left(\phi_{\frac{\epsilon}{2}}^{U}\circ\phi_{\epsilon}^{K}\circ\phi_{\frac{\epsilon}{2}}^{U}\right)^{L}\label{eq:leap_frog_flow}\\
 & =e^{\tau\hat{H}}+{\cal O}(\epsilon^{2})\nonumber 
\end{align}

\end_inset

The second equality implies that such descrete trajectories correspond to
 a level set of a modified Hamiltonian within a 
\begin_inset Formula ${\cal O}(\epsilon^{2})$
\end_inset

 pertubation of the original one, see 
\begin_inset CommandInset citation
LatexCommand cite
key "leimkuhler2004"

\end_inset

, 
\begin_inset CommandInset citation
LatexCommand cite
key "hairer2006geometric"

\end_inset

.
 While this discretisation error is small in practice, it still induces
 a bias on the resulting Markov chain.
 In order to maintain asymptotic correctess, a Metropolis acceptance procedure
 can be applied.
 Starting from a 
\begin_inset Formula $(p',q)$
\end_inset

, this is achieved by accepting an end-point of the approximate trajectory
 with probability
\begin_inset Formula 
\begin{align}
\alpha(\phi_{\epsilon,\tau}^{H}(p',q)) & =\min\left[1,\exp\left(H\circ\phi_{\epsilon,\tau}^{H}(p',q)-H(p',q)\right)\right].\label{eq:hmc_accept_prob}
\end{align}

\end_inset

It is clear that 
\begin_inset Formula $\lim_{\epsilon\to0}H\circ\phi_{\epsilon,\tau}^{H}(p',q)-H(p',q)=0$
\end_inset

 and the above acceptance probability tends to 
\begin_inset Formula $1$
\end_inset

 as the stepsize tends to 
\begin_inset Formula $0$
\end_inset

.
 This makes HMC a powerful sampling algorithm, which is able to propose
 distant, uncorrelated moves with a high acceptance probability.
\end_layout

\begin_layout Paragraph
HMC and intractable densities
\end_layout

\begin_layout Standard
In our case, the the gradient of 
\begin_inset Formula $\log\pi(q)=\text{const}-U(q)$
\end_inset

 and therefore the potential energy operator 
\begin_inset Formula 
\[
\hat{U}=-\frac{\partial U}{\partial q}\frac{\partial}{\partial p}
\]

\end_inset

is assumed to be unavailable
\begin_inset Foot
status open

\begin_layout Plain Layout
Unavailable due to analytic intractability, as opposed to computationally
 expensive in large 
\begin_inset Formula $n$
\end_inset

 cases.
 
\begin_inset Formula $H(p,q)$
\end_inset

 itself is available.
\end_layout

\end_inset

.
 A typical example of this scenario is the pseudo-marignal case, 
\begin_inset CommandInset citation
LatexCommand cite
key "beaumont2003estimation"

\end_inset

,
\begin_inset CommandInset citation
LatexCommand cite
key "Andrieu2009a"

\end_inset

, for which usually only adaptive random walk MCMC algorithms are available,
 see for example 
\begin_inset CommandInset citation
LatexCommand cite
key "FilipponeIEEETPAMI13"

\end_inset

.
 While (kernel) adaptive Metropolis-Hastings can be used to generate more
 efficient proposals, 
\begin_inset CommandInset citation
LatexCommand cite
key "sejdinovic_kernel_2014"

\end_inset

,
\begin_inset CommandInset citation
LatexCommand cite
key "Andrieu2008"

\end_inset

, these still suffer from random walk behaviour.
 It is well known that overcoming the latter results in significantly more
 efficeint sampling, 
\begin_inset CommandInset citation
LatexCommand cite
key "neal2011mcmc"

\end_inset

.
\end_layout

\begin_layout Standard
Kamiltonian Monte Carlo is based on the idea of replacing the potential
 energy operator 
\begin_inset Formula $\hat{U}$
\end_inset

 in  
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:potential_energy_operator"

\end_inset

 by a kernel induced surrogate computed from the history of the Markov chain.
 As we will see, the surrogate will be tractable and while not requiring
 gradients of the log-target density.
 It induces a kernel Hamiltonian flow, which can be numerically integrated
 using the integrator in  
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:leap_frog_flow"

\end_inset

.
 As the Hamiltonian 
\begin_inset Formula $H(p,q)$
\end_inset

 itself is 
\emph on
not
\emph default
 changed, any deviation of the alternative kernel level set from the true
 level set is corrected for via the Metropolis acceptance procedure in 
 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:hmc_accept_prob"

\end_inset

.
 Consequently, the stationary distribution of the Markov chain will 
\emph on
remain correct
\emph default

\begin_inset Foot
status open

\begin_layout Plain Layout
As usually when constructing adaptive MCMC algorithms, we will need to take
 care generating proposals based on the history of the Markov chain.
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
We now describe a technique to obtain a kernel induced potential energy
 surrogate, via fitting an infinite dimensional exponential family model
 using score matching.
\end_layout

\begin_layout Section
Kernel induced Hamiltonian dynamics
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "sec:kernel_hmc"

\end_inset


\end_layout

\begin_layout Standard
Recall our aim is to replace the potential energy operator 
\begin_inset Formula $\hat{U}$
\end_inset

 in  
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:potential_energy_operator"

\end_inset

 by a kernel induced surrogate that is tractable and does not require gradients
 of the log-target.
 To that end, we fit an infinite exponential family model,
\begin_inset CommandInset citation
LatexCommand cite
key "SriFukKumGreHyv14"

\end_inset

, of the form
\begin_inset Formula 
\begin{equation}
\exp\left(\langle f,k(x,\cdot)\rangle_{{\cal H}}-A(f)\right).\label{eq:infinite_exp_family}
\end{equation}

\end_inset

Here, 
\begin_inset Formula ${\cal H}$
\end_inset

 is a reproducing kernel Hilbert space (RKHS) of real valued functions on
 
\begin_inset Formula ${\cal X}$
\end_inset

.
 It has a uniquely associated symmetric, positive definite function (
\emph on
kernel
\emph default
) 
\begin_inset Formula $k:{\cal X}\times{\cal X}\rightarrow\mathbb{R}$
\end_inset

, which satisfies 
\begin_inset Formula $f(x)=\langle f,k(x,\cdot)\rangle$
\end_inset

 for any 
\begin_inset Formula $f\in{\cal H}$
\end_inset

, 
\begin_inset CommandInset citation
LatexCommand cite
key "BerTho04"

\end_inset

.
 The cannonical feature map 
\begin_inset Formula $\varphi\in{\cal H}$
\end_inset

 defined as 
\begin_inset Formula $\varphi:\mathcal{X}\to\mathcal{H},\varphi:x\mapsto k(\cdot,x)$
\end_inset

 here takes the role of the 
\emph on
sufficient statistics
\emph default
 and 
\begin_inset Formula $f\in{\cal H}$
\end_inset

 are the 
\emph on
natural parameters
\emph default
.
 
\begin_inset Formula $A(f):=\log\int_{{\cal X}}\exp(\langle f,k(x,\cdot)\rangle_{{\cal H}})dx$
\end_inset

 is the cimulant generating function.
  
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:infinite_exp_family"

\end_inset

 defines a very rich class of probability distributions and a broad class
 of denisties (for example continuous densities defined on compact domains)
 can be approximated arbritarily well.
 It was shown in 
\begin_inset CommandInset citation
LatexCommand cite
key "SriFukKumGreHyv14"

\end_inset

 that, under mild conditions, it is possible to consistently fit an 
\emph on
unnormalised
\emph default
 version of  
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:infinite_exp_family"

\end_inset

 using score matching, and convergences rates in Kullback-Leibler divergence,
 Hellinger and total-variation distances were established.
 Furthermore, experimental results suggest good performance in high dimensions,
 as for example opposed to kernel density estimation.
\end_layout

\begin_layout Standard
The proposed empirical estimator to 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:infinite_exp_family"

\end_inset

 in 
\begin_inset CommandInset citation
LatexCommand cite
key "SriFukKumGreHyv14"

\end_inset

 requires to invert an 
\begin_inset Formula $(n\times d)^{2}$
\end_inset

 matrix, where 
\begin_inset Formula $n$
\end_inset

 is the number of 
\begin_inset Formula $d$
\end_inset

-dimensional data, resulting in computational costs of 
\begin_inset Formula ${\cal O}(n^{6}d^{6})$
\end_inset

.
 As we are eventually interested in constructing an adaptive MCMC sampler
 that uses 
\emph on
all
\emph default
 of the history of the Markov chain to guide proposals, a cost scaling as
 a sixth order polynomial is infeasible.
\end_layout

\begin_layout Subsection
Score Matching
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout

\series bold
AG:
\series default
 Score matching references, 
\begin_inset CommandInset citation
LatexCommand citet
key "Hyvarinen-05,Hyvarinen-07"

\end_inset

.
 The latter gave solutions in terms of a finite mixture at fixed centres.
 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Following 
\begin_inset CommandInset citation
LatexCommand citet
key "Hyvarinen-05"

\end_inset

, we assume that a variable 
\begin_inset Formula $\xi\sim\pi$
\end_inset

 has some unknown probability density function 
\begin_inset Formula $\pi(\cdot)$
\end_inset

 defined on 
\begin_inset Formula $\mathbb{R}^{d}$
\end_inset

.
 We model the log probability with a parametric model of the form 
\begin_inset Formula 
\begin{equation}
\log\tilde{\pi}(\xi;\theta)=\log q(\xi;\theta)-\log Z(f),\label{eq:score_matching_parametric_model}
\end{equation}

\end_inset

 where 
\begin_inset Formula $f$
\end_inset

 is a vector of parameters of yet unspecified dimension (c.f.
 natural parameters 
\begin_inset Formula $f$
\end_inset

 of 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:infinite_exp_family"

\end_inset

), and 
\begin_inset Formula $Z(\theta)$
\end_inset

 an unknown normalising constant.
 We aim to approximate 
\begin_inset Formula $\pi(\cdot)$
\end_inset

 by 
\begin_inset Formula $\tilde{\pi}(\cdot;\hat{\theta})$
\end_inset

, i.e., to find 
\begin_inset Formula $\hat{\theta}$
\end_inset

 from a set of fixed i.i.d.
 samples 
\begin_inset Formula $\{x_{i}\sim\pi\}_{i=1}^{n}$
\end_inset

 -- 
\emph on
without 
\emph default
estimating 
\begin_inset Formula $Z(\theta)$
\end_inset

.
 From 
\begin_inset CommandInset citation
LatexCommand citet
after "equation. 2"
key "Hyvarinen-05"

\end_inset

, the criterion being optimized is the expected squared distance between
 score functions,
\emph on
 
\emph default

\begin_inset Formula 
\[
J(\theta)=\frac{1}{2}\int_{\xi}\pi(\xi)\left\Vert \psi(\xi;\theta)-\psi_{\pi}(\xi)\right\Vert ^{2}d\xi,
\]

\end_inset

where 
\begin_inset Formula 
\[
\psi(\xi;\theta)=\nabla_{\xi}\log\tilde{\pi}(\xi;\theta),
\]

\end_inset

and 
\begin_inset Formula $\psi_{\pi}(\xi)$
\end_inset

 is the derivative wrt 
\begin_inset Formula $\xi$
\end_inset

 of the unknown true density 
\begin_inset Formula $\pi(\xi)$
\end_inset

.
 As proved in 
\begin_inset CommandInset citation
LatexCommand citet
after "Theorem 1"
key "Hyvarinen-05"

\end_inset

 using partial integration, it is possible to express 
\begin_inset Formula $J(f)$
\end_inset

 without access to the unknown 
\begin_inset Formula $\psi_{\pi}(\xi)$
\end_inset

 as
\begin_inset Formula 
\[
J(\theta)=\int_{\xi}\pi(\xi)\sum_{\ell=1}^{d}\left[\partial_{\ell}\psi_{\ell}(\xi;\theta)+\frac{1}{2}\psi_{\ell}(\xi;\theta)^{2}\right]d\xi,
\]

\end_inset

where
\begin_inset Formula 
\begin{equation}
\psi_{\ell}(\xi;\theta)=\frac{\partial\log q(\xi;\theta)}{\partial\xi_{\ell}}\label{eq:score_match_first_score}
\end{equation}

\end_inset

and
\begin_inset Formula 
\begin{equation}
\partial_{\ell}\psi_{\ell}(\xi;\theta)=\frac{\partial^{2}\log q(\xi;\theta)}{\partial\xi_{\ell}^{2}}.\label{eq:score_match_second_score}
\end{equation}

\end_inset

Replacing the integral 
\begin_inset Formula $\int_{\xi}\pi(\xi)$
\end_inset

 with an average over the samples 
\begin_inset Formula $x_{i}$
\end_inset

 gives us a sample version of 
\begin_inset Formula $J(\theta)$
\end_inset

, minimising of which is consistent,
\begin_inset Formula 
\begin{equation}
\hat{J}(\theta)=\frac{1}{m}\sum_{i=1}^{n}\sum_{\ell=1}^{d}\left[\partial_{\ell}\psi_{\ell}(x_{i};\theta)+\frac{1}{2}\psi_{\ell}^{2}(x_{i};\theta)\right].\label{eq:score_matching_objective_sample_version}
\end{equation}

\end_inset


\end_layout

\begin_layout Subsection
Doubly stochastic infinite exponential family estimators
\end_layout

\begin_layout Standard
It was shown in 
\begin_inset CommandInset citation
LatexCommand cite
after "Theorem 3"
key "SriFukKumGreHyv14"

\end_inset

 that the unnormalised infinite exponential family model in 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:infinite_exp_family"

\end_inset

 can be fitted via score matching.
 This is done via first writing the score matching objective as
\begin_inset Formula 
\begin{align*}
J(f) & =\frac{1}{2}\langle f,Cf\rangle_{{\cal H}}+\langle f,b\rangle_{{\cal H}}
\end{align*}

\end_inset

where 
\begin_inset Formula $C:{\cal H\times{\cal H}\rightarrow{\cal H}}$
\end_inset

 and 
\begin_inset Formula $b\in{\cal H}$
\end_inset

 are defined as
\begin_inset Formula 
\begin{align*}
Cf & =\int p_{0}(x)\sum_{\ell=1}^{d}\frac{\partial k(\cdot,x)}{\partial x_{\ell}}\frac{\partial f(x)}{\partial x_{\ell}}dx\\
\xi & =\int p_{0}(x)\sum_{\ell=1}^{d}\left(\frac{\partial k(\cdot,x)}{\partial x_{\ell}}\frac{\partial\log q_{0}(x)}{\partial x_{\ell}}+\frac{\partial^{2}k(\cdot,x)}{\partial x_{\ell}^{2}}\right)dx,
\end{align*}

\end_inset

and then minimising 
\begin_inset Formula 
\begin{equation}
\argmin_{f\in{\cal H}}J(f)+\frac{\lambda}{2}\Vert f\Vert^{2}.\label{eq:infinite_exp_score_match_objective}
\end{equation}

\end_inset

The unique minimiser can be obtained by solving an 
\begin_inset Formula $nd$
\end_inset

-dimensional linear system.
 This is clearly infeasible as we would like to estimate trajectories of
 Markov chains of increasing length.
 We now apply the doubly stochastic gradient for kernel methods framework
 
\begin_inset CommandInset citation
LatexCommand cite
key "dai2014scalable"

\end_inset

, which is based on unbiasedly estimating the functional gradients of 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:infinite_exp_score_match_objective"

\end_inset

 via both mini-batches of data and the random kitchen sinks kernel expansion
 developed by 
\begin_inset CommandInset citation
LatexCommand cite
key "Rahimi2007"

\end_inset

.
 Intuitively, 
\begin_inset CommandInset citation
LatexCommand cite
key "dai2014scalable"

\end_inset

 rewrite the objective function as an expected value of some loss function
 that involves the kernel, and then approximate the expectation over the
 data with a single data point and the kernel function as an expectation
 over random Fourier features.
 This eventually allows to minimise the objective using stochastic gradient
 descent, and comes with theoretical guarantees for convergence and approximatio
n error.
\end_layout

\begin_layout Standard
Here, due the the form of 
\begin_inset Formula $J(f)$
\end_inset

, we will replace kernel gradients by random feature approximations.
 Due to linearity of expectation, inner product and gradients, the functional
 gradient of 
\begin_inset Formula $J(f)$
\end_inset

 at 
\begin_inset Formula $f$
\end_inset

 is given as (we assume 
\begin_inset Formula $q_{0}(x)=1$
\end_inset

)
\begin_inset Formula 
\begin{align*}
\nabla_{f}\left[J(f)\right] & =\nabla_{f}\left[\frac{1}{2}\langle f,Cf\rangle_{{\cal H}}+\langle f,b\rangle_{{\cal H}}\right]\\
 & =Cf+b\\
 & =\mathbb{E}_{x}\left[\sum_{\ell=1}^{d}\left(\frac{\partial k(\cdot,x)}{\partial x_{\ell}}\frac{\partial f(x)}{\partial x_{\ell}}+\frac{\partial^{2}k(\cdot,x)}{\partial x_{\ell}^{2}}\right)\right]\\
 & =\mathbb{E}_{x,\omega}\left[\phi_{\omega}(\cdot)\sum_{\ell=1}^{d}\left(\dot{\phi}_{\omega}^{\ell}(x)\frac{\partial f(x)}{\partial x_{\ell}}+\ddot{\phi}_{\omega}^{\ell}(x)\right)\right],
\end{align*}

\end_inset

where we defined 
\begin_inset Formula $\dot{\phi}_{\omega}^{\ell}(x):=\frac{\partial}{\partial x_{\ell}}\phi(x)$
\end_inset

 and 
\begin_inset Formula $\ddot{\phi}_{\omega}^{\ell}(x):=\frac{\partial^{2}}{\partial x_{\ell}^{2}}\phi_{\omega}(x)$
\end_inset

.
 An unbiased estimator can be obtained for a single data 
\begin_inset Formula $x_{t}\sim p_{0}(x)$
\end_inset

 and a single frequency 
\begin_inset Formula $\omega_{t}\sim\Gamma(\omega)$
\end_inset

, i.e.
\begin_inset Formula 
\begin{align*}
\zeta_{t}(\cdot): & =\phi_{\omega_{t}}(\cdot)\sum_{\ell=1}^{d}\left(\dot{\phi}_{\omega_{t}}^{\ell}(x_{t})\frac{\partial f(x_{t})}{\partial[x_{t}]_{\ell}}+\ddot{\phi}_{\omega_{t}}^{\ell}\right),
\end{align*}

\end_inset

an expression which is similar to 
\begin_inset CommandInset citation
LatexCommand cite
after "Eq. 4"
key "dai2014scalable"

\end_inset

.
 We can now apply additive stochastic gradient updates, i.e.
 starting from 
\begin_inset Formula $f_{1}(\cdot)=0\in{\cal H}$
\end_inset

, we define for 
\begin_inset Formula $t>1$
\end_inset


\begin_inset Formula 
\[
f_{t+1}(\cdot):=f_{t}(\cdot)-\gamma_{t}(\zeta_{t}(\cdot)+\lambda f_{t}(\cdot)),
\]

\end_inset

where we added the regulariser term 
\begin_inset Formula $\nabla_{f}\frac{\lambda}{2}\Vert f\Vert^{2}=\lambda f$
\end_inset

.
 Evaluation of 
\begin_inset Formula $f_{t+1}(\cdot)$
\end_inset

 at any point 
\begin_inset Formula $x$
\end_inset

 works as
\begin_inset Formula 
\[
f_{t+1}(x)=\sum_{i=1}^{t}\alpha_{i}\phi_{\omega_{i}}(x),
\]

\end_inset

where 
\begin_inset Formula $\alpha_{i}:=-\gamma_{i}\sum_{\ell=1}^{d}\left(\dot{\phi}_{\omega_{i}}^{\ell}(x_{i})\frac{\partial f(x_{i})}{\partial[x_{i}]_{\ell}}+\ddot{\phi}_{\omega_{i}}^{\ell}\right)$
\end_inset

.
 Evaluation of the gradient 
\begin_inset Formula $\frac{\partial f(x_{i})}{\partial[x_{i}]_{\ell}}$
\end_inset

 also works recursively via the reproducing property and the unbiased kernel
 approximation
\begin_inset Formula 
\[
\frac{\partial f(x_{i})}{\partial[x_{i}]_{\ell}}=\frac{\partial\langle\phi_{\omega_{i}}(\cdot)\phi_{\omega_{t}}(x_{i}),f\rangle}{\partial[x_{i}]_{\ell}}=\dot{\phi}_{\omega_{i}}^{\ell}(x_{i})\langle\phi_{\omega_{i}}(\cdot),f\rangle=\dot{\phi}_{\omega_{i}}^{\ell}(x_{i})f_{i}(x_{i})
\]

\end_inset


\end_layout

\begin_layout Subsection
Finite dimensional approximate infinite exponential family estimators
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "sub:finite_approx_infinite_exp_family"

\end_inset


\end_layout

\begin_layout Standard
We next apply the score matching estimator in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:score_matching_objective_sample_version"

\end_inset

 to estimate the parameters of the infinite exponential family model in
 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:infinite_exp_family"

\end_inset

.
 But rather than following the 
\begin_inset Formula ${\cal O}(n^{6}d^{6})$
\end_inset

 approach from 
\begin_inset CommandInset citation
LatexCommand cite
key "SriFukKumGreHyv14"

\end_inset

, we use an approximate feature space version with a finite set of random
 set of basis functions.
\end_layout

\begin_layout Standard
By the representer theorem, one can estimate 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:infinite_exp_family"

\end_inset

 in its dual form via minimising 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:score_matching_objective_sample_version"

\end_inset

.
 However, this requires 
\begin_inset Formula ${\cal O}(n^{6}d^{6})$
\end_inset

 computation and 
\begin_inset Formula ${\cal O}(n^{2}d^{2})$
\end_inset

 storage; which is clearly limiting, in particular if training samples comes
 from an increasingly long Markov chain.
 
\begin_inset CommandInset citation
LatexCommand cite
key "sejdinovic_kernel_2014"

\end_inset

, whose computational costs are 
\begin_inset Formula ${\cal O}(nd^{2})$
\end_inset

 side stepped the problem of increasing computation via repeatedly updating
 a random subsample of fixed size from the Markov chain history, and recomputing
 their proposal mechanism.
 In this section, we take a different approach: instead of fitting an exact
 infinite dimensional model on a subset of the available data, we fit an
 approximation to the infinite model using 
\emph on
all
\emph default
 available data.
 To that end, we construct an approximate estimator of 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:infinite_exp_family"

\end_inset

 in 
\emph on
primal
\emph default
 form, whose computational costs remain constant as 
\begin_inset Formula $n\to\infty$
\end_inset

.
\end_layout

\begin_layout Standard
Define an 
\begin_inset Formula $m$
\end_inset

-dimensional approximate
\begin_inset Foot
status open

\begin_layout Plain Layout
We deliberatly don't state the form of the approximation yet, but will give
 details later.
\end_layout

\end_inset

 feature space 
\begin_inset Formula ${\cal H}_{m}=\mathbb{R}^{m}$
\end_inset

 with a fixed set of random basis vectors 
\begin_inset Formula $\{\omega_{i}\}_{i=1}^{m}$
\end_inset

.
 Denote by 
\begin_inset Formula $\phi_{x}\in\mathbb{{\cal H}}^{m}$
\end_inset

 the embedding of a point 
\begin_inset Formula $x\in{\cal X}=\mathbb{R^{d}}$
\end_inset

 into 
\begin_inset Formula ${\cal H}_{m}=\mathbb{R}^{m}$
\end_inset

.
 We then can approximate the kernel function using the finite rank expansion
 
\begin_inset Formula $K_{ij}:=k(x_{i},x_{j})\approx\phi_{x_{i}}^{T}\phi_{x_{j}}$
\end_inset

.
 The infinite dimensional exponential family form in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:infinite_exp_family"

\end_inset

 can be expressed in this approximate feature space via replacing natural
 parameters 
\begin_inset Formula $f\in{\cal H}$
\end_inset

 by 
\begin_inset Formula $\theta\in{\cal H}_{m}$
\end_inset

 and sufficient statistics 
\begin_inset Formula $k(x,\cdot)\in{\cal H}$
\end_inset

 by 
\begin_inset Formula $\phi_{x}\in{\cal H}_{m}$
\end_inset

 as
\begin_inset Formula 
\begin{align}
\exp\left(\langle f,k(x,\cdot)\rangle_{{\cal H}}-A(f)\right) & \approx\exp\left(\langle\theta,\phi_{x}\rangle_{{\cal H}_{m}}-A(\theta)\right)\nonumber \\
 & =\exp\left(\theta^{T}\phi_{x}-A(\theta)\right)\label{eq:infinite_exp_family_random_feats}
\end{align}

\end_inset

Via setting the unnormalised log-density of the parametric form in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:score_matching_parametric_model"

\end_inset

 to 
\begin_inset Formula $\log q(\xi;\theta)=\theta^{T}\phi_{x}$
\end_inset

, we obtain an finite dimensional approximate infinite exponential family
 model of the target density 
\begin_inset Formula $\pi$
\end_inset

.
 In order to fit 
\begin_inset Formula $\hat{\theta}$
\end_inset

, we need to compute the score matching functions in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:score_match_first_score"

\end_inset

,
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:score_match_second_score"

\end_inset

, which have the simple linear form
\begin_inset Formula 
\begin{align}
\psi_{\ell}(\xi;\theta) & =\theta^{T}\dot{\phi}_{x}^{\ell}\quad\text{and}\quad\partial_{\ell}\psi_{\ell}(\xi;\theta)=\theta^{T}\ddot{\phi}_{x}^{\ell},\label{eq:score_function_fourier}
\end{align}

\end_inset

where we defined the 
\begin_inset Formula $m$
\end_inset

-dimensional feature vector derivatives 
\begin_inset Formula $\dot{\phi}_{x}^{\ell}:=\frac{\partial}{\partial x_{\ell}}\phi_{x}$
\end_inset

 and 
\begin_inset Formula $\ddot{\phi}_{x}^{\ell}:=\frac{\partial^{2}}{\partial x_{\ell}^{2}}\phi_{x}$
\end_inset

.
 Plugging those into the empirical score matching objective in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:score_matching_objective_sample_version"

\end_inset

, we arrive at
\begin_inset Formula 
\begin{align}
J(\theta) & =\frac{1}{n}\sum_{i=1}^{n}\sum_{\ell=1}^{d}\left[\partial_{\ell}\psi_{\ell}(x_{i};\theta)+\frac{1}{2}\psi_{\ell}^{2}(x_{i};\theta)\right]\nonumber \\
 & =\frac{1}{n}\sum_{i=1}^{n}\sum_{\ell=1}^{d}\left[\theta^{T}\ddot{\phi}_{x_{i}}^{\ell}+\frac{1}{2}\theta^{T}\left(\dot{\phi}_{x_{i}}^{\ell}\left(\dot{\phi}_{x_{i}}^{\ell}\right)^{T}\right)\theta\right]\nonumber \\
 & =\frac{1}{2}\theta^{T}C\theta-\theta^{T}b\label{eq:score_match_objective_random_feats}
\end{align}

\end_inset

 where
\begin_inset Formula 
\begin{equation}
b:=-\frac{1}{n}\sum_{i=1}^{n}\sum_{\ell=1}^{d}\ddot{\phi}_{x_{i}}^{\ell}\in\mathbb{R}^{m}\quad\text{and}\quad C:=\frac{1}{n}\sum_{i=1}^{n}\sum_{\ell=1}^{d}\left(\dot{\phi}_{x_{i}}^{\ell}\left(\dot{\phi}_{x_{i}}^{\ell}\right)^{T}\right)\in\mathbb{R}^{m\times m}.\label{eq:b_and_C_random_feats}
\end{equation}

\end_inset

Assuming 
\begin_inset Formula $C$
\end_inset

 is invertible (trivial for 
\begin_inset Formula $n\geq m$
\end_inset

), the objective is uniquely minimised by differentiating 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:score_match_objective_random_feats"

\end_inset

 wrt.
 
\begin_inset Formula $\theta$
\end_inset

, setting to zero, and solving for 
\begin_inset Formula $\theta$
\end_inset

.
 This gives
\begin_inset Formula 
\begin{equation}
\hat{\theta}:=C^{-1}b.\label{eq:score_match_linear_system_random_feats}
\end{equation}

\end_inset

Having computed 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:score_match_linear_system_random_feats"

\end_inset

, we can evaluate log-pdf and its gradients as 
\begin_inset Formula 
\begin{align}
\label{eq:density_estimate_grad_random_feats}\\
\nabla_{x}\log q(x;\hat{\theta}) & =\nabla_{x}\left[\hat{\theta}^{T}\phi_{x}-A(f)\right]\\
 & =\left[\nabla_{x}\phi_{x}\right]^{T}\hat{\theta}\nonumber 
\end{align}

\end_inset


\end_layout

\begin_layout Standard
Similar to 
\begin_inset CommandInset citation
LatexCommand cite
key "SriFukKumGreHyv14"

\end_inset

, we in practice add a term 
\begin_inset Formula $\lambda\Vert\theta\Vert^{2}$
\end_inset

 for 
\begin_inset Formula $\lambda\in\mathbb{R}^{+}$
\end_inset

 to 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:score_match_objective_random_feats"

\end_inset

, in order to control the norm of the natural parameters 
\begin_inset Formula $\theta\in{\cal H}^{m}$
\end_inset

.
 This results in the regularised and numerically more stable solution 
\begin_inset Formula $\hat{\theta}_{\lambda}:=(C+\lambda I)^{-1}b$
\end_inset

, at the cost of having to choose 
\begin_inset Formula $\lambda$
\end_inset

, which we describe later.
 We use the un-regularised solution 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:score_match_linear_system_random_feats"

\end_inset

 for notational ease throughout the rest of the paper, but always regularise
 in practice.
 
\begin_inset Formula $\lambda$
\end_inset

 will be chosen by cross-validation.
\end_layout

\begin_layout Standard
Next, we be more concrete about the approximate feature space 
\begin_inset Formula ${\cal H}^{m}$
\end_inset

.
 Note that the above approach can be combined with 
\emph on
any
\emph default
 set of finite dimensional approximate feature mappings 
\begin_inset Formula $\phi_{x}$
\end_inset

.
\end_layout

\begin_layout Subsubsection
Example: Random Fourier features for the Gaussian kernel
\end_layout

\begin_layout Standard
We now combine the finite dimensional approximate infinite exponential family
 model with the 
\begin_inset Quotes eld
\end_inset

random kitchen sink
\begin_inset Quotes erd
\end_inset

 framework made popular by 
\begin_inset CommandInset citation
LatexCommand cite
key "Rahimi2007"

\end_inset

.
 Assume a translation invariant kernel 
\begin_inset Formula $k(x,y)=\tilde{k}(x-y)$
\end_inset

.
 Bochner's theorem gives the representation
\begin_inset Formula 
\[
k(x,y)=\tilde{k}(x-y)=\int_{\mathbb{R}^{d}}\exp\left(i\omega^{T}(x-y)\right)d\Gamma(\omega),
\]

\end_inset

where 
\begin_inset Formula $\Gamma(\omega)$
\end_inset

 is the Fourier transform of the kernel.
 An approximate feature mapping for such kernels can be obtained via dropping
 imaginary terms and approximating the integral with Monte Carlo integration.
 This gives 
\begin_inset Formula 
\[
\phi_{x}=\sqrt{\frac{2}{m}}\left[\cos(\omega_{1}^{T}x+u_{1}),\dots,\cos(\omega_{m}^{T}x+u_{m})\right],
\]

\end_inset

with fixed random basis vector realisations that depend on the kernel via
 
\begin_inset Formula $\Gamma(\omega)$
\end_inset

,
\begin_inset Formula 
\begin{align*}
\omega_{i}\sim\Gamma(\omega),
\end{align*}

\end_inset

and fixed random offset realisations 
\begin_inset Formula 
\[
u_{i}\sim\texttt{Uniform}[0,2\pi],
\]

\end_inset

for 
\begin_inset Formula $i=1\dots m$
\end_inset

.
 It is easy to see that this approximation is consistent for 
\begin_inset Formula $m\to\infty$
\end_inset

, i.e.
\begin_inset Formula 
\[
\mathbb{E}_{\omega,b}\left[\phi_{x}^{T}\phi_{y}\right]=k(x,y).
\]

\end_inset

See 
\begin_inset CommandInset citation
LatexCommand cite
key "Rahimi2007"

\end_inset

 for details and a uniform convergence bound.
 Note that one can achieve logarithmic computational costs in 
\begin_inset Formula $d$
\end_inset

 exploiting properties of Hadamard matrices, see 
\begin_inset CommandInset citation
LatexCommand cite
key "le2013fastfood"

\end_inset

.
\end_layout

\begin_layout Standard
The score functions 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:score_function_fourier"

\end_inset

 are given by
\begin_inset Formula 
\begin{align*}
\dot{\phi}_{\xi}^{\ell} & =\sqrt{\frac{2}{m}}\frac{\partial}{\partial\xi_{\ell}}\left[\cos(\omega_{1}^{T}\xi+u_{1}),\dots,\cos(\omega_{m}^{T}\xi+u_{m})\right]\\
 & =-\sqrt{\frac{2}{m}}\left[\sin(\omega_{1}^{T}\xi+u_{1})\omega_{1\ell},\dots,\sin(\omega_{m}^{T}\xi+u_{m})\omega_{m\ell}\right]\\
 & =-\sqrt{\frac{2}{m}}\left[\sin(\omega_{1}^{T}\xi+u_{1}),\dots,\sin(\omega_{m}^{T}\xi+u_{m})\right]\odot\left[\omega_{1\ell},\dots,\omega_{m\ell}\right],
\end{align*}

\end_inset

where 
\begin_inset Formula $\omega_{1\ell}$
\end_inset

 is the 
\begin_inset Formula $\ell$
\end_inset

-th component of 
\begin_inset Formula $\omega_{1}$
\end_inset

, and
\begin_inset Formula 
\begin{align*}
\ddot{\phi}_{\xi}^{\ell}: & =-\sqrt{\frac{2}{m}}\frac{\partial}{\partial\xi_{\ell}}\left[\sin(\omega_{1}^{T}\xi+u_{1}),\dots,\sin(\omega_{m}^{T}\xi+u_{m})\right]\odot\left[\omega_{1\ell},\dots,\omega_{m\ell}\right]\\
 & =-\sqrt{\frac{2}{m}}\left[\cos(\omega_{1}^{T}\xi+u_{1}),\dots,\cos(\omega_{m}^{T}\xi+u_{m})\right]\odot\left[\omega_{1\ell}^{2},\dots,\omega_{m\ell}^{2}\right]\\
 & =-\phi_{\xi}\odot\left[\omega_{1\ell}^{2},\dots,\omega_{m\ell}^{2}\right],
\end{align*}

\end_inset

where 
\begin_inset Formula $\odot$
\end_inset

 is the element-wise product.
 Consequently the gradient itself is given by 
\begin_inset Formula 
\[
\nabla_{\xi}\phi_{\xi}=\begin{bmatrix}\dot{\phi}_{\xi}^{1}\\
\vdots\\
\dot{\phi}_{\xi}^{d}
\end{bmatrix}\in\mathbb{R}^{d\times m}
\]

\end_inset


\end_layout

\begin_layout Standard
An example pair of translation invariant kernel and its Fourier transform
 for the well-known 
\emph on
Gaussian kernel
\emph default
 are
\begin_inset Formula 
\[
k(x,y)={\cal \exp}\left(-\gamma\Vert x-y\Vert_{2}^{2}\right)\quad\text{and}\quad\Gamma(\omega)={\cal N}\left(\omega_{i}\Big\vert\mathbf{0},\gamma^{2}I_{m}\right).
\]

\end_inset


\end_layout

\begin_layout Subsubsection
Rank-
\begin_inset Formula $d$
\end_inset

 updates for constant cost updates
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "sub:rank_d_updates"

\end_inset


\end_layout

\begin_layout Standard
A convenient property of the above approximation is that it is possible
 to update 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:b_and_C_random_feats"

\end_inset

 in an online fashion.
 Each new point of the Markov chain history only adds a term of the form
 
\begin_inset Formula $-\sum_{\ell=1}^{d}\ddot{\phi}_{x_{i}}^{\ell}\in\mathbb{R}^{m}$
\end_inset

 and 
\begin_inset Formula $\sum_{\ell=1}^{d}\dot{\phi}_{x_{i}}^{\ell}(\dot{\phi}_{x_{i}}^{\ell})^{T}\in\mathbb{R}^{m\times m}$
\end_inset

 to the moving averages of 
\begin_inset Formula $b$
\end_inset

 and 
\begin_inset Formula $C$
\end_inset

 respectively.
 Consequently, rather than fully re-computing 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:score_match_linear_system_random_feats"

\end_inset

 at the cost of 
\begin_inset Formula ${\cal O}(nm^{3})$
\end_inset

 for every new point, we can use rank-
\begin_inset Formula $d$
\end_inset

 updates to construct the minimiser of 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:score_match_objective_random_feats"

\end_inset

 from the solution of the previous iteration.
 Assume we have computed the sum of all moving average terms, 
\begin_inset Formula 
\[
\tilde{C}_{n}^{-1}:=\left(\sum_{i=1}^{n}\sum_{\ell=1}^{d}\left(\dot{\phi}_{x_{i}}^{\ell}\left(\dot{\phi}_{x_{i}}^{\ell}\right)^{T}\right)\right)^{-1}
\]

\end_inset

 from feature vectors derivatives 
\begin_inset Formula $\ddot{\phi}_{x_{i}}^{\ell}\in\mathbb{R}^{m}$
\end_inset

 of some set of points 
\begin_inset Formula $\left\{ x_{i}\right\} _{i=1}^{n}$
\end_inset

, and subsequently receive receive a new point 
\begin_inset Formula $x_{n+1}$
\end_inset

.
 We can then write the inverse of the new sum as
\begin_inset Formula 
\begin{align*}
\tilde{C}_{n+1}^{-1}: & =\left(\tilde{C}_{n}+\sum_{\ell=1}^{d}\left(\dot{\phi}_{x_{n+1}}^{\ell}\left(\dot{\phi}_{x_{n+1}}^{\ell}\right)^{T}\right)\right)^{-1}.
\end{align*}

\end_inset

This is an inversion of the rank-
\begin_inset Formula $d$
\end_inset

 perturbed previous matrix 
\begin_inset Formula $\tilde{C}_{n}$
\end_inset

.
 We can therefore construct this inverse using 
\begin_inset Formula $d$
\end_inset

 successive applications of the Sherman-Morrison-Woodbury formula for rank-one
 updates 
\begin_inset CommandInset citation
LatexCommand cite
key "gill1974methods"

\end_inset

, each using 
\begin_inset Formula ${\cal O}(m^{2})$
\end_inset

 computation.
 Since 
\begin_inset Formula $\tilde{C}_{n}$
\end_inset

 is positive definite
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset Formula $C$
\end_inset

 is the empirical covariance of the feature derivatives 
\begin_inset Formula $\dot{\phi}_{x_{i}}^{\ell}$
\end_inset

.
\end_layout

\end_inset

, we can represent its inverse as a numerically much more stable Cholesky
 factorisation 
\begin_inset Formula $\tilde{C}_{n}=\tilde{L}_{n}\tilde{L}_{n}^{T}$
\end_inset

.
 It is also possible to perform cheap rank-
\begin_inset Formula $d$
\end_inset

 updates of such Cholesky factors, see 
\begin_inset CommandInset citation
LatexCommand cite
key "gill1974methods"

\end_inset

,
\begin_inset CommandInset citation
LatexCommand cite
key "seeger2004low"

\end_inset


\begin_inset Foot
status open

\begin_layout Plain Layout
We use the open-source implementation provided at 
\begin_inset Flex URL
status open

\begin_layout Plain Layout

https://github.com/jcrudy/choldate
\end_layout

\end_inset


\end_layout

\end_inset

.
 Denote by 
\begin_inset Formula $\tilde{b}_{n}$
\end_inset

 the sum of the moving average 
\begin_inset Formula $b$
\end_inset

.
 We solve 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:score_match_linear_system_random_feats"

\end_inset

 as 
\begin_inset Formula 
\begin{align*}
\hat{\theta} & =C^{-1}b=\left(\frac{1}{n}\tilde{C}_{n}\right)^{-1}\left(\frac{1}{n}\tilde{b}_{n}\right)=\tilde{C}_{n}^{-1}\tilde{b}_{n}=\tilde{L}_{n}^{-T}\tilde{L}_{n}^{-1}\tilde{b}_{n},
\end{align*}

\end_inset

using cheap triangular back-substitution from 
\begin_inset Formula $\tilde{L}_{n}$
\end_inset

, and never storing 
\begin_inset Formula $\tilde{C}_{n}^{-1}$
\end_inset

 or 
\begin_inset Formula $\tilde{L}_{n}^{-1}$
\end_inset

 explicitly.
\end_layout

\begin_layout Standard
Using such updates, the computational costs for updating the approximate
 infinite exponential family model in 
\emph on
every 
\emph default
iteration of the Markov chain are 
\begin_inset Formula ${\cal O}(dm^{2})$
\end_inset

, which 
\emph on
constant in 
\begin_inset Formula $n$
\end_inset

.
 
\emph default
We can therefore use 
\emph on
all
\emph default
 points in the history for constructing a proposal -- without the previously
 exploding computational costs of 
\begin_inset Formula ${\cal O}(ndm^{3})$
\end_inset

.
\end_layout

\begin_layout Subsection
Kernel induced Hamiltonian flow
\end_layout

\begin_layout Standard
We now combine the (approximate) infinite exponential family estimator with
 Hamiltonian dynamics to define a 
\emph on
kernel induced Hamiltonian flow
\emph default
.
 More precisely, we define a kernel induced Hamiltonian operator 
\begin_inset Formula $\hat{H}_{k}=\hat{K}+\hat{U}_{k}$
\end_inset

, where 
\begin_inset Formula $\hat{K}$
\end_inset

 is defined as in  
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:potential_energy_operator"

\end_inset

, and 
\begin_inset Formula 
\begin{equation}
\hat{U}_{k}=\frac{\partial U_{k}}{\partial p}\frac{\partial}{\partial q}\label{eq:kernel_potential_energy_operator}
\end{equation}

\end_inset

with 
\begin_inset Formula $\nabla U_{k}=\nabla\log q(\xi;\alpha)$
\end_inset

 being the gradient of the fitted approximate infinite exponential family
 model from 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:density_estimate_grad_random_feats"

\end_inset

.
 Using the described numerical leap-frog integration, the approximate kernel
 induced Hamiltonian flow then follows similar to  
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:leap_frog_flow"

\end_inset

, 
\begin_inset Formula 
\begin{equation}
\phi_{\epsilon,\tau}^{\tilde{H}_{k}}:=\left(\phi_{\frac{\epsilon}{2}}^{U_{k}}\circ\phi_{\epsilon}^{K}\circ\phi_{\frac{\epsilon}{2}}^{U_{k}}\right)^{L}\label{eq:kernel_leap_frog_flow}
\end{equation}

\end_inset

It is clear that the kernel induced potential energy operator 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:kernel_potential_energy_operator"

\end_inset

 results different level sets than those induced by the discretised true
 Hamiltonian flow 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:potential_energy_operator"

\end_inset

.
 In order to correct for any induced bias on the resulting Markov chain,
 we perform the Metropolis acceptance step from  
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:hmc_accept_prob"

\end_inset

.
 We propose the end-point of an approximate integration of the kernel induced
 Hamiltonian flow, and accept according to the value of the 
\emph on
true
\emph default
 Hamiltonian,
\begin_inset Formula 
\begin{equation}
\alpha(\phi_{\epsilon,\tau}^{\tilde{H}_{k}}(p',q))=\min\left[1,\exp\left(H\circ\phi_{\epsilon,\tau}^{\tilde{H}_{k}}(p',q)-H(p',q)\right)\right].\label{eq:kmc_accept_prob}
\end{equation}

\end_inset

Consequently, any deviations of the level sets induced by 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:kernel_leap_frog_flow"

\end_inset

 from those induced by 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:leap_frog_flow"

\end_inset

 will result in a decreased acceptance probability of approximate integrations
 
\begin_inset Formula $\alpha(\phi_{\epsilon,\tau}^{\tilde{H_{k}}}(p',q))$
\end_inset

.
 We therefore need to control the approximation quality of the kernel induced
 potential energy to maintain good performance in practice.
 At this point, it is unclear, whether and how finite sample rates for estimatin
g infinite exponential family models, 
\begin_inset CommandInset citation
LatexCommand cite
key "SriFukKumGreHyv14"

\end_inset

, can be translated to (bound?) the difference in acceptance probabilities
 
\begin_inset Formula 
\[
\vert\alpha(\phi_{\epsilon,\tau}^{\tilde{H}_{k}}(p',q))-\alpha(\phi_{\epsilon,\tau}^{\tilde{H}}(p',q))\vert
\]

\end_inset

Further complication comes from the approximation to the solution of the
 density estimator described in section 
\begin_inset CommandInset ref
LatexCommand eqref
reference "sub:finite_approx_infinite_exp_family"

\end_inset

.
 Consistency here is does not really help as gradient subsamples in the
 sense of 
\begin_inset CommandInset citation
LatexCommand cite
key "ChenICML2014"

\end_inset

 are consistent but are shown to exhibit catastrophic performance as demonstrate
d in 
\begin_inset CommandInset citation
LatexCommand cite
key "Betancourt2015"

\end_inset

.
 However, we provide a thorough numerical study in the experiments.
\end_layout

\begin_layout Paragraph
An illustrative example
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "par:illustrative_example"

\end_inset


\end_layout

\begin_layout Standard
We now illustrate how well trajectories along the level set of the kernel
 induced Hamiltonian flow are aligned with their true counterparts.
 We begin with a simple toy example of a Hamiltonian 
\begin_inset Formula $H(p,q)=K(p)+U(q)$
\end_inset

 defined by standard Gaussian both momentum 
\begin_inset Formula $K(p)=-\frac{1}{2}p^{T}p$
\end_inset

 target density, i.e.
 
\begin_inset Formula $U(q)=\frac{1}{2}q^{T}q$
\end_inset

.
 We fit the approximate infinite exponential family model using 
\begin_inset Formula $500$
\end_inset

 points with 
\begin_inset Formula $\lambda=1$
\end_inset

 and a Gaussian kernel with 
\begin_inset Formula $\sigma=500$
\end_inset

.
 Starting from a point in 
\begin_inset Formula $(p,q)$
\end_inset

-space, we simulate approximate trajectories on both true and kernel induced
 Hamiltonian level sets, using 
\begin_inset Formula $L=300$
\end_inset

 leapfrog steps of size 
\begin_inset Formula $\epsilon=0.1$
\end_inset

.
 For each point on the discrete trajectory, we compute the Metropolis acceptance
 probability in  
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:hmc_accept_prob"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:kmc_accept_prob"

\end_inset

.
 See Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:kmc_trajectories"

\end_inset

 for an example trajectory.
 For plain HMC, the average acceptance probability quantifies how well the
 leap-frog integration of the level sets align with the true level set --
 and is usually large.
 For kernel induced Hamiltonian dynamics, the average acceptance probability
 quantifies both the integration error 
\emph on
and
\emph default
 how well the kernel induced Hamiltonian flow matches the true Hamiltonian
 flow -- where we are interested in the latter and will investigate it in
 the experiments.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/gaussian_trajectories_hmc.eps

\end_inset


\begin_inset Graphics
	filename figures/gaussian_trajectories_kmc.eps

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/gaussian_trajectories_momentum_hmc.eps

\end_inset


\begin_inset Graphics
	filename figures/gaussian_trajectories_momentum_kmc.eps

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/gaussian_trajectories_acceptance_hmc.eps

\end_inset


\begin_inset Graphics
	filename figures/gaussian_trajectories_acceptance_kmc.eps

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:kmc_trajectories"

\end_inset

Hamiltonian trajectories on 2-dimensional standard Gaussian both target
 and momentum.
 Starting (red stars) at 
\begin_inset Formula $q_{0}=(-1,-1)^{T}$
\end_inset

, we randomly sample momentum and simulate for 
\begin_inset Formula $L=200$
\end_inset

 leap-frog steps of size 
\begin_inset Formula $\epsilon=0.1$
\end_inset

.
 End points of such trajectories (blue stars) form the proposal of HMC-like
 algorithms.
 The acceptance probability is reported on the bottom.
 
\series bold
Left:
\series default
 Plain Hamiltonian trajectories oscillate on a stable orbit.
 Acceptance prbability according to 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:kmc_accept_prob"

\end_inset

 
\series bold
Right:
\series default
 Kernel induced trajectories on an empirical energy function estimated from
 
\begin_inset Formula $N=200$
\end_inset

 samples (blue points) with 
\begin_inset Formula $\lambda=0.001$
\end_inset

 and a Gaussian kernel with 
\begin_inset Formula $\sigma=1$
\end_inset

.
 Acceptance prbability according to 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:kmc_accept_prob"

\end_inset

.
 The trajectories slightly drift compared to the true Hamiltonian flow.
 The acceptance probability oscilates in a larger interval while its average
 is still high.
 Quantification of this difference is crucial for kernel induced Hamiltonian
 dynamics being useful in practice.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Geometry extension (TODO)
\end_layout

\begin_layout Itemize
Can we use second order information of our density estimate? Then run RM-HMC
 or M-MALA
\end_layout

\begin_layout Itemize
What about the Kameleon proposal as a Hessian estimate? Formal relationship?
 In practice
\end_layout

\begin_layout Section
Kamiltonian Monte Carlo
\end_layout

\begin_layout Standard
We now describe how to utilise the described kernel induced Hamiltonian
 flow in order to construct a gradient free, adaptive kernel Hamiltonian
 Monte Carlo algorithm: 
\emph on
Kamiltonian Monte Carlo, 
\emph default
see Algorithm 
\begin_inset CommandInset ref
LatexCommand eqref
reference "alg:Kamiltonian-Monte-Carlo"

\end_inset

.
\end_layout

\begin_layout Standard
The remainder of this section covers details on adaptation schedules needed
 to ensure both asymptotic correctness and computational feasibility, and
 describe various tuning mechanisms for applying KMC in practice.
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
placement H
wide false
sideways false
status open

\begin_layout Plain Layout

\series bold
\begin_inset Caption Standard

\begin_layout Plain Layout

\series bold
Kamiltonian Monte Carlo
\begin_inset CommandInset label
LatexCommand label
name "alg:Kamiltonian-Monte-Carlo"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\emph on
Input
\emph default
: target 
\begin_inset Formula $\pi$
\end_inset

, momentum 
\begin_inset Formula $\exp(-K(p))$
\end_inset

, adaptation weights 
\begin_inset Formula $a_{t}$
\end_inset

, random basis & offset 
\begin_inset Formula $\{\omega_{i},b_{i}\}_{i=1}^{m}$
\end_inset


\end_layout

\begin_layout Itemize
At iteration 
\begin_inset Formula $t+1$
\end_inset

, previous state 
\begin_inset Formula $x_{t}$
\end_inset


\end_layout

\begin_layout Itemize

\shape italic
Online-update density estimate.
\end_layout

\begin_deeper
\begin_layout Enumerate
Update sums 
\begin_inset Formula 
\begin{equation}
\tilde{b}_{t}\leftarrow\tilde{b}_{t-1}-a_{t}\sum_{\ell=1}^{d}\ddot{\phi}_{x_{t}}^{\ell}\quad\text{and}\quad\tilde{C}_{t}\leftarrow\tilde{C}_{t-1}+\frac{a_{t}}{2}\sum_{\ell=1}^{d}\dot{\phi}_{x_{t}}^{\ell}(\dot{\phi}_{x_{t}}^{\ell})^{T}\label{eq:algo_updating_b_C}
\end{equation}

\end_inset

 and perform rank-
\begin_inset Formula $d$
\end_inset

 update to obtain updated Cholesky factorisation 
\begin_inset Formula $\tilde{L}_{t}\tilde{L}_{t}^{T}=\tilde{C}_{t}$
\end_inset

.
\end_layout

\begin_layout Enumerate
Update approximate infinite exponential family parameters
\end_layout

\begin_layout Plain Layout
\begin_inset Formula 
\[
\hat{\theta}\leftarrow\tilde{L}_{t}^{-T}\tilde{L}_{t}^{-1}\tilde{b}_{t}
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize

\shape italic
Kernel induced Hamiltonian proposal.
\end_layout

\begin_deeper
\begin_layout Enumerate
\begin_inset ERT
status open

\begin_layout Plain Layout

[5.]
\backslash
setcounter{enumi}{3}
\end_layout

\end_inset

Sample momentum 
\begin_inset Formula $p'\sim\exp(-K(p))$
\end_inset


\end_layout

\begin_layout Enumerate
Using 
\begin_inset Formula $\hat{\theta}$
\end_inset

, simulate kernel Hamiltonian flow 
\begin_inset Formula $(p^{*},x^{*})=\phi_{\epsilon,\tau}^{\tilde{H}_{k}}(p',x_{t})$
\end_inset


\end_layout

\begin_layout Enumerate
Perform Metropolis step 
\begin_inset Formula 
\begin{align*}
x_{t+1} & =\begin{cases}
x^{*} & \text{with probability }\alpha(\phi_{\epsilon,\tau}^{\tilde{H}_{k}}(p',x_{t}))\\
x_{t} & \text{otherwise}
\end{cases}
\end{align*}

\end_inset

 
\end_layout

\end_deeper
\end_inset


\end_layout

\begin_layout Subsection
Vanishing adaptation for asymptotic correctness
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "sub:adaptive_subsampling"

\end_inset


\end_layout

\begin_layout Standard
It is well known that construction of MCMC algorithms that utilise the history
 of the Markov chain for constructing proposals yields asymptotic bias.
 Recent kernel based proposals in 
\begin_inset CommandInset citation
LatexCommand cite
key "sejdinovic_kernel_2014"

\end_inset

 adapted the idea of 
\begin_inset Quotes eld
\end_inset

vanishing adaptation
\begin_inset Quotes erd
\end_inset

, 
\begin_inset CommandInset citation
LatexCommand cite
key "Andrieu2008"

\end_inset

, to avoid such biases.
 They update a fixed number of 
\begin_inset Formula $n$
\end_inset

 random subsamples of the Markov chain history with a decaying probability.
 While this ensures asymptotic correctness, it is clearly undesirable to
 only use parts of the chain history.
\end_layout

\begin_layout Standard
As established in Section 
\begin_inset CommandInset ref
LatexCommand eqref
reference "sub:rank_d_updates"

\end_inset

, updating the density estimator in Algorithm 
\begin_inset CommandInset ref
LatexCommand eqref
reference "alg:Kamiltonian-Monte-Carlo"

\end_inset

 for a new point costs 
\begin_inset Formula ${\cal O}(dm^{2})$
\end_inset

, which allows us to update the proposal in 
\emph on
every 
\emph default
iteration of the Markov chain.
 To guarantee convergence to the correct stationary distribution, we introduce
 vanishing adaptation weights 
\begin_inset Formula $\left\{ a_{t}\right\} _{i=0}^{\infty}$
\end_inset

 such that 
\begin_inset Formula 
\[
\lim_{t\to\infty}a_{t}=0\quad\text{and}\quad\sum_{t=0}^{\infty}a_{t}=\infty,
\]

\end_inset

and update the sums in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:algo_updating_b_C"

\end_inset

 accordingly.
 
\begin_inset CommandInset citation
LatexCommand cite
after "Theorem 1"
key "RobertsRosenthal2007"

\end_inset

 guarantees that the resulting Markov kernel is ergodic and converges to
 the correct target.
 
\emph on
TODO: Really? Carefull here!
\end_layout

\begin_layout Subsection
Learning free parameters
\end_layout

\begin_layout Standard
KMC has two free parameters, the Gaussian kernel bandwidth 
\begin_inset Formula $\sigma$
\end_inset

, and the regularisation parameter 
\begin_inset Formula $\lambda$
\end_inset

.
 Earlier adaptive kernel-based MCMC methods, 
\begin_inset CommandInset citation
LatexCommand cite
key "sejdinovic_kernel_2014"

\end_inset

, did not cover choice of kernel parameters, i.e.
 the Gaussian bandwidth 
\begin_inset Formula $\sigma$
\end_inset

 in our case.
 As the choice of such parameters is crucial to obtain an efficient sampler,
 this clearly limits its generality.
 While in practice, pilot runs can to be devised in order to tune the parameter,
 a general framework to cover their choice is desirable.
\end_layout

\begin_layout Standard
As KMC's performance is eventually tied with the accuracy of the infinite
 exponential family model in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:infinite_exp_family"

\end_inset

 and its approximate estimator in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:infinite_exp_family_random_feats"

\end_inset

, we can the score matching objective function in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:score_match_objective_random_feats"

\end_inset

 to learn 
\begin_inset Formula $\sigma,\lambda$
\end_inset

.
 In order to avoid overfitting, we use a standard cross-validation procedure
 to evaluate the objective function on a test set that was held out during
 the fitting procedure.
 We can then use the cross-validated estimate of 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:score_matching_objective_sample_version"

\end_inset

 to compare different 
\begin_inset Formula $\sigma,\lambda$
\end_inset

 pairs with each other.
 As the 
\begin_inset Formula $\sigma,\lambda$
\end_inset

 surface is highly non-convex, we have to resort to global optimisation
 procedures, such as the computationally expensive grid-search 
\begin_inset CommandInset citation
LatexCommand cite
key "hsu2003practical"

\end_inset

, or faster alternatives from stochastic 
\begin_inset CommandInset citation
LatexCommand cite
key "hansen1996adapting"

\end_inset

 or Bayesian optimisation 
\begin_inset CommandInset citation
LatexCommand cite
key "hernandez2014predictive"

\end_inset

.
\end_layout

\begin_layout Standard
Figure shows the 
\begin_inset Formula $\sigma,\lambda$
\end_inset

 for the 
\begin_inset Formula $2$
\end_inset

-dimensional Gaussian and Banana examples from Section 
\begin_inset CommandInset ref
LatexCommand eqref
reference "par:illustrative_example"

\end_inset

.
\end_layout

\begin_layout Subsection
Smoothing and and annealing interpretation
\end_layout

\begin_layout Standard
Write about the fact that we have an 
\begin_inset Quotes eld
\end_inset

annealing
\begin_inset Quotes erd
\end_inset

 interpretation of the smoothing.
\end_layout

\begin_layout Subsection
Incorporating geometric approaches
\end_layout

\begin_layout Standard
Write about how this can be turned into a geometric version aka RM-HMC
\end_layout

\begin_layout Section
Experiments
\end_layout

\begin_layout Subsection
Trajectories in high dimensions
\end_layout

\begin_layout Standard
In order to quantify KMC's behaviour in high dimensions, we study average
 acceptance probabilities along trajectories as described in section 
\begin_inset CommandInset ref
LatexCommand eqref
reference "par:illustrative_example"

\end_inset

.
 We use the following two synthetic examples with increasing dimension:
 a standard Gaussian and the Banana described in 
\begin_inset CommandInset citation
LatexCommand citet
key "Haario1999"

\end_inset

, both with standard Gaussian momentum.
\end_layout

\begin_layout Standard
Figure 
\begin_inset CommandInset ref
LatexCommand eqref
reference "fig:kmc_trajectories_mean_acceptance"

\end_inset

 shows the average acceptance probability along a trajectory as a function
 of dimension of 
\begin_inset Formula $p$
\end_inset

 and 
\begin_inset Formula $q$
\end_inset

 in the same spirit as 
\begin_inset CommandInset citation
LatexCommand cite
after "Figure 3"
key "Betancourt2015"

\end_inset

.
 Error bars are obtained over 
\begin_inset Formula $100$
\end_inset

 trials in each of which we fit the infinite exponential family estimator
 on 
\begin_inset Formula $n=500$
\end_inset

 randomly sampled data and then run a random number of leapfrog steps in
 
\begin_inset Formula $[100,1000]$
\end_inset

 on both true and kernel induced flow.
 The plot shows that while the spread of the acceptance rate gets worse
 in high dimensions, most of the time, it is close to the true Hamiltonian
 ground truth.
 Note that in order to ensure that the kernel induced Hamiltonian flow cannot
 
\begin_inset Quotes eld
\end_inset

cheat
\begin_inset Quotes erd
\end_inset

 via oscillating in small areas of the space, we monitored the volume of
 the levels sets and ensured they are roughly equal.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/average_accept_gaussian_target.eps

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:kmc_trajectories_mean_acceptance"

\end_inset

Behaviour of kernel induced Hamiltonian flow in high dimensions.
 We show quantiles of the distribution of the average acceptance probability
 along a simulated trajectory (true Hamiltonian flow has negligible variance).
 While worst-case acceptance probability of the kernel induced flow suffers
 in high dimensions, the median acceptance probability is remarkably high
 and close to the true Hamiltonian ground truth.
 
\series bold
Left:
\series default
 Gaussian.
 
\series bold
Right:
\series default
 Banana.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Comparison of trajectories
\end_layout

\begin_layout Standard
Empirically compare on how well we can approximate the true HMC trajectories
\end_layout

\begin_layout Itemize
Kernel choice via cross-validation and how we get most similar trajectories
\end_layout

\begin_layout Itemize
Optimise acceptance rate? Is this even well conditioned? Plot 
\begin_inset Formula $J(\alpha)$
\end_inset

 and ESS, and see whether cross-validation kernel is good for mixing
\end_layout

\begin_layout Subsection
Pseudo-Marginal MCMC
\end_layout

\begin_layout Itemize
Reproduce Kameleon experiments from Kameleon paper with KHMC
\end_layout

\begin_layout Itemize
Higher dimensional, also compare to plain HMC, (and random walk)
\end_layout

\begin_layout Itemize
Mushroom as a cool dataset?
\end_layout

\begin_layout Subsection
Comparisons to geometry HMC?
\end_layout

\begin_layout Standard
For that, first need to work out how we would even do that.
\end_layout

\begin_layout Section
Conclusions
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "local"
options "unsrt"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
normalsize
\end_layout

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
onecolumn
\end_layout

\end_inset


\end_layout

\begin_layout Section
\start_of_appendix
Proofs
\end_layout

\begin_layout Subsection
Consistency?
\end_layout

\end_body
\end_document
