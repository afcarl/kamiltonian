#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\begin_preamble
\usepackage{nips14submit_e,times}
\usepackage{hyperref}
\usepackage{url}
%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09


\title{Kamiltonian Monte Carlo}


\author{
Heiko Strathmann%\thanks{ Use footnote for providing further information about author (webpage, alternative address)---\emph{not} for acknowledging funding agencies.}
\\
Gatsby Unit\\
University College London \\
\texttt{heiko.strathmann@gmail.com} \\
\And
Dino Sejdinovoc \\
Department of Statistics \\
University of Oxford \\
\texttt{dino.sejdinovic@gmail.com} \\
\AND
Arthur Gretton \\
Gatsby Unit\\
University College London \\
\texttt{athur.gretton@gmail.com} 
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\nipsfinalcopy % Uncomment for camera-ready version
\end_preamble
\use_default_options false
\begin_modules
theorems-std
\end_modules
\maintain_unincluded_children false
\language british
\language_package none
\inputencoding auto
\fontencoding default
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize letterpaper
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 0
\use_package mathtools 1
\use_package mhchem 0
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plainnat
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 2
\tocdepth 2
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\diag}{\text{diag}}
{\text{diag}}
\end_inset


\end_layout

\begin_layout Title
Kamiltonian Monte Carlo
\end_layout

\begin_layout Abstract
We propose an adaptive Kernel Hamiltonian Monte Carlo (KMC) algorithm to
 simulate from an arbritary target probability density.
 Our sampler fills a gap: when the density is intractable, classic HMC methodolo
gy 
\emph on
cannot
\emph default
 be applied -- one is left with random walk methods which suffer from bad
 mixing behaviour.
 We extend recent ideas of adaptively learning target covariance structure
 in a Reproducing Kernel Hilbert Space (RKHS), based on the history of the
 Markov chain.
 But rather than 
\emph on
locally 
\emph default
smoothing the chain history, we directly model its 
\emph on
global
\emph default
 log-density as an RKHS function via fitting an approximate infinite exponential
 family model.
 This is not only a more interpretable and well-posed problem, but it also
 provides a differentiable log-density approximation, which can directly
 be used to simulate trajectories from a Hamiltonian dynamical system.
 We construct an 
\emph on
exact 
\emph default
MCMC algorithm that (i) behaves similar to HMC in terms of autocorrelation
 and acceptance rates, but that (ii) does not require target gradients,
 and that in particular (iii) works well in high dimensions.
 We support our claims with experimental studies.
\end_layout

\begin_layout Paragraph
Next steps (Heiko)
\end_layout

\begin_layout Itemize

\strikeout on
Proof read Gaussian kernel score matching
\end_layout

\begin_layout Itemize

\strikeout on
Implement carefully
\end_layout

\begin_layout Itemize

\strikeout on
Implement trajectories, possibly within a given framework
\end_layout

\begin_layout Itemize

\strikeout on
Create plots for illustration
\end_layout

\begin_layout Itemize

\strikeout on
Write framework to compute average acceptance probability of a trajectory
\end_layout

\begin_layout Itemize
Write section that empirically analyses approximate trajectories on Gaussians
 and Banana in terms of dimension
\end_layout

\begin_layout Itemize
Write section on optimising 
\begin_inset Formula $\lambda,\sigma$
\end_inset

 via x-validation or stochastic optimisation
\end_layout

\begin_layout Itemize
Adapt notation and keywords from 
\begin_inset CommandInset citation
LatexCommand cite
key "Betancourt2015"

\end_inset

: Hamiltonian flow, level set
\end_layout

\begin_layout Itemize
find out whats next
\end_layout

\begin_layout Paragraph*
Experiments to do
\end_layout

\begin_layout Itemize
Write framework for doing trajectory comparisons on cluster
\end_layout

\begin_layout Itemize
Set up NUTS sampler for using pymc for kamiltonian and hamiltonian
\end_layout

\begin_layout Itemize
Set up sampling on cluster
\end_layout

\begin_layout Itemize
Create surface plots for the x-validation of 
\begin_inset Formula $\lambda,\sigma$
\end_inset


\end_layout

\begin_layout Standard
Trajectory experiment on Gaussian.
 To illustrate that it maintains acceptance prob in high dimensions
\end_layout

\begin_layout Enumerate
Increase 
\begin_inset Formula $d$
\end_inset

 for a fixed number of data.
 Should at some point fall apart
\end_layout

\begin_layout Enumerate
Increase number of data to show that 
\begin_inset Quotes eld
\end_inset

falling apart
\begin_inset Quotes erd
\end_inset

 happens later.
\end_layout

\begin_layout Enumerate
Translate to a guideline how many samples one needs to achieve similar performan
ce in high dimensions (exponential?)
\end_layout

\begin_layout Enumerate
Use low-rank Cholesky to exlore limitations of scaling (appendix: compare
 low-rank with exact)
\end_layout

\begin_layout Enumerate
Sanity check: is covered volume roughly equal?
\end_layout

\begin_layout Standard
Quantiles on Banana.
 To illustrate that it continues to explore tails in high dimensions
\end_layout

\begin_layout Enumerate
Same as above, but with MCMC and NUTS
\end_layout

\begin_layout Standard
Need something on burnin time.
\end_layout

\begin_layout Standard

\series bold
Open questions:
\end_layout

\begin_layout Itemize
Is the value of the approximate hamiltonian bounded?
\end_layout

\begin_layout Itemize
Is the distance of the approximate hamiltonian and the true hamiltonian
 bounded? Sam mentioned that for leapfrog, the distance between true level
 set and discretised version is bounded.
 See Geometric Numerical Integration by Hairer and Lubich and reference
 is Neal's chapter 5
\end_layout

\begin_layout Itemize
Maximum distance of trajectory to Hamiltonian contour (2d plot), how does
 it behave?
\end_layout

\begin_layout Itemize
What do the tails of the desnity estimate do?
\end_layout

\begin_layout Itemize
Convergence rates of estimator translated into guarantees of the markov
 chain?
\end_layout

\begin_layout Itemize
Multimodality smoothing might be nice as an extension
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Itemize
Why MCMC
\end_layout

\begin_layout Itemize
Intractable densities
\end_layout

\begin_layout Itemize
Kernel Metropolis Hastings 
\begin_inset CommandInset citation
LatexCommand citep
key "sejdinovic_kernel_2014"

\end_inset

 and pros/cons
\end_layout

\begin_layout Itemize
Want: HMC like sampler
\end_layout

\begin_layout Itemize
Idea: Hamiltonian dynamics on kernel energy as proposal
\end_layout

\begin_layout Itemize
Give paper outline
\end_layout

\begin_layout Itemize
Here are some plots that might be useful
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
target "http://nbviewer.ipython.org/gist/anonymous/329d11916eb29743a7de"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Paragraph
Paper outline
\end_layout

\begin_layout Standard
Section
\end_layout

\begin_layout Section
Background & Previous work
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "sec:previous_work"

\end_inset


\end_layout

\begin_layout Subsection
Notation
\end_layout

\begin_layout Itemize
Unnormalised target density 
\begin_inset Formula $\pi:\mathbb{R^{d}}\to\mathbb{R}$
\end_inset

 , unbiased estimator 
\begin_inset Formula $\mathbb{E}[\hat{\pi}(\theta)]=\pi(\theta)$
\end_inset


\end_layout

\begin_layout Itemize
Markov chain history at iteration 
\begin_inset Formula $t$
\end_inset

 with current position 
\begin_inset Formula $x_{t}$
\end_inset

: 
\begin_inset Formula $\left\{ x_{i}\right\} _{i=0}^{t-1}$
\end_inset


\end_layout

\begin_layout Itemize
History subsample 
\begin_inset Formula $\mathbf{z}=\left\{ z_{i}\right\} _{i=1}^{n}\subseteq\left\{ x_{i}\right\} _{i=0}^{t-1}$
\end_inset

 
\end_layout

\begin_layout Standard
HMC:
\end_layout

\begin_layout Itemize
Number of leapfrog steps, 
\begin_inset Formula $L$
\end_inset


\end_layout

\begin_layout Itemize
Step size 
\begin_inset Formula $\epsilon$
\end_inset


\end_layout

\begin_layout Subsection
Problem setting
\end_layout

\begin_layout Standard
Write what we want to do and what are important parts of it.
\end_layout

\begin_layout Subsection
Previous work
\end_layout

\begin_layout Standard
Write about Adaptive MCMC, 
\begin_inset CommandInset citation
LatexCommand cite
key "Haario1999"

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "Andrieu2008"

\end_inset

, Kameleon MCMC, 
\begin_inset CommandInset citation
LatexCommand citet
key "sejdinovic_kernel_2014"

\end_inset

, and how it improves over adaptive MH.
 Mention downsides and what we would like to have.
 In particular the pseudo-marginal case, 
\begin_inset CommandInset citation
LatexCommand cite
key "beaumont2003estimation"

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "Andrieu2009a"

\end_inset

.
\end_layout

\begin_layout Subsection
Hamiltonian Monte Carlo
\begin_inset Note Note
status open

\begin_layout Plain Layout
AG: we don't really need a separate section on mean embeddings
\end_layout

\end_inset


\end_layout

\begin_layout Standard
We really would like to do HMC.
 Here is what it is.
\end_layout

\begin_layout Standard
Following 
\begin_inset CommandInset citation
LatexCommand citet
key "Neal2010"

\end_inset

, Hamiltonian Monte Carlo is based around a fictious dynamical system of
 particles 
\begin_inset Formula $(p,q)\in{\cal X}\times{\cal X}$
\end_inset

 whose evolution in time is described by the Hamiltonian,
\begin_inset Formula 
\begin{align*}
H(p,q) & =K(p)+U(q),
\end{align*}

\end_inset

which consists of potential energy 
\begin_inset Formula $U(q)$
\end_inset

, the log-pdf of the target 
\begin_inset Formula $\pi$
\end_inset

, and a kinetic energy function 
\begin_inset Formula $K(p)$
\end_inset

, usually a Gaussian.
 Evolution of the system,
\begin_inset Formula 
\begin{align*}
\frac{dq_{i}}{dt} & =\frac{\partial H}{\partial p_{i}}\\
\frac{dp_{i}}{dt} & =-\frac{\partial H}{\partial q_{i}},
\end{align*}

\end_inset

leaves the value of 
\begin_inset Formula $H$
\end_inset

 constant.
 Refering to 
\begin_inset CommandInset citation
LatexCommand citep
key "Neal2010"

\end_inset

 for details, such trajectories of 
\begin_inset Formula $(p,q)$
\end_inset

 along the contour lines of 
\begin_inset Formula $H$
\end_inset

 can be used to construct proposals for a Metropolis-Hastings type algorithm:
\end_layout

\begin_layout Enumerate
Re-sample momentum 
\begin_inset Formula $p'\sim K(p)$
\end_inset

.
 This produces a new point 
\begin_inset Formula $(p',q)$
\end_inset

 which has the same distribution as 
\begin_inset Formula $(p,q)$
\end_inset

 since 
\begin_inset Formula $p'$
\end_inset

 comes from the true marginal distribution of the chain.
 Note that the value of 
\begin_inset Formula $H$
\end_inset

 is changed in this step, i.e., we jump to another contour of 
\begin_inset Formula $H$
\end_inset

.
\end_layout

\begin_layout Enumerate
Starting from 
\begin_inset Formula $(p',q)$
\end_inset

, simulate the system for a number of discrete time steps to obtain 
\begin_inset Formula $(p^{*},q^{*})$
\end_inset

.
 This step requires access to gradients of 
\begin_inset Formula $H$
\end_inset

, and therefore of the log-pdf of the target 
\begin_inset Formula $\log\pi(\cdot)=U(\cdot)$
\end_inset

.
\end_layout

\begin_layout Enumerate
In order to correct for discretisation errors arising from numerical simulation,
 accept this proposal according to its new value of 
\begin_inset Formula $H(p^{*},q^{*})$
\end_inset

 using a Metorpolis-Hastings step with an acceptance probability 
\begin_inset Formula 
\begin{align*}
 & \min\left[1,\exp\left(H(p^{*},q^{*})-H(p',q)\right)\right]
\end{align*}

\end_inset

Note that this step does not require gradients but only evaluation of the
 target.
\end_layout

\begin_layout Standard
When iterating, the resulting Markov chain in 
\begin_inset Formula $(p,q)$
\end_inset

-space can be marginalised by simply dropping all 
\begin_inset Formula $p$
\end_inset

 components.
 One can show that 
\begin_inset Formula $\exp(-U(\cdot))\propto\pi(\cdot)$
\end_inset

 is the invariant distribution.
 Furthermore, acceptance rate of the third step will be usually large, while
 autocorrelation is small.
\end_layout

\begin_layout Paragraph
HMC and intractable densities
\end_layout

\begin_layout Standard
Write about how HMC cannot be applied if gradients is not known.
\end_layout

\begin_layout Standard
Kamiltonian Monte Carlo is based on the idea of replacing potential energy
 
\begin_inset Formula $U$
\end_inset

 in the second step by a kernel induced surrogate, that does not require
 gradients of the log-pdf of the target.
 Invariant distribution will remain unchanged as we 
\emph on
do not
\emph default
 change anything in the third step.
 We now give a brief review of score matching that we use to estimate potential
 energy.
\end_layout

\begin_layout Subsection
Score Matching
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout

\series bold
AG:
\series default
 Score matching references, 
\begin_inset CommandInset citation
LatexCommand citet
key "Hyvarinen-05,Hyvarinen-07"

\end_inset

.
 The latter gave solutions in terms of a finite mixture at fixed centres.
 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Following 
\begin_inset CommandInset citation
LatexCommand citet
key "Hyvarinen-05"

\end_inset

, we assume that a variable 
\begin_inset Formula $\xi\sim p_{x}$
\end_inset

 has some unknown probability density function 
\begin_inset Formula $p_{x}(\cdot)$
\end_inset

 defined on 
\begin_inset Formula $\mathbb{R}^{d}$
\end_inset

, and that the log probability takes the form 
\begin_inset Formula 
\[
\log p(\xi;\alpha)=\log q(\xi;\alpha)-\log Z(\alpha),
\]

\end_inset

 where 
\begin_inset Formula $\alpha$
\end_inset

 is a vector of parameters of yet unspecified dimension.
 We aim to approximate 
\begin_inset Formula $p_{x}(\cdot)$
\end_inset

 by 
\begin_inset Formula $p(\cdot;\hat{\theta})$
\end_inset

, i.e., to find 
\begin_inset Formula $\hat{\theta}$
\end_inset

 from a set of fixed i.i.d.
 samples 
\begin_inset Formula $\{x_{i}\sim p_{x}\}_{i=1}^{n}$
\end_inset

.
 From 
\begin_inset CommandInset citation
LatexCommand citet
after "equation. 2"
key "Hyvarinen-05"

\end_inset

, the criterion being optimized is the expected squared distance between
 score functions,
\emph on
 
\emph default

\begin_inset Formula 
\[
J(\alpha)=\frac{1}{2}\int_{\xi}p_{x}(\xi)\left\Vert \psi(\xi;\alpha)-\psi_{x}(\xi)\right\Vert ^{2}d\xi,
\]

\end_inset

where 
\begin_inset Formula 
\[
\psi(\xi;\alpha)=\nabla_{\xi}\log p(\xi;\alpha),
\]

\end_inset

and 
\begin_inset Formula $\psi_{x}(\xi)$
\end_inset

 is the derivative wrt 
\begin_inset Formula $\xi$
\end_inset

 of the unknown true density 
\begin_inset Formula $p_{x}(\xi)$
\end_inset

.
 As proved in 
\begin_inset CommandInset citation
LatexCommand citet
after "Theorem 1"
key "Hyvarinen-05"

\end_inset

, it is possible to express 
\begin_inset Formula $J(\alpha)$
\end_inset

 without access to the unknown 
\begin_inset Formula $\psi_{x}(\xi)$
\end_inset

 as
\begin_inset Formula 
\[
J(\alpha)=\int_{\xi}p_{x}(\xi)\sum_{\ell=1}^{d}\left[\partial_{\ell}\psi_{\ell}(\xi;\alpha)+\frac{1}{2}\psi_{\ell}(\xi;\alpha)^{2}\right]d\xi,
\]

\end_inset

where
\begin_inset Formula 
\[
\psi_{\ell}(\xi;\alpha)=\frac{\partial\log q(\xi;\alpha)}{\partial\xi_{\ell}}
\]

\end_inset

and
\begin_inset Formula 
\[
\partial_{\ell}\psi_{\ell}(\xi;\alpha)=\frac{\partial^{2}\log q(\xi;\alpha)}{\partial\xi_{\ell}^{2}}.
\]

\end_inset

Replacing the integral 
\begin_inset Formula $\int_{\xi}p_{x}(\xi)$
\end_inset

 with an average over the samples 
\begin_inset Formula $x_{i}$
\end_inset

 gives us a sample version of 
\begin_inset Formula $J(\alpha)$
\end_inset

, minimising of which is consistent,
\begin_inset Formula 
\begin{equation}
\hat{J}(\alpha)=\frac{1}{m}\sum_{\ell=1}^{d}\sum_{i=1}^{n}\left[\partial_{\ell}\psi_{\ell}(x_{i};\alpha)+\frac{1}{2}\psi_{\ell}^{2}(x_{i};\alpha)\right].\label{eq:score_matching_objective_sample_version}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "sec:kernel_hmc"

\end_inset


\end_layout

\begin_layout Standard
We now combine score matching with HMC.
\end_layout

\begin_layout Section
Kernel Hamiltonian Dynamics for MCMC
\end_layout

\begin_layout Subsection
Kernel Score Matching
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
AG: I think 
\begin_inset Quotes eld
\end_inset

regularization and smoothing
\begin_inset Quotes erd
\end_inset

 should go here: any empirical solution will already be regularized, so
 it doesn't make sense to have a separate regularization section.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Recall our aim is to replace the potential energy function in HMC, which
 is the log-pdf of the target 
\begin_inset Formula $\pi$
\end_inset

, by a surrogate whose gradients can be computed easily.
 More specifically, we will use kernelised score matching to fit the target
 log-pdf with a RKHS function, which corresponds to fitting an exponential
 family model in the RKHS.
 Elements 
\begin_inset Formula $f:{\cal X}\to\mathbb{R}$
\end_inset

 of an RKHS 
\begin_inset Formula ${\cal H}$
\end_inset

 uniquely induced by a reproducing kernel 
\begin_inset Formula $k:{\cal X}\times{\cal X}\to\mathbb{R}$
\end_inset

 can be written as linear combinations of feature maps, 
\begin_inset Formula $f(\cdot)=\sum_{i=1}^{m}\alpha_{i}k(x_{i},\cdot)$
\end_inset

.
 That is, we simply assume the parametric form for the unnormalised log-pdf
\begin_inset Formula 
\[
\log q(\xi;\alpha)=\log\Pr(\xi;\alpha)+\log Z(\alpha)=\sum_{i=1}^{n}\alpha_{i}k(x_{i},\xi),
\]

\end_inset

where 
\begin_inset Formula $\alpha\in\mathbb{R}^{n}$
\end_inset

 is an 
\begin_inset Formula $n$
\end_inset

-dimensional parameter vector.
 Note that where we did not specify its dimension previously, its dimensionality
 now equals the number of training points 
\begin_inset Formula $\{x_{i}\sim p_{x}\}_{i=1}^{n}$
\end_inset

.
\end_layout

\begin_layout Paragraph
Gaussian Kernel
\end_layout

\begin_layout Standard
We now implement score matching for the well known Gaussian kernel 
\begin_inset Formula $k(x,y)=\exp\left(-\frac{\|x-y\|^{2}}{\sigma}\right)$
\end_inset

.
 Note that its corresponding RKHS is infinite dimensional, we are fitting
 an infinite exponential family model in the RKHS, see also 
\begin_inset CommandInset citation
LatexCommand citet
key "SriFukKumGreHyv14"

\end_inset

.
 The score function is then given by
\begin_inset Formula 
\[
\psi_{\ell}(\xi;\alpha)=\frac{2}{\sigma}\sum_{i=1}^{m}\alpha_{i}(x_{i\ell}-\xi_{\ell})\exp\left(-\frac{\|x_{i}-\xi\|^{2}}{\sigma}\right).
\]

\end_inset

Omitting intermediate details (see appendix 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:appendix_kernel_score_matching"

\end_inset

), the resulting sample objective function 
\begin_inset Formula $J(\alpha)$
\end_inset

 in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:score_matching_objective_sample_version"

\end_inset

 is globally minimised for
\begin_inset Formula 
\begin{equation}
\hat{\alpha}=\frac{-\sigma}{2}(C+\lambda I)^{-1}b,\label{eq:kernel_score_matching_linear_system}
\end{equation}

\end_inset

where we added a regulariser using the norm of 
\begin_inset Formula $\alpha$
\end_inset

, and defined
\begin_inset Formula 
\begin{align*}
b & =\sum_{\ell=1}^{D}\left(\frac{2}{\sigma}(Ks_{\ell}+D_{s_{\ell}}K\mathbf{1}-2D_{x_{\ell}}Kx_{\ell})-K\mathbf{1}\right)\in\mathbb{R}^{n}\\
C & =\sum_{\ell=1}^{D}\left[D_{x_{\ell}}K-KD_{x_{\ell}}\right]\left[KD_{x_{\ell}}-D_{x_{\ell}}K\right]\in\mathbb{R}^{n\times n},
\end{align*}

\end_inset

and
\begin_inset Formula 
\begin{align*}
K_{ij}: & =\exp\left(-\frac{\|x_{i}-x_{j}\|^{2}}{\sigma}\right)\\
s_{i\ell} & :=x_{i\ell}^{2}\\
x_{\ell} & :=\left[\begin{array}{ccc}
x_{1\ell} & \dots & x_{m\ell}\end{array}\right]^{\top}\\
D_{x_{\ell}} & :=\diag(x_{\ell}).
\end{align*}

\end_inset


\end_layout

\begin_layout Subsection
Examples
\end_layout

\begin_layout Standard
Put some nice plots to illustrate ideas (no experiments yet)
\end_layout

\begin_layout Itemize
Low dimensional Banana
\end_layout

\begin_layout Itemize
Smoothing property of regularisation
\end_layout

\begin_layout Itemize
High dimensional banana, projected to 2D
\end_layout

\begin_layout Itemize
Comparison of HMC and KHMC trajectories
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/banana_trajectories_hmc.eps

\end_inset


\begin_inset Graphics
	filename figures/banana_trajectories_kmc.eps

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/banana_trajectories_momentum_hmc.eps

\end_inset


\begin_inset Graphics
	filename figures/banana_trajectories_momentum_kmc.eps

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/banana_trajectories_hamiltonian_hmc.eps

\end_inset


\begin_inset Graphics
	filename figures/banana_trajectories_hamiltonian_kmc.eps

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Hamiltonian trajectories on 2-dimensional Banana target with a standard
 Gaussian momentum.
 Starting (red stars) at 
\begin_inset Formula $q_{0}=(0,-3)^{\top}$
\end_inset

, we randomly sample momentum 
\begin_inset Formula $p_{0}\approx(-0.33,-0.27)$
\end_inset

 and simulate for 
\begin_inset Formula $L=300$
\end_inset

 leap-frog steps of size 
\begin_inset Formula $\epsilon=0.1$
\end_inset

.
 The resulting 
\begin_inset Formula $(p,q)$
\end_inset

 (blue stars) is the proposal of HMC-like algorithms.
 The values of the Hamiltonians (
\series bold
bottom
\series default
) are shifted to a comparable level.
 
\series bold
Left:
\series default
 Plain Hamiltonian trajectories.
 
\series bold
Right:
\series default
 Kernel induced trajectories on an empirical energy function estimated from
 
\begin_inset Formula $N=200$
\end_inset

 samples (blue points) with 
\begin_inset Formula $\lambda=1$
\end_inset

 and a Gaussian kernel with 
\begin_inset Formula $\sigma=50$
\end_inset

.
 The trajectories explore the target density in a similar way as plain HMC.
 Values of the Hamiltonian oscilate more.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Approximate Hamiltonian flow
\end_layout

\begin_layout Standard
We now examine how well trajectories along the level set of the approximate
 Hamiltonian are aligned with their true counterparts.
 This is essential for performance within an MCMC algorithm as deviations
 from the true Hamiltonian level set result in decreasing acceptance probability.
 We begin with a simple toy example of a Hamiltonian defined by standard
 Gaussian momentum 
\begin_inset Formula $K(p)=\log{\cal N}(\mathbf{0},I)$
\end_inset

, and a standard Gaussian target density, i.e.
 
\begin_inset Formula $U(q)=\log{\cal N}(\mathbf{0},I)$
\end_inset

.
 We use an approximate density 
\begin_inset Formula $\tilde{U}(q)$
\end_inset

 obtained via the above approximate infinite exponential family model, fitted
 on 
\begin_inset Formula $n=500$
\end_inset

 points.
 Starting from a random sample from both 
\begin_inset Formula $K(p)$
\end_inset

 and 
\begin_inset Formula $U(q)$
\end_inset

, we simulate trajectories on both exact and approximate Hamiltonian 
\begin_inset Formula $H(p,q)$
\end_inset

 and 
\begin_inset Formula $\tilde{H}(p,q,)$
\end_inset

 respectively.
 We use 
\begin_inset Formula $L=1000$
\end_inset

 leapfrog step of size 
\begin_inset Formula $\epsilon=0.1$
\end_inset

.
 This procecure gives an average acceptance probability that quantifies
 how well the leap-frog integration along 
\begin_inset Formula $H(p,q)$
\end_inset

 and 
\begin_inset Formula $\tilde{H}(p,q,)$
\end_inset

 aligns with the true level set.
 We plot this probability as a function of dimension in the same spirit
 as 
\begin_inset CommandInset citation
LatexCommand cite
after "Figure 3"
key "Betancourt2015"

\end_inset


\end_layout

\begin_layout Subsection
Geometry extension (TODO)
\end_layout

\begin_layout Itemize
Can we use second order information of our density estimate? Then run RM-HMC
 or M-MALA
\end_layout

\begin_layout Itemize
What about the Kameleon proposal as a Hessian estimate? Formal relationship?
 In practice
\end_layout

\begin_layout Section
Kamiltonian Monte Carlo
\end_layout

\begin_layout Subsection
Proposal construction
\end_layout

\begin_layout Standard
Describe in words and then give algorithm
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Plain Layout

\series bold
Kamiltonian Monte Carlo (TODO)
\end_layout

\begin_layout Plain Layout

\emph on
Input
\emph default
: unnormalized target 
\begin_inset Formula $\pi$
\end_inset

, subsample size 
\begin_inset Formula $n$
\end_inset

, scaling parameters 
\begin_inset Formula $\nu,\gamma,$
\end_inset

 kernel 
\begin_inset Formula $k$
\end_inset

, 
\end_layout

\begin_layout Itemize
At iteration 
\begin_inset Formula $t+1$
\end_inset

,
\end_layout

\begin_deeper
\begin_layout Enumerate
Obtain a random subsample 
\begin_inset Formula $\mathbf{z}=\left\{ z_{i}\right\} _{i=1}^{n}$
\end_inset

 of the chain history 
\begin_inset Formula $\left\{ x_{i}\right\} _{i=0}^{t-1}$
\end_inset

,
\end_layout

\begin_layout Enumerate
Sample proposed point 
\begin_inset Formula $x^{*}$
\end_inset

 from 
\begin_inset Formula $q_{\mathbf{z}}(\cdot|x_{t})=\mathcal{N}(x_{t},\gamma^{2}I+\nu^{2}M_{\mathbf{z},x_{t}}HM_{\mathbf{z},x_{t}}^{\top})$
\end_inset

, 
\end_layout

\begin_layout Enumerate
Accept/Reject with the Metropolis-Hastings acceptance probability 
\begin_inset Formula $A(x_{t},x^{*})$
\end_inset

 in eq.
 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:MetAcceptProb"

\end_inset

,
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vspace{-0.5cm}
\end_layout

\end_inset

 
\end_layout

\begin_deeper
\begin_layout Plain Layout
\begin_inset Formula 
\begin{eqnarray*}
x_{t+1} & = & \begin{cases}
x^{*}, & \textrm{w.p.}\; A(x_{t},x^{*}),\\
x_{t}, & \textrm{w.p.}\;1-A(x_{t},x^{*}).
\end{cases}
\end{eqnarray*}

\end_inset


\end_layout

\end_deeper
\end_deeper
\end_inset


\end_layout

\begin_layout Subsection
Properties
\end_layout

\begin_layout Itemize
Properties of kernel energy function, consequences for algorithm
\end_layout

\begin_layout Itemize
Kernel choice via cross-validation (plot of objective function)
\end_layout

\begin_layout Itemize
Extensions:
\end_layout

\begin_deeper
\begin_layout Itemize
Second order information? 
\begin_inset CommandInset citation
LatexCommand citet
key "Girolami2011"

\end_inset


\end_layout

\begin_layout Itemize
Relation to geometry once again
\end_layout

\end_deeper
\begin_layout Paragraph
Linear costs via Incomplete Cholesky (TODO write properly)
\end_layout

\begin_layout Standard
Solving the linear system in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:kernel_score_matching_linear_system"

\end_inset

 for 
\begin_inset Formula $\alpha$
\end_inset

 requires 
\begin_inset Formula ${\cal O}(n^{3})$
\end_inset

 computation and 
\begin_inset Formula ${\cal O}(n^{2})$
\end_inset

 storage.
 This can be limiting, in particular if training data comes from an increasingly
 long Markov chain.
 
\begin_inset CommandInset citation
LatexCommand citet
key "sejdinovic_kernel_2014"

\end_inset

 sidestepped a similar problem via fixing the number of training data when
 computing MCMC proposals.
 However, we would ideally like to use 
\emph on
all
\emph default
 available data.
 We therefore compute an incomplete Cholesky factorisation 
\begin_inset Formula $K\approx LL^{T},$
\end_inset

 where 
\begin_inset Formula $L$
\end_inset

 is 
\begin_inset Formula $n\times t$
\end_inset

 and 
\begin_inset Formula $t\ll n$
\end_inset

, which costs 
\begin_inset Formula ${\cal O}(nt)$
\end_inset

 storage.
 Using the Woodbury identity, we can then rewrite the linear system into
 a form similar to 
\begin_inset Formula $L(L^{T}L+\lambda I)^{-1}L^{T}$
\end_inset

 which costs 
\begin_inset Formula ${\cal O}(t^{3}+t^{2}n)$
\end_inset

 computation.
 Treating 
\begin_inset Formula $t$
\end_inset

 as fixed, this is linear in both storage and computation, allowing to use
 all Markov chain history.
 See appendix 
\begin_inset CommandInset ref
LatexCommand eqref
reference "sec:appendix_kernel_score_matching"

\end_inset

 for details.
\end_layout

\begin_layout Section
Experiments
\end_layout

\begin_layout Subsection
Comparison of trajectories
\end_layout

\begin_layout Standard
Empirically compare on how well we can approximate the true HMC trajectories
\end_layout

\begin_layout Itemize
How to put in numbers? Maybe run leapfrog with same momentum and step size
 for the same number of steps.
 Then somehow compute how far this travelled, and the average acceptance
 probability.
 Latter is in particular interesting.
\end_layout

\begin_layout Itemize
2D Banana & other synthetic
\end_layout

\begin_layout Itemize
Higher dimensions, projections
\end_layout

\begin_layout Itemize
Kernel choice via cross-validation and how we get most similar trajectories
\end_layout

\begin_layout Itemize
Optimise acceptance rate? Is this even well conditioned? Plot 
\begin_inset Formula $J(\alpha)$
\end_inset

 and ESS, and see whether cross-validation kernel is good for mixing
\end_layout

\begin_layout Subsection
Pseudo-Marginal MCMC
\end_layout

\begin_layout Itemize
Reproduce Kameleon experiments from Kameleon paper with KHMC
\end_layout

\begin_layout Itemize
Higher dimensional, also compare to plain HMC, (and random walk)
\end_layout

\begin_layout Itemize
Mushroom as a cool dataset?
\end_layout

\begin_layout Subsection
Comparisons to geometry HMC?
\end_layout

\begin_layout Standard
For that, first need to work out how we would even do that.
\end_layout

\begin_layout Section
Conclusions
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "local"
options "unsrt"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
normalsize
\end_layout

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
onecolumn
\end_layout

\end_inset


\end_layout

\begin_layout Section
\start_of_appendix
Proofs
\end_layout

\begin_layout Subsection
\begin_inset CommandInset label
LatexCommand label
name "sec:appendix_kernel_score_matching"

\end_inset

Kernel Score Matching Details
\end_layout

\begin_layout Paragraph
Objective function
\end_layout

\begin_layout Standard
Assume
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\log q(\xi;\alpha):=\log\Pr(\xi;\alpha)+\log Z(\alpha)=\sum_{i=1}^{m}\alpha_{i}k(x_{i},\xi)
\]

\end_inset

where
\begin_inset Formula 
\[
k(x_{i},\xi)=\exp\left(-\frac{\|x_{i}-\xi\|^{2}}{\sigma}\right)=\exp\left(-\frac{1}{\sigma}\sum_{\ell=1}^{d}(x_{i\ell}-\xi_{\ell})^{2}\right)
\]

\end_inset

Thus
\begin_inset Formula 
\[
\psi_{\ell}(\xi;\alpha)=\frac{2}{\sigma}\sum_{i=1}^{m}\alpha_{i}(x_{i\ell}-\xi_{\ell})\exp\left(-\frac{\|x_{i}-\xi\|^{2}}{\sigma}\right)
\]

\end_inset

and
\begin_inset Formula 
\begin{align*}
\partial_{\ell}\psi_{\ell}(\xi;\alpha) & =\frac{-2}{\sigma}\sum_{i=1}^{m}\alpha_{i}\exp\left(-\frac{\|x_{i}-\xi\|^{2}}{\sigma}\right)+\left(\frac{2}{\sigma}\right)^{2}\sum_{i=1}^{m}\alpha_{i}(x_{i\ell}-\xi_{\ell})^{2}\exp\left(-\frac{\|x_{i}-\xi\|^{2}}{\sigma}\right)\\
 & =\frac{2}{\sigma}\sum_{i=1}^{m}\alpha_{i}\exp\left(-\frac{\|x_{i}-\xi\|^{2}}{\sigma}\right)\left[-1+\frac{2}{\sigma}(x_{i\ell}-\xi_{\ell})^{2}\right].
\end{align*}

\end_inset

Substituting this into 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:score_matching_objective_sample_version"

\end_inset

 yields
\begin_inset Formula 
\begin{align*}
J(\alpha) & =\frac{1}{m}\sum_{i=1}^{m}\sum_{\ell=1}^{d}\left[\partial_{\ell}\psi_{\ell}(x_{i};\alpha)+\frac{1}{2}\psi_{\ell}(x_{i};\alpha)^{2}\right]\\
 & =\frac{2}{m\sigma}\sum_{\ell=1}^{d}\sum_{i=1}^{m}\sum_{j=1}^{m}\alpha_{i}\exp\left(-\frac{\|x_{i}-x_{j}\|^{2}}{\sigma}\right)\left[-1+\frac{2}{\sigma}(x_{i\ell}-x_{j\ell})^{2}\right]\\
 & \qquad+\frac{2}{m\sigma^{2}}\sum_{\ell=1}^{d}\sum_{i=1}^{m}\left[\sum_{j=1}^{m}\alpha_{j}(x_{j\ell}-x_{i\ell})\exp\left(-\frac{\|x_{i}-x_{j}\|^{2}}{\sigma}\right)\right]^{2}
\end{align*}

\end_inset


\end_layout

\begin_layout Paragraph
Matrix form for the cost function
\end_layout

\begin_layout Standard
The expression for the term 
\begin_inset Formula $J(\alpha)$
\end_inset

 being optimized is the sum of two terms.
 
\end_layout

\begin_layout Standard
Consider the 
\series bold
first term
\series default
:
\begin_inset Formula 
\[
\sum_{\ell=1}^{d}\sum_{i=1}^{m}\sum_{j=1}^{m}\alpha_{i}\exp\left(-\frac{\|x_{i}-x_{j}\|^{2}}{\sigma}\right)\left[-1+\frac{2}{\sigma}(x_{i\ell}-x_{j\ell})^{2}\right]
\]

\end_inset

 The term we need to compute is
\begin_inset Formula 
\begin{align*}
 & \sum_{i=1}^{m}\sum_{j=1}^{m}\alpha_{i}\exp\left(-\frac{\|x_{i}-x_{j}\|^{2}}{\sigma}\right)(x_{i\ell}-x_{j\ell})^{2},\\
= & \sum_{i=1}^{m}\sum_{j=1}^{m}\alpha_{i}\exp\left(-\frac{\|x_{i}-x_{j}\|^{2}}{\sigma}\right)\left(x_{i\ell}^{2}+x_{j\ell}^{2}-2x_{i\ell}x_{j\ell}\right).
\end{align*}

\end_inset

Define 
\begin_inset Formula 
\[
x_{\ell}:=\left[\begin{array}{ccc}
x_{1\ell} & \hdots & x_{m\ell}\end{array}\right]^{\top}.
\]

\end_inset

The final term may be computed cheaply with the right ordering of operations,
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
We assume we have an incomplete Cholesy representation of 
\begin_inset Formula $K_{ij}:=\exp\left(-\frac{\|x_{i}-x_{j}\|^{2}}{\sigma}\right)$
\end_inset

, 
\begin_inset Formula 
\[
K\approx LL^{\top},
\]

\end_inset

where 
\begin_inset Formula $L$
\end_inset

 is 
\begin_inset Formula $m\times t$
\end_inset

 and 
\begin_inset Formula $t\ll m$
\end_inset

.
\end_layout

\end_inset


\begin_inset Formula 
\[
-2(\alpha\odot x_{\ell})^{\top}Kx_{\ell}\approx-2(\alpha\odot x_{\ell})^{\top}LL^{\top}x_{\ell},
\]

\end_inset

where 
\begin_inset Formula $\alpha\odot x_{\ell}$
\end_inset

 is the entrywise product.
 The remaining terms are sums with constant row or column terms, and can
 likewise be computed cheaply: define 
\begin_inset Formula $s_{\ell}:=x_{\ell}\odot x_{\ell}$
\end_inset

 with components 
\begin_inset Formula $s_{i\ell}=x_{i\ell}^{2}$
\end_inset

.
 Then
\begin_inset Formula 
\begin{align*}
\sum_{i=1}^{m}\sum_{j=1}^{m}\alpha_{i}k_{ij}s_{j\ell} & =\alpha^{\top}Ks_{\ell}\\
 & \approx\alpha^{\top}LL^{\top}s_{\ell},
\end{align*}

\end_inset

which is cheap to compute.
 Likewise
\begin_inset Formula 
\[
\sum_{i=1}^{m}\sum_{j=1}^{m}\alpha_{i}x_{i\ell}^{2}k_{ij}\approx(\alpha\odot s_{\ell})^{\top}LL^{\top}1.
\]

\end_inset


\end_layout

\begin_layout Standard
We now write out the 
\series bold
second term
\series default
.
 Considering only the 
\begin_inset Formula $\ell$
\end_inset

th dimension, this is
\begin_inset Formula 
\begin{align*}
 & \sum_{i=1}^{m}\left[\sum_{j=1}^{m}\alpha_{j}(x_{j\ell}-x_{i\ell})\exp\left(-\frac{\|x_{i}-x_{j}\|^{2}}{\sigma}\right)\right]^{2}
\end{align*}

\end_inset

In matrix notation, the inner sum is a column vector,
\begin_inset Formula 
\[
K(\alpha\odot x_{\ell})-\left(K\alpha\right)\odot x_{\ell}\approx LL^{\top}(\alpha\odot x_{\ell})-\left(LL^{\top}\alpha\right)\odot x_{\ell}.
\]

\end_inset

We then take the entrywise square and sum the resulting vector, where both
 operations cost 
\begin_inset Formula $O(m)$
\end_inset

.
\end_layout

\begin_layout Paragraph
Solving 
\begin_inset Formula $J(\alpha)$
\end_inset

 for 
\begin_inset Formula $\alpha$
\end_inset


\end_layout

\begin_layout Standard
If we denote by 
\begin_inset Formula $D_{x}$
\end_inset

 the matrix with the vector 
\begin_inset Formula $x$
\end_inset

 on its diagonal, then the following two relations hold
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
K(\alpha\odot x) & =KD_{x}\alpha\\
(K\alpha)\odot x & =D_{x}K\alpha
\end{align}

\end_inset

This means that 
\begin_inset Formula $J(\alpha)$
\end_inset

 as defined previously,
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
J(\alpha) & =\frac{2}{m\sigma}\sum_{\ell=1}^{D}\left[\frac{2}{\sigma}\left[\alpha^{T}Ks_{\ell}+(\alpha\odot s_{\ell})^{T}K\mathbf{1}-2(\alpha\odot x_{\ell})^{T}Kx_{\ell}\right]-\alpha^{T}K\mathbf{1}\right]\\
 & +\frac{2}{m\sigma^{2}}\sum_{\ell=1}^{D}\left[(\alpha\odot x_{\ell})^{T}K-x_{\ell}^{T}\odot(\alpha^{T}K)\right]\left[K(\alpha\odot x_{\ell})-(K\alpha)\odot x_{\ell}\right],
\end{align*}

\end_inset

can be rewritten
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
J(\alpha) & =\frac{2}{m\sigma}\alpha^{T}\sum_{\ell=1}^{D}\left[\frac{2}{\sigma}(Ks_{\ell}+D_{s_{\ell}}K\mathbf{1}-2D_{x_{\ell}}Kx_{\ell})-K\mathbf{1}\right]\\
 & +\frac{2}{m\sigma^{2}}\alpha^{T}\left(\sum_{\ell=1}^{D}\left[D_{x_{\ell}}K-KD_{x_{\ell}}\right]\left[KD_{x_{\ell}}-D_{x_{\ell}}K\right]\right)\alpha\\
 & =\frac{2}{m\sigma}\alpha^{T}b+\frac{2}{m\sigma^{2}}\alpha^{T}C\alpha
\end{align*}

\end_inset

where
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
b & =\sum_{\ell=1}^{D}\left(\frac{2}{\sigma}(Ks_{\ell}+D_{s_{\ell}}K\mathbf{1}-2D_{x_{\ell}}Kx_{\ell})-K\mathbf{1}\right)\in\mathbb{R}^{m}\\
C & =\sum_{\ell=1}^{D}\left[D_{x_{\ell}}K-KD_{x_{\ell}}\right]\left[KD_{x_{\ell}}-D_{x_{\ell}}K\right]\in\mathbb{R}^{m\times m}.
\end{align*}

\end_inset

Assuming 
\begin_inset Formula $C$
\end_inset

 is invertible, this is minimised by 
\emph on

\begin_inset Formula 
\[
\hat{\alpha}=\frac{-\sigma}{2}C^{-1}b.
\]

\end_inset


\end_layout

\begin_layout Paragraph
Incomplete Cholesky
\end_layout

\begin_layout Standard
TODO
\end_layout

\begin_layout Subsection
Consistency?
\end_layout

\end_body
\end_document
