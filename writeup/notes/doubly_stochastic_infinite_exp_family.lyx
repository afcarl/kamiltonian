#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass scrartcl
\use_default_options true
\begin_modules
theorems-ams
theorems-sec
\end_modules
\maintain_unincluded_children false
\language british
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 12
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 1
\use_package esint 0
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine natbib
\cite_engine_type numerical
\biblio_style plainnat
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Subsection
Doubly stochastic infinite exponential family estimators
\end_layout

\begin_layout Standard
It was shown in 
\begin_inset CommandInset citation
LatexCommand cite
after "Theorem 3"
key "SriFukKumGreHyv14"

\end_inset

 that the unnormalised infinite exponential family model in 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:infinite_exp_family"

\end_inset

 can be fitted via score matching.
 This is done via first writing the score matching objective as
\begin_inset Formula 
\begin{align*}
J(f) & =\frac{1}{2}\langle f,Cf\rangle_{{\cal H}}+\langle f,b\rangle_{{\cal H}},
\end{align*}

\end_inset

where 
\begin_inset Formula $C:{\cal H\times{\cal H}\rightarrow{\cal H}}$
\end_inset

 and 
\begin_inset Formula $b\in{\cal H}$
\end_inset

 are defined as
\begin_inset Formula 
\begin{align*}
Cf & =\int p_{0}(x)\sum_{\ell=1}^{d}\frac{\partial k(\cdot,x)}{\partial x_{\ell}}\frac{\partial f(x)}{\partial x_{\ell}}dx\\
\xi & =\int p_{0}(x)\sum_{\ell=1}^{d}\left(\frac{\partial k(\cdot,x)}{\partial x_{\ell}}\frac{\partial\log q_{0}(x)}{\partial x_{\ell}}+\frac{\partial^{2}k(\cdot,x)}{\partial x_{\ell}^{2}}\right)dx,
\end{align*}

\end_inset

and then minimising 
\begin_inset Formula 
\begin{equation}
\argmin_{f\in{\cal H}}J(f)+\frac{\lambda}{2}\Vert f\Vert^{2}.\label{eq:infinite_exp_score_match_objective}
\end{equation}

\end_inset

The unique minimiser can be obtained by solving an 
\begin_inset Formula $nd$
\end_inset

-dimensional linear system.
 This is clearly infeasible here as we would like to estimate trajectories
 of Markov chains of increasing length.
 We therefore apply the doubly stochastic gradient for kernel methods framework
 
\begin_inset CommandInset citation
LatexCommand cite
key "dai2014scalable"

\end_inset

, which is based on unbiasedly estimating the functional gradients of 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:infinite_exp_score_match_objective"

\end_inset

 via using both mini-batches and the random kitchen sink framework developed
 by 
\begin_inset CommandInset citation
LatexCommand cite
key "Rahimi2007"

\end_inset

.
 Intuitively, 
\begin_inset CommandInset citation
LatexCommand cite
key "dai2014scalable"

\end_inset

 rewrite the objective function as an expected value of some loss function
 that involves the kernel, and then approximate the expectation over the
 data with a single data point and the kernel with a single random Fourier
 feature product.
 This eventually allows to minimise the objective using stochastic gradient
 descent, and comes with theoretical guarantees for convergence and approximatio
n error.
\end_layout

\begin_layout Standard
Here, due the the form of 
\begin_inset Formula $J(f)$
\end_inset

, we will replace kernel gradients by random feature approximations.
 Due to linearity of expectation, inner product and gradients, the functional
 gradient of 
\begin_inset Formula $J(f)$
\end_inset

 at 
\begin_inset Formula $f$
\end_inset

 is given as (we assume 
\begin_inset Formula $q_{0}(x)=1$
\end_inset

)
\begin_inset Formula 
\begin{align*}
\nabla_{f}\left[J(f)\right] & =\nabla_{f}\left[\frac{1}{2}\langle f,Cf\rangle_{{\cal H}}+\langle f,b\rangle_{{\cal H}}\right]\\
 & =Cf+b\\
 & =\mathbb{E}_{x}\left[\sum_{\ell=1}^{d}\left(\frac{\partial k(\cdot,x)}{\partial x_{\ell}}\frac{\partial f(x)}{\partial x_{\ell}}+\frac{\partial^{2}k(\cdot,x)}{\partial x_{\ell}^{2}}\right)\right]\\
 & =\mathbb{E}_{x,\omega}\left[\phi_{\omega}(\cdot)\sum_{\ell=1}^{d}\left(\dot{\phi}_{\omega}^{\ell}(x)\frac{\partial f(x)}{\partial x_{\ell}}+\ddot{\phi}_{\omega}^{\ell}(x)\right)\right],
\end{align*}

\end_inset

where we defined 
\begin_inset Formula $\dot{\phi}_{\omega}^{\ell}(x):=\frac{\partial}{\partial x_{\ell}}\phi(x)$
\end_inset

 and 
\begin_inset Formula $\ddot{\phi}_{\omega}^{\ell}(x):=\frac{\partial^{2}}{\partial x_{\ell}^{2}}\phi_{\omega}(x)$
\end_inset

.
 An unbiased estimator can be obtained for a single data 
\begin_inset Formula $x_{t}\sim p_{0}(\cdot)$
\end_inset

 and a single frequency 
\begin_inset Formula $\omega_{t}\sim\Gamma(\cdot)$
\end_inset

, i.e.
\begin_inset Formula 
\begin{align*}
\zeta_{t}(\cdot): & =\phi_{\omega_{t}}(\cdot)\sum_{\ell=1}^{d}\left(\dot{\phi}_{\omega_{t}}^{\ell}(x_{t})\frac{\partial f(x_{t})}{\partial[x_{t}]_{\ell}}+\ddot{\phi}_{\omega_{t}}^{\ell}(x_{t})\right),
\end{align*}

\end_inset

an expression which is similar to 
\begin_inset CommandInset citation
LatexCommand cite
after "Eq. 4"
key "dai2014scalable"

\end_inset

.
 We can now apply additive stochastic gradient updates, i.e.
 starting from 
\begin_inset Formula $f_{1}(\cdot)=0\in{\cal H}$
\end_inset

, we define for 
\begin_inset Formula $t>1$
\end_inset


\begin_inset Formula 
\[
f_{t+1}(\cdot):=f_{t}(\cdot)-\gamma_{t}(\zeta_{t}(\cdot)+\lambda f_{t}(\cdot)),
\]

\end_inset

where we added the regulariser term 
\begin_inset Formula $\nabla_{f}\frac{\lambda}{2}\Vert f\Vert^{2}=\lambda f$
\end_inset

 and 
\begin_inset Formula $\gamma_{t}$
\end_inset

 is a learning rate which behaves as 
\begin_inset Formula ${\cal O}(1/t)$
\end_inset

.
 Evaluation of 
\begin_inset Formula $f_{t+1}(\cdot)$
\end_inset

 at any point 
\begin_inset Formula $x$
\end_inset

 works as
\begin_inset Formula 
\[
f_{t+1}(x)=\sum_{i=1}^{t}\alpha_{i}\phi_{\omega_{i}}(x),
\]

\end_inset

where 
\begin_inset Formula $\alpha_{i}:=-\gamma_{i}\sum_{\ell=1}^{d}\left(\dot{\phi}_{\omega_{i}}^{\ell}(x_{i})\frac{\partial f(x_{i})}{\partial[x_{i}]_{\ell}}+\ddot{\phi}_{\omega_{i}}^{\ell}(x_{i})\right)$
\end_inset

.
 Evaluation of the gradient 
\begin_inset Formula $\frac{\partial f(x_{i})}{\partial[x_{i}]_{\ell}}$
\end_inset

 also works recursively via the reproducing property and the kernel approximatio
n
\begin_inset Formula 
\[
\frac{\partial f(x_{i})}{\partial[x_{i}]_{\ell}}=\frac{\partial\langle\phi_{\omega_{i}}(\cdot)\phi_{\omega_{t}}(x_{i}),f\rangle}{\partial[x_{i}]_{\ell}}=\dot{\phi}_{\omega_{i}}^{\ell}(x_{i})\langle\phi_{\omega_{i}}(\cdot),f\rangle=\dot{\phi}_{\omega_{i}}^{\ell}(x_{i})f_{i}(x_{i})
\]

\end_inset

Consequently, a stochastic gradient descent algorithm minimising 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:infinite_exp_score_match_objective"

\end_inset

 computes the 
\begin_inset Formula $\alpha_{t}$
\end_inset

 recursively using a fixed series of random basis functions 
\begin_inset Formula $\phi_{\omega_{t}}^{\ell}(x_{t})$
\end_inset

 and their derivatives.
\end_layout

\end_body
\end_document
