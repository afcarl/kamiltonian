\documentclass{article}

\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{bm}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{amsfonts}
%\usepackage{bbold}
\usepackage{sectsty}
\usepackage[compact]{titlesec}
\usepackage{color}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\usepackage{mdframed}
%\usepackage{pgfornament}
\usepackage{todonotes}

% Paragraph settings
\setlength{\parindent}{0in}
\setlength{\parskip}{0.3cm}

% Title and section title spacing and formatting
\titlespacing{\chapter}{0pt}{*0}{*0}
\titlespacing{\section}{0pt}{*2.5}{*0}
\titlespacing{\subsection}{0pt}{*0}{*0}
%\allsectionsfont{\centering}
\sectionfont{\normalsize}
\subsectionfont{\normalsize \itshape}
\subsubsectionfont{\normalfont \itshape}

% new commands %
\newcommand{\pd}[2]{ \frac{\partial #1}{\partial #2} }
\newcommand{\comment}[1]{}
\newcommand{\p}[1]{ (#1_t)_{t \geq 0} }
\newcommand{\ch}[1]{ \{#1_t\}_{t \geq 0} }
\newcommand{\X}{\mathcal{X}}
\newcommand{\B}{\mathcal{B}}
\newcommand{\TV}{\text{TV}}


\title{\Large \bfseries A textbook proof of geometric ergodicity.}
\author{Sam Livingstone, Heiko Strathmann}
\date{\today}

\begin{document}
\maketitle

\listoftodos

\section{Notation and Concepts}

\begin{itemize}
\item We use upper-case letters for random variables $X$ and events $A\in \mathcal{B}$. Lower-case values represent values of random variables, i.e. $X=x$ means the random variable $X$ takes the value $x$.
\item Denote by $\mathbb{P}(X_{i+1} \in A |X_i = x)$ the probability of the event $X_{i+1} \in A$ given that $X_i = x$.
\item \textbf{Total variation distance}
\begin{equation*}
\|\mu(\cdot) - \nu(\cdot)\|_{\TV} := \sup_{A \in \mathcal{B}} |\mu(A) - \nu(A)|
\end{equation*}
is the total variation distance between $\mu(\cdot)$ and $\nu(\cdot)$. Informally, this is the largest possible difference between the probabilities that $\mu(\cdot)$ and $\nu(\cdot)$ can assign to the same event.
\item \textbf{Irreducibility}. When $\X$ is countable, a Markov chain $\ch{X}$ is called \emph{irreducible} if for any $x,y \in \X$ there exist $n = n(x,y)$ and $m = m(y,x)$ such that $P^n(x,y)>0$ and $P^m(y,x) > 0$.  In the uncountable case, the chain is called $\varphi$-irreducible for some measure $\varphi(\cdot)$ if for any $x \in \X$ and any $A \in \B$ there is an $n = n(x,A)$ such that $P^n(x,A) > 0$ whenever $\varphi(A) > 0$.
\item \textbf{Aperiodicity}. When $\X$ is countable, the period $p_x$ of a state $x$ of a Markov chain is defined as
\[
p_x = \text{gcd}(\{ n \in \mathbb{N} : P^n(x,x) > 0 \}),
\]
where $\text{gcd}(A)$ denotes the greatest common divisor of the argument.  For irreducible chains all states have the same period \cite{norris1997markov}.  The chain is called \emph{aperiodic} if $p_x = 1$.  In the uncountable case, it suffices to note that any chain with a non-zero probability of remaining at the current position is aperiodic (so any Metropolis--Hastings algorithm) (for a full definition see \cite{tierney1994markov}).
\item \textbf{Coupling}.  A coupling of two probability distributions $\mu(\cdot)$ and $\nu(\cdot)$  on the measurable space $(\X,\B)$ is a joint distribution $\Lambda(\cdot)$ on the product space $(\X \times \X,\B \times \B)$ such that if $(X,Y) \sim \Lambda(\cdot)$ then marginally $X \sim \mu(\cdot)$ and $Y \sim \nu(\cdot)$.

\end{itemize}

\section{Geometric Ergodicity of Markov chains}

We describe the Markov chain $\{ X_t \}_{t \geq 0}$ on the measurable space $(\mathcal{X},\mathcal{B})$ through a starting point $X_0 = x$ and a transition kernel $P: \X \times \B \to [0,1]$.  For each $x \in \X$, $P(x,\cdot)$ defines a probability measure where $P(x,A) = \mathbb{P}(X_{i+1} \in A |X_i = x)$ for any $A \in \B$.  We will also use the shorthand $X_{i+1} \sim P(X_i,\cdot)$ rather than writing `If $X_i = x_i$ for any $x_i \in \X$ then $X_{i+1} \sim P(x_i,\cdot)$.'  The $n$-step kernel $P^n(x,A)$ is defined similarly for $X_{i+n}$.  We say $\pi(\cdot)$ is an invariant distribution of $P$ if $\int P(x,A)\pi(dx) = \pi(A)$.  We assume for the purpose of this note that each $\pi(\cdot)$ admits a Lebesgue density, and write $\pi(dx) = \pi(x)dx$.  The chain is called \emph{geometrically ergodic} if
\begin{equation} \label{eqn:geometric_ergodicity}
\| P^n(x,\cdot) - \pi(\cdot) \|_{\TV} = \mathcal{O}(\rho^n)
\end{equation}
for some $\rho < 1$.  In words, $P^n(x,\cdot)$ approaches $\pi(\cdot)$ at a geometric rate in $n$.  As the name suggests, a kernel $P(x,\cdot)$, can \emph{only} converge to a $\pi(\cdot)$ which is invariant for that kernel.

\section{Establishing Geometric Ergodicity}

\subsection{Doeblin condition for finite state spaces}
In fact, all Markov chains with a finite state space ($|\mathcal{X}| < \infty$)\todo{HS: Not clear where this comes in below} which are irreducible and aperiodic have a unique invariant measure \cite{norris1997markov}, and are geometrically ergodic.  A straightforward way of establishing this is through the \emph{coupling inequality}. Given some pair of random variables $(X,Y)$ with marginal distributions $X \sim \mu(\cdot)$ and $Y \sim \nu(\cdot)$, we have that
\begin{equation}
\|\mu(\cdot) - \nu(\cdot)\|_{\TV} \leq \mathbb{P}(X \neq Y).\label{eqn:coupling_inequality}
\end{equation}
The proof is straightforward, and direct, moving from the left to the right-hand side in a few steps using basic probability rules (see e.g. \cite{roberts2004general}).  We can use the coupling inequality to establish (\ref{eqn:geometric_ergodicity}) by constructing two Markov chains $\{Y_n\}_{n \geq 0}$ and $\{ X_n\}_{n \geq 0}$ such that (i) $X_n \sim P^n(x,\cdot)$ and $Y_n \sim \pi(\cdot)$ for all $n$, and (ii) the upper bound $\mathbb{P}(X_n \neq Y_n)$ in \eqref{eqn:coupling_inequality} decreases geometrically with $n$. 
Imagine that we can write $P$ (with limiting distribution $\pi(\cdot)$) as a mixture, i.e. assume there exists a measure $\nu(\cdot)$, a transition kernel $R(x,A)$, and  $\epsilon$  $0<\epsilon\leq1$, such that
\begin{equation} \label{eqn:doeblin}
P(x,\cdot) = \epsilon \nu(\cdot) + (1 - \epsilon)R(x,A).
\end{equation}
This way of writing $P$ as a mixture is known as the `splitting' technique, and can be done \emph{in certain regions of $\X$} whenever the Markov chain is recurrent (see below) \cite{meyn2009markov}.  While we avoid a full definition, any Markov chain which is $\pi$-irreducible and $\pi$-invariant is recurrent (so these are necessary requirements for $P(x,\cdot)$ to converge to $\pi(\cdot)$).  The following procedure gives a bound on the TV between $P^n(x,\cdot)$ and $\pi(\cdot)$:
\begin{enumerate}
\item Start with $Y_0 \sim \pi(\cdot)$ and $X_0 = x$.
\item For each iteration $n$ of the chain, sample $b_n \sim \texttt{Bernoulli}(\epsilon)$. 
\begin{itemize}
\item If $b_n = 1$ then sample $Y_n \sim \nu(\cdot)$ and set $X_n = Y_n$, and then make the chains take equal values $X_{n+t}=Y_{n+t}$ for all $t>0$. We say the chains  `coalesce'.
\item If $b_n = 0$ then sample $X_{n+1} \sim R(X_n,\cdot)$ and $Y_{n+1} \sim R(Y_{n},\cdot)$ independently.
\end{itemize}
\end{enumerate}
Intuitively, the construction leads to two Markov chains on $(X,Y)$-space that are independent (therefore $X\neq Y$) at first, but that start `coalescing' (i.e. $X=Y$) with a geometrically decaying probability as $n$ increases. This can be seen as follows:\todo{HS: I rewrote this quite a bit, verbosing way more. Hope it is still correct}
\begin{enumerate}
\item Before the chains `coalesce', at each iteration $X_{n+1}|X_n$ is marginally (in $\epsilon$ sense) sampled from $P(X_n,\cdot)$. Since $X_0=x$, we therefore have $X_{n+1} \sim P^{n+1}(x,\cdot)$.
\item The same is true for $Y_{n+1}|Y_n$. Since $\pi(\cdot)$ is the invariant distribution  of $P$ and $Y_0\sim\pi(\cdot)$ by construction, however, we have $Y_{n+1} \sim \pi(\cdot)$ for all $n$.
\item This means that $\mathbb{P}(X_n \neq Y_n)$ is an upper bound on the TV distance between $P^{n}(x,\cdot)$ and $\pi(\cdot)$.
\item For $n=0$, we clearly have $\mathbb{P}(X_0 \neq Y_0)=1$, as $\pi(\cdot)$ is continuous and $X_0 = x$.
\item For $n=1$, since the probability of `coalescence' is $\epsilon$ and they have zero probability of taking the same value otherwise (both continuous and independent), we have $\mathbb{P}(X_1 \neq Y_1)=\mathbb{P}(X_0 \neq Y_0)\mathbb{P}(\epsilon=0)=1-\epsilon$.  Induction gives $\mathbb{P}(X_n \neq Y_n) = (1-\epsilon)^n$. 
\end{enumerate}

The kernel $R(x,\cdot) = (P(x,\cdot) - \epsilon\nu(\cdot))/(1-\epsilon)$ is called the \emph{residual kernel}, and represents the part of $P(x,\cdot)$ in which there is dependence on the current point $x$, and the independent measure $\nu(\cdot)$ represents the part of $P(x,\cdot)$ that can be written as independent of $x$.

\vspace{0.3cm}

\textit{Example.}  Consider a Gaussian random walk Metropolis on the State space $\X = [0,10]$ with $\pi(\cdot) = U[0,10]$.  Then for any $x,y \in \X$ and $A \in \B$ we have
\[
P(x,A) \geq \mathbb{P}[y \in A ~~ \text{proposed and accepted}] = \int_A \alpha(x,y)Q(x,dy) \geq \int_A \min_{x,y \in \X}{\alpha(x,y)q(y|x)} dx.
\]
Since $q(y|x) = e^{-(x-y)^2/2}/\sqrt{2\pi} \geq e^{-50}/\sqrt{2\pi}$ and $\alpha(x,y) = 1$ as $\pi(\cdot)$ is uniform, then setting $\epsilon = e^{-50}/\sqrt{2\pi}$ gives
\[
P(x,A) \geq \epsilon \mu^L(A)
\]
where $\mu^L(\cdot)$ is the \emph{normalised} Lebesgue measure on $\mathbb{R}$ (normalised by dividing by $\mu^L(\X) = 10$ to get a probability measure).  Setting $R(x,A) = (P(x,A) - \epsilon \nu(A))/(1-\epsilon)$ gives the mixture representation, as desired.

\vspace{0.3cm}

This approach was first introduced by Doeblin (1936), and (\ref{eqn:doeblin}) is sometimes called the Doeblin condition.

\subsection{Lyapunov functions for unbounded state spaces}
On unbounded state spaces, however, it is often difficult to satisfy the Doeblin condition with a uniform $\epsilon$ for all $x \in \X$.  Instead we tend to establish (\ref{eqn:doeblin}) for any $x \in C$, where $C$ is called a `small set'.  Under mild conditions any compact set is small.  To establish a geometric bound here entails showing (\ref{eqn:doeblin}) inside $C$, together with another condition, the stipulation that the \emph{return time} to $C$ after leaving it,
\begin{equation} \label{eqn:return}
\tau_C := \inf \{ t \geq 1 : X_{m+t} \in C | X_m \in C \},
\end{equation}
follows a distribution with geometric tails, i.e. for large enough $t$ and some $\beta > 1$
\begin{equation}
\mathbb{P}[\tau_C = t] \propto \beta^{-t}. \label{eqn:geometric_return_time}
\end{equation}
\todo{HS: Should the return time not depend on $X_n$ for some fixed $n<t$ somehow? SJL: No, I've fixed your definition (you defined the hitting time).  And the geometric tails would be in $n$ I guess?  SJL: Yes, or whatever you call the iterations.}Meyn \& Tweedie showed that \eqref{eqn:geometric_return_time} is equivalent to the condition that there exists a function $V: \X \to [1,\infty)$ such that
\begin{equation}
\int V(y)P(x,dy) \leq \lambda V(x) + b1_C(x), \label{eqn:lyapunov_condition}
\end{equation}
or some $\lambda<1$, $b < \infty$, and set indicator function $1_C(x)$, with $V$ called a \emph{Lyapunov} function.  Intuitively, $\{ V(X_t) \}_{t \geq 0}$ can be thought of as a one-dimensional projection of the chain.\todo{HS: Is there intuition why \eqref{eqn:lyapunov_condition} is the same as \eqref{eqn:geometric_return_time}? Like: Imagine your MC was 1D, then this exactly corresponds? SJL: Not that I know of.}  The condition effectively states that to establish \eqref{eqn:geometric_return_time}, we only need look at the one-dimensional projections of $\ch{X}$.

Roberts \& Tweedie \cite{roberts1996geometric} further simplified matters by showing that if all compact sets are small then we need not explicitly find a $C$\todo{So far, we talked about finding a $V$, now all of a sudden there is a $C$? A bit spaghetti.  SJL: Don't get confused.  $C$ is the small set which we've already talked about a lot.}, but instead find a $V$ such that
\begin{equation} \label{eqn:ge2}
\limsup_{|x| \to \infty} \int \frac{V(y)}{V(x)} P(x,dy) < 1.
\end{equation}
Effectively, showing (\ref{eqn:ge2}) establishes geometric ergodicity.  In the case where $P$ is a Metropolis--Hastings kernel, then we can equivalently write
\begin{equation} \label{eqn:gemh}
\limsup_{|x| \to \infty} \int \left[ \frac{V(y)}{V(x)} - 1 \right] \alpha(x,y)Q(x,dy) < 0,
\end{equation}
where $Q$ is the proposal kernel centred at $x$, and $\alpha(x,y)$ is the acceptance probability to accept the proposal $y$ at $x$.  The skill in establishing the result in a given scenario is find a suitable way to bound $\alpha$ and choosing an appropriate $V$ such that (\ref{eqn:gemh}) can be established.\todo{HS: This is great! Really a good story on why all this is done. I guess the only major thing missing for me is what I said above about the compact set, and why the Doeblin works then, and doesnt in the general case.  SJL: Hopefully the example helps!}

\section{Geometric Ergodicity of the Random Walk Metropolis in 1D}

For the Random Walk Metropolis the kernel choice is such that $Q(x,dy) = q(|x-y|)dy$, meaning $\alpha(x,y) = 1 \wedge \pi(y)/\pi(x)$.  Since the acceptance rate is just the ratio of target densities, it lends itself quite nicely to a simple bound.  If we assume $\pi(x)$ is log-concave in the tails, then $\alpha(x,y)=\pi(y)/\pi(x) \leq \exp\left(-a(|y|-|x|)\right)$ for large enough $x$, and fixed $a$.  With this, a sensible choice of Lyapunov function is
\begin{equation}
V(x) = e^{s|x|}
\end{equation}
 for some $0 < s < a$.  Let's consider the positive tail, i.e. the case $x \to \infty$.  In this instance we can split the integral in (\ref{eqn:gemh}) into the regions $(-\infty,0,x,2x,\infty)$ as
\begin{align}
\label{eqn:integral_split}
\int_{-\infty}^0 [e^{s(|y|-x)} - 1]\alpha(x,y)&Q(x,dy) + \int_0^x [e^{s(y-x)} - 1]\alpha(x,y)Q(x,dy) \\ &+ \int_x^{2x} [e^{s(y-x)} - 1]\alpha(x,y)Q(x,dy) + \int_{2x}^\infty [e^{s(y-x)} - 1]\alpha(x,y)Q(x,dy).\nonumber
\end{align}
In the first and last terms in \eqref{eqn:integral_split} can be made arbitrarily small by taking $x$ large.  In the first case this is because
\[
\int_{-\infty}^0 [e^{s(|y|-x)} - 1]\alpha(x,y)Q(x,dy) \leq \underbrace{\int_{-x}^0 [e^{s(|y|-x)} - 1]Q(x,dy)}_{<0} + \underbrace{\int_{-\infty}^{-x} [e^{s(|y|-x)} - 1]e^{-a(|y|-x)}Q(x,dy)}_{\leq Q(x,(-\infty,-x))},
\]
where the first integral on the right-hand side is strictly negative for any $x$, since $-x\leq y$ which implies $|y|-x<0$ and therefore $e^{s(|y|-x)} - 1<0$. The second integral on the right-hand side is bounded above by $Q(x,(-\infty,-x))$, since $s<a$ and $y\leq x$ imply that $e^{(s-a)(|y|-x)}<1$, and $Q(x,(-\infty,-x))$ becomes negligibly small as $x$ grows.

Similarly to above, the last term in \eqref{eqn:integral_split} can be upper bounded using the log-concave restriction $\alpha(x,y) \leq e^{-a(|y|-|x|)}$ for $|y| \geq |x|$ as
\[
\int_{2x}^\infty [e^{(s-a)(y-x)} - e^{-a(y-x)}]Q(x,dy) \leq Q(x,(2x,\infty)) \to 0.
\]

We now have vanishing upper bounds for the first and last term in \eqref{eqn:integral_split} and are left with the middle two terms.  We can combine these by writing $y = x + z$, for $z \sim \mu(\cdot)$, meaning $\mu(\cdot)$ denotes zero mean proposal `increment' distribution.  Typically $\mu(\cdot)$ might be a zero mean Gaussian, e.g.\ $Q(x,\cdot) = \mathcal{N}(\cdot|x,\sigma^2)$.  We can then bound the middle two integrals with
\begin{align} \label{eqn:rwm}
&&\int_0^x [e^{s(y-x)} - 1]\alpha(x,y)Q(x,dy)+ \int_x^{2x} [e^{s(y-x)} - 1]\alpha(x,y)Q(x,dy)\\
&\leq &\int_0^x [e^{-sz} - 1 + e^{(s-a)z} + e^{-az}]\mu(dz) \nonumber \\
&=& -\int_0^{x} (1-e^{(s-a)z})(1-e^{-sz}) \mu(dz), \nonumber
\end{align}
which is strictly negative.  \todo{SJL: This way of laying it out is hideous.  Don't even remember whether you did it or me?}

To summarise, for large $x$ the entire integral will be comprised of terms which can be made arbitrarily small and terms which are strictly negative. This establishes (\ref{eqn:gemh}) as $x \to \infty$. Using equivalent arguments, (\ref{eqn:gemh}) also holds for $x \to -\infty$.

\section{Geometric Ergodicity of KMC-lite in 1D}

\subsection{Notation}

If $x$ denotes the current position in the chain, we denote the next candidate move as $y$.  We can write for the KMC lite kernel
\[
y(x,p) = x + c(x,p) + P, ~~ P \sim \mathcal{N}(0,1),
\]
where $c(x,p)$ is basically a sequence of gradient steps, which depends on the current point $x$ and the random variable $P$.  For brevity, we write $y(p) = x + c(x) + P$.  We also write $\mu(\cdot)$ to denote a standard Gaussian measure.

\subsection{Extra Assumptions}

We assumed that $c(x) \xrightarrow{p} 0$ as $|x| \to \infty$, which seems valid given the KMC-lite kernel, and that $c(x) < M$ for all $x \in \X$.

We also assumed the conditions of Theorem 2.2 in \cite{roberts1996geometric}.  These are technical conditions, but well-known among the Markov chain community.  They are extremely loose, and assert that
\begin{itemize}
\item The Markov chain is $\mu^L$-irreducible, where $\mu^L(\cdot)$ denotes Lebesgue measure, and aperiodic
\item All compact sets are small
\end{itemize}
This means that we can use (\ref{eqn:ge2}) to establish geometric ergodicity here too.  There is not a direct proof of this assumption, but noone would question that it is true in the cases of interest to us (I actually have a rough proof which will go in the geometric ergodicity of HMC paper we are writing, but the paper is still in progress at the moment).

\textbf{Theorem 2.2} (Roberts \& Tweedie).  {\itshape Assume that $\pi(\cdot)$ and $Q$ admit densities $\pi(x)$ and $q(y|x)$, that $\pi(x)$ is bounded away from 0 on compact sets, and that there are $\epsilon_q$ and $delta_q$ such that
\[
|x - y| \leq \delta_q \implies q(y|x) \geq \epsilon_q.
\]
Then the Metropolis--Hastings chain with proposal kernel $Q$ is $\mu^L$-irreducible and aperiodic, and every compact set is small.}

\subsection{Details}

We extend the Random Walk result, using the intuition the KMC-lite approaches a random walk in the tails.

The result can straightforwardly be adjusted to an integral over any set $(-x^\delta,x^{\delta})$ for some $\delta \in (0,1]$.  This will be crucial to the approach here, as the entire set $(x - x^\delta,x + x^\delta)$ can be pushed arbitrarily far into the tails of the distribution for $\delta < 1$, whereas $(0,2x)$ always contains the centre of the space.  So denoting
\[
I_a^b := \int_a^b \left[ e^{s(|y| - |x|)} - 1 \right]\alpha(x,y)Q(x,dy),
\]
we divide the integral of interest into
\[
I_{-\infty}^{x - x^\delta} + I_{x - x^\delta}^{x + x^\delta} + I_{x+x^\delta}^\infty.
\]
We can also write (\ref{eqn:ge2}) as an integral with respect to the Gaussian measure $\mu(\cdot)$ rather than $Q(x,\cdot)$, as $P$ is the only random variable, giving
\[
\int \left[ e^{s(|y(p)| - |x|)} - 1 \right] \alpha(x,y(p)) \mu(dp)
\]
The crux of the proof is basically to show that in the set $y \in (x - x^\delta,x+ x^{\delta})$, this integral can be made arbitrarily close to (\ref{eqn:rwm}), which is strictly negative (meaning this integral will be too).  Specifically, on this set the integral reduces to
\begin{equation} \label{eqn:kmc1}
\int \left[ e^{s(c(x) + p)} - 1 \right] \alpha(x,x + c(x) + p) \mu(dp).
\end{equation}
Note that $c(x)$ can be made arbitrarily small for any choice of $p \in (-x^\delta,x^\delta)$, and also that therefore $\alpha(x,y(p))$ can be made arbitrarily close to $1 \wedge \pi(y(p))/\pi(x)$.  Because of these, we can say that (\ref{eqn:kmc1}) can be made arbitrarily close to (\ref{eqn:rwm}) on this set, and hence will be strictly negative for large enough $x$.

For proposals $y$ outside of the set $(x - x^\delta,x + x^\delta)$, then in the tails the two integrals $I_{-\infty}^{x - x^\delta}$ and $I_{x + x^\delta}^\infty$ can be made arbitrarily close to zero.  For $y \in (-\infty,x-x^{\delta})$, since $x - M + p \leq y \leq x + M + p$ for all $y \in \X$ (where $M$ is the bound on the gradient term $c(x)$), there exists some constant $C_1 \in \mathbb{R}$ such that the integral is bounded above by
\[
\int_{-\infty}^{- C_1 - x^\delta} \left[e^{s(M + |p|)} - 1 \right]\alpha(x,y(p))\mu(dp),
\]
noting that $|y| = |x + M + p| \leq |x| + M + |p|$.  for $x$ large to get the term inside the exponential. As $0 \leq \alpha(x,y(p)) \leq 1$, this is again bounded above by
\[
e^{sM}\int_{-\infty}^{- C_1 - x^\delta} e^{s|p|}\mu(dp) = e^{sM}\int_{C_1 + x^{\delta}}^{\infty} e^{sp}\mu(dp) = e^{sM}e^{s^2/2}\Phi(s - x^\delta - C_1),
\]
using properties of the Gaussian distribution, where $\Phi$ is the standard Gaussian CDF.  The last term tends to $0$ as $x \to \infty$, as required.  The same argument can be applied to any $y \in (x + x^\delta,\infty)$ to show that the integral over that set can also be made arbitrarily small.

\subsection{Proof in paper}

I am sure this is not the most elegant way to make the above arguments precise, but I was a little pushed for time before submission.  Here is the proof in the paper reproduced in full for convenience.  Hopefully makes a bit more sense after the above explanation.

\paragraph{Notation}

Denote by $\alpha(x_{t},x^{*}(p'))$ is the probability of accepting
a $(p',x^{*})$ proposal at state $x_{t}$. Let $a\wedge b=\min(a,b)$.
Define $c(x^{(0)}):=\epsilon^{2}\sum_{i=0}^{L-1}\nabla f(x^{(i\epsilon)})/2$
and $d(x^{(0)}):=\epsilon(\nabla f(x^{(0)})+\nabla f(x^{(L\epsilon)}))/2+\epsilon\sum_{i=1}^{L-1}\nabla f(x^{(i\epsilon)})$,
where $x^{(i\epsilon)}$ is the $i$-th point of the leapfrog integration
from $x=x^{(0)}$.


We assumed $\pi(x)$ is log-concave in the tails, meaning $\exists x_{U}>0$
s.t. for $x^{*}>x_{t}>x_{U}$, we have $\pi(x^{*})/\pi(x_{t})\leq e^{-\alpha_{1}(\Vert x^{*}\Vert_{2}-\Vert x_{t}\Vert_{2})}$
and for $x_{t}>x^{*}>x_{U}$, we have $\pi(x^{*})/\pi(x_{t})\geq e^{-\alpha_{1}(\Vert x^{*}\Vert_{2}-\Vert x_{t}\Vert_{2})}$,
and a similar condition holds in the negative tail. Furthermore, we
assumed fixed HMC parameters: $L$ leapfrog steps of size $\epsilon$,
and wlog the identity mass matrix $I$. Following \cite{roberts1996geometric,mengersen1996rates},
it is sufficient to show 
\[
\limsup_{\Vert x_{t}\Vert_{2}\to\infty}\int\left[e^{s(\Vert x^{*}(p')\Vert_{2}-\Vert x_{t}\Vert_{2})}-1\right]\alpha(x_{t},x^{*}(p'))\mu(dp')<0,
\]
for some $s>0$, where $\mu(\cdot)$ is a standard Gaussian measure.
Denoting the integral $I_{-\infty}^{\infty}$, we split it into 
\[
I_{-\infty}^{-x_{t}^{\delta}}+I_{-x_{t}^{\delta}}^{x_{t}^{\delta}}+I_{x_{t}^{\delta}}^{\infty},
\]
for some $\delta\in(0,1)$. We show that the first and third terms
decay to zero whilst the second remains strictly negative as $x_{t}\to\infty$
(a similar argument holds as $x_{t}\to-\infty$). We detail the case
$\nabla f(x)\uparrow0$ as $x\to\infty$ here, the other is analogous.
Taking $I_{-x_{t}^{\delta}}^{x_{t}^{\delta}}$, we can choose an $x_{t}$
large enough that $x_{t}-C-L\epsilon x_{t}^{\delta}>x_{U}$, $-\gamma_{1}<c(x_{t}-x_{t}^{\delta})<0$
and $-\gamma_{2}<d(x_{t}-x_{t}^{\delta})<0$. So for $p'\in(0,x_{t}^{\delta})$
we have 
\[
L\epsilon p'>x^{*}-x_{t}>L\epsilon p'-\gamma_{1}\implies e^{-\alpha_{1}(-\gamma_{1}+L\epsilon p')}\geq e^{-\alpha_{1}(x^{*}-x_{t})}\geq\pi(x^{*})/\pi(x_{t}),
\]
where the last inequality is from (i). For $p'\in(\gamma_{2}^{2}/2,x_{t}^{\delta})$
\[
\alpha(x_{t},x^{*})\leq1\wedge\frac{\pi(x^{*})}{\pi(x_{t})}\exp\left(p'\gamma_{2}/2-\gamma_{2}^{2}/2\right)\leq1\wedge\exp\left(-\alpha_{2}p'+\alpha_{1}\gamma_{1}-\gamma_{2}^{2}/2\right),
\]
where $x_{t}$ is large enough that $\alpha_{2}=\alpha_{1}L\epsilon-\gamma_{2}/2>0$.
Similarly for $p'\in(\gamma_{1}/L\epsilon,x_{t}^{\delta})$ 
\[
e^{sL\epsilon p'}-1\geq e^{s(x^{*}-x_{t})}-1\geq e^{s(L\epsilon p'-\gamma_{1})}-1>0.
\]
Because $\gamma_{1}$ and $\gamma_{2}$ can be chosen to be arbitrarily
small, then for large enough $x_{t}$ we will have 
\begin{align}
0<I_{0}^{x_{t}^{\delta}} & \leq\int_{\gamma_{1}/L\epsilon}^{x_{t}^{\delta}}[e^{sL\epsilon p'}-1]\exp\left(-\alpha_{2}p'+\alpha_{1}\gamma_{1}-\gamma_{2}^{2}/2\right)\mu(dp')+I_{0}^{\gamma_{1}/L\epsilon}\nonumber \\
 & =e^{c_{1}}\int_{\gamma_{1}/L\epsilon}^{x_{t}^{\delta}}[e^{s_{2}p'}-1]e^{-\alpha_{2}p'}\mu(dp')+I_{0}^{\gamma_{1}/L\epsilon},\label{eqn1}
\end{align}
where $c_{1}=\alpha_{1}\gamma_{1}-\gamma_{2}^{2}/2>0$ for large enough
$x_{t}$, as $\gamma_{1}$ and $\gamma_{2}$ are of the same order.
Now turning to $p'\in(-x_{t}^{\delta},0)$, we can use an exact rearrangement
of the same argument (noting that $c_{1}$ can be made arbitrarily
small) to get 
\begin{equation}
I_{-x_{t}^{\delta}}^{0}\leq e^{c_{1}}\int_{\gamma_{1}/L\epsilon}^{x_{t}^{\delta}}[e^{-s_{2}p'}-1]\mu(dp')<0.\label{eqn2}
\end{equation}
Combining \eqref{eqn1} and \eqref{eqn2} and rearranging as in \cite[Theorem 3.2]{mengersen1996rates}
shows that $I_{-x_{t}^{\delta}}^{x_{t}^{\delta}}$ is strictly negative
in the limit if $s_{2}=sL\epsilon$ is chosen small enough, as $I_{0}^{\gamma_{2}/L\epsilon}$
can also be made arbitrarily small.

For $I_{-\infty}^{-x_{t}^{\delta}}$ it suffices to note that the
Gaussian tails of $\mu(\cdot)$ will dominate the exponential growth
of $e^{s(\Vert x^{*}(p')\Vert_{2}-\Vert x_{t}\Vert_{2})}$ meaning
the integral can be made arbitrarily small by choosing large enough
$x_{t}$, and the same argument holds for $I_{x_{t}^{\delta}}^{\infty}$.

\begin{thebibliography}{1}

  \bibitem{norris1997markov} Norris, James R. \emph{Markov chains}. No. 2008. Cambridge university press, 1998.

  \bibitem{tierney1994markov}  Tierney, Luke. "Markov chains for exploring posterior distributions." \emph{The Annals of Statistics} (1994): 1701-1728.

  \bibitem{roberts2004general} Roberts, Gareth O., and Jeffrey S. Rosenthal. "General state space Markov chains and MCMC algorithms." \emph{Probability Surveys} 1 (2004): 20-71.

  \bibitem{meyn2009markov} Meyn, Sean P., and Richard L. Tweedie. \emph{Markov chains and stochastic stability}. Springer Science \& Business Media, 2012.
  
  \bibitem{roberts1996geometric} Roberts, Gareth O., and Richard L. Tweedie. "Geometric convergence and central limit theorems for multidimensional Hastings and Metropolis algorithms." \emph{Biometrika} 83, no. 1 (1996): 95-110.
  
  \bibitem{mengersen1996rates} Mengersen, Kerrie L., and Richard L. Tweedie. "Rates of convergence of the Hastings and Metropolis algorithms." \emph{The Annals of Statistics} 24, no. 1 (1996): 101-121.

  \end{thebibliography}



\end{document}